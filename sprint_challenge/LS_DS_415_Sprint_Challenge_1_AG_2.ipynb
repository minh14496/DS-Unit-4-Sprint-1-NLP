{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bL5csS5KkXH"
   },
   "source": [
    "\n",
    "## Autograded Notebook (Canvas & CodeGrade)\n",
    "\n",
    "This notebook will be automatically graded. It is designed to test your answers and award points for the correct answers. Following the instructions for each Task carefully.\n",
    "Instructions\n",
    "\n",
    "- **Download** this notebook as you would any other ipynb file \n",
    "- **Upload** to Google Colab or work locally (if you have that set-up)\n",
    "- **Delete** `raise NotImplementedError()`\n",
    "\n",
    "- **Write** your code in the `# YOUR CODE HERE` space\n",
    "\n",
    "\n",
    "- **Execute** the Test cells that contain assert statements - these help you check your work (others contain hidden tests that will be checked when you submit through Canvas)\n",
    "\n",
    "- **Save** your notebook when you are finished\n",
    "- **Download** as a ipynb file (if working in Colab)\n",
    "- **Upload** your complete notebook to Canvas (there will be additional instructions in Slack and/or Canvas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zUCyQzsKkXK"
   },
   "source": [
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more manageable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "Successfully complete all these objectives to earn full credit. \n",
    "\n",
    "**Successful completion is defined as passing all the unit tests in each objective.**  \n",
    "\n",
    "Each unit test that you pass is 1 point. \n",
    "\n",
    "There are 5 total possible points in this sprint challenge. \n",
    "\n",
    "\n",
    "There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews\n",
    "\n",
    "____\n",
    "\n",
    "# Before you submit your notebook you must first\n",
    "\n",
    "1) Restart your notebook's Kernel\n",
    "\n",
    "2) Run all cells sequentially, from top to bottom, so that cell numbers are sequential numbers (i.e. 1,2,3,4,5...)\n",
    "- Easiest way to do this is to click on the **Cell** tab at the top of your notebook and select **Run All** from the drop down menu. \n",
    "\n",
    "3) Comment out the cell that generates a pyLDAvis visual in objective 4 (see instructions in that section). \n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KM6-P_tKkXL"
   },
   "source": [
    "\n",
    "\n",
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "id": "0y0jn4LzKkXM",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bec125eb29f89460cf0c19ba9aa9a2f",
     "grade": false,
     "grade_id": "cell-395851cd95d17235",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "bf61a96a-7cbc-448f-bf51-d809a318a6c0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Load reviews from URL\n",
    "data_url = 'https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_4/unit1_nlp/review_sample.json'\n",
    "\n",
    "# Import data into a DataFrame named df\n",
    "# YOUR CODE HERE\n",
    "df = pd.read_json(data_url, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "WmblO74kKkXN",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "356579363f311da83f4ef7abaf3c9212",
     "grade": true,
     "grade_id": "cell-cb5006475e42b8f9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "5abb9509-a49f-4f1f-884f-f5026839f5a8"
   },
   "outputs": [],
   "source": [
    "# Visible Testing\n",
    "assert isinstance(df, pd.DataFrame), 'df is not a DataFrame. Did you import the data into df?'\n",
    "assert df.shape[0] == 10000, 'DataFrame df has the wrong number of rows.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jl2x2ERKkXN"
   },
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S698x69YKkXO",
    "outputId": "062357c4-22b8-4fb2-a90a-0b75a22b2105"
   },
   "outputs": [],
   "source": [
    "# Optional: Consider using spaCy in your function. The spaCy library can be imported by running this cell.\n",
    "# A pre-trained model (en_core_web_sm) has been made available to you in the CodeGrade container.\n",
    "# If you DON'T need use the en_core_web_sm model, you can comment it out below.\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "id": "pvmUOq9KKkXO",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4837ed2a1cc13057ba40203859d46ff6",
     "grade": false,
     "grade_id": "cell-3d570d5a1cd6cb64",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "b2f9f01e-eebf-4f61-8cdd-812671e9612c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = nlp.Defaults.stop_words.union(['s', 'year'])\n",
    "def tokenize(doc):\n",
    "    \"\"\"\n",
    "    Takes a doc and returns a list of tokens in the form of lemmas\n",
    "    Non-alphabet, stop words, punctation, and pronoun are filtered out.\n",
    "    \"\"\"\n",
    "    # use regex to strip out multi white space and non alphabet\n",
    "    non_alpha = '[^a-zA-Z]'\n",
    "    multi_white_spaces = \"[ ]{2,}\"\n",
    "    doc = re.sub(non_alpha, ' ', doc)\n",
    "    doc = re.sub(multi_white_spaces, \" \", doc)\n",
    "    \n",
    "    # use nlp\n",
    "    doc = nlp(doc)\n",
    "    \n",
    "    return [token.lemma_.strip() for token in doc if (token.text.lower() not in STOP_WORDS) and \n",
    "            (token.is_punct != True) and (token.pos_ != 'PRON')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "swQLwHuoKkXP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2181ca9d36070260b1f75dcfd9e58965",
     "grade": true,
     "grade_id": "cell-02da164f6fbe730a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "f6f41af6-606c-4f07-a721-bf0e5c94f574"
   },
   "outputs": [],
   "source": [
    "'''Testing'''\n",
    "assert isinstance(tokenize(df.sample(n=1)[\"text\"].iloc[0]), list), \"Make sure your tokenizer function accepts a single document and returns a list of tokens!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeMz5w3-KkXP"
   },
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews (i.e. create a doc-term matrix).\n",
    "2. Write a fake review and query for the 10 most similar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, use `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "id": "hkAoa1TwKkXQ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d70a0a1a96cf8406c60b17e50b255a1a",
     "grade": false,
     "grade_id": "cell-0e96491cb529202c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "09764836-a57f-464e-f2a9-0a526f4d629b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 810 ms, total: 2min 2s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create a vector representation of the reviews \n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tokenize)\n",
    "\n",
    "# Name that doc-term matrix \"dtm\"\n",
    "dtm = tfidf_vect.fit_transform(df['text'])\n",
    "\n",
    "# View Feature Matrix as DataFrame\n",
    "dtm = pd.DataFrame(data=dtm.toarray(), columns=tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "id": "whuGseIFKkXQ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32b220e23c9aa1f602f08d1c2e879d0a",
     "grade": false,
     "grade_id": "cell-3d5bc610a8ec6b24",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "901f9d41-29fc-4a3c-bb04-fbaa30a3d173"
   },
   "outputs": [],
   "source": [
    "# Create and fit a NearestNeighbors model named \"nn\"\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# YOUR CODE HERE\n",
    "nn = NearestNeighbors(n_neighbors=10).fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4P2QfYLKkXQ"
   },
   "source": [
    "#### \"instructor confirmed that unit test needs to be updated\" I uploaded this to colab to edit the testing cell and change 'sklearn.neighbors.unsupervised' to 'sklearn.neighbors._unsupervised'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EHpG0r7dKkXR",
    "outputId": "40591f11-5e80-4771-f9ca-94b4b222937a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sklearn.neighbors._unsupervised'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.__module__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hP1rgTyxKkXR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0c054bd863f1a18f6f06f62b7f2664f",
     "grade": true,
     "grade_id": "cell-c43704dcff67e99b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c8a129e2-0671-4b43-ed51-477a582ad9ee",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Testing.'''\n",
    "assert nn.__module__ == 'sklearn.neighbors._unsupervised', ' nn is not a NearestNeighbors instance.'\n",
    "assert nn.n_neighbors == 10, 'nn has the wrong value for n_neighbors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P8ya4-NZKkXR",
    "outputId": "83845a35-c8f0-4fda-f2e5-1bf5c012eee5"
   },
   "outputs": [],
   "source": [
    "# Create a fake review and find the 10 most similar reviews\n",
    "fake_review = 'Fake. This is the worst company I have ever seen. I could not believe what they did to me. I already called their office number to talk to their manager. They never answer the phone'\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tokenize)\n",
    "# YOUR CODE HERE\n",
    "df_new = df['text'].copy()\n",
    "df_new.loc[len(df_new.index)] = fake_review\n",
    "dtm_new = tfidf_vect.fit_transform(df_new)\n",
    "# View Feature Matrix as DataFrame\n",
    "dtm_new = pd.DataFrame(data=dtm_new.toarray(), columns=tfidf_vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "id": "Uq70b1EZKkXS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3da2ced9f187ed0aa1a890785e2ba00e",
     "grade": false,
     "grade_id": "cell-496203e8746296ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "4339dee1-8c2f-475e-de33-a4920c492096"
   },
   "outputs": [],
   "source": [
    "# Fit into NearestNeighbors\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='auto').fit(dtm_new)\n",
    "doc = [dtm_new.iloc[-1].values]\n",
    "\n",
    "# Query Using kneighbors \n",
    "neigh_dist, neigh_index = nn.kneighbors(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4pyqEXvuKkXS",
    "outputId": "d5bc79e2-955b-4c5b-88fd-11c6421fbe41",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...\n",
       "2943     Well, from the outside it looks like a pretty ...\n",
       "3180     This Walmart has the rudest of employees I hav...\n",
       "4406     Probably the worst HVAC service I have used. A...\n",
       "4491     This is a update to my earlier review. The mec...\n",
       "5956     Yesterday my two friends and I were at Madison...\n",
       "6019     I overall liked the atmosphere of this locatio...\n",
       "8470     if could leave a 0 star i would i was on hold ...\n",
       "9587     Other than the pricing, this company is awful....\n",
       "10000    Fake. This is the worst company I have ever se...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.loc[df_new.index.isin(neigh_index[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kw7A7i4OKkXS"
   },
   "source": [
    "#### All the text are negative review of the companies. There is a pattern that using these word: worst, rudest, fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSMyXCM0KkXS"
   },
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a pipeline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier.\n",
    "    - Use that pipeline to train a model to predict the `stars` feature (i.e. the labels). \n",
    "    - Use that Pipeline to predict a star rating for your fake review from Part 2. \n",
    "\n",
    "\n",
    "\n",
    "2. Create a parameter dict including `one parameter for the vectorizer` and `one parameter for the model`. \n",
    "    - Include 2 possible values for each parameter\n",
    "    - **Use `n_jobs` = 1** \n",
    "    - Due to limited computational resources on CodeGrader `DO NOT INCLUDE ADDITIONAL PARAMETERS OR VALUES PLEASE.`\n",
    "    \n",
    "    \n",
    "3. Train the entire pipeline with a GridSearch\n",
    "    - Name your GridSearch object as `gs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tVvlm7WSKkXS",
    "outputId": "e17c64f1-481a-42bc-9c82-073ed9a5f1ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    4462\n",
       "4    2185\n",
       "1    1496\n",
       "3    1098\n",
       "2     759\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stars.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wzcVxS7QKkXT",
    "outputId": "f68633f8-1546-4c5e-cd5e-be7b450ab509"
   },
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "Y = df['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "id": "zR1WcHghKkXT",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1d18da8521d51d8bfc4b5b9d005fa34",
     "grade": false,
     "grade_id": "cell-e2beb0252d274bba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "f34db50d-c19f-4384-96d3-4cfc594cdfc0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(tokenizer=<function tokenize at 0x7fc40e778af0>)),\n",
       "                                       ('clf', RandomForestClassifier())]),\n",
       "             n_jobs=1,\n",
       "             param_grid={'clf__n_estimators': [50, 100],\n",
       "                         'vect__max_df': [1, 0.9]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "vect = TfidfVectorizer(tokenizer=tokenize)\n",
    "clf = RandomForestClassifier()\n",
    "pipe = Pipeline([('vect', vect),\n",
    "                 ('clf', clf)])\n",
    "\n",
    "# create a hyper-parameter dict\n",
    "params = {\n",
    "    \"vect__max_df\": [1, .9],\n",
    "    \"clf__n_estimators\": [50, 100]  \n",
    "}\n",
    "\n",
    "# Name the gridsearch instance \"gs\"\n",
    "gs = GridSearchCV(pipe, \n",
    "                  params, \n",
    "                  n_jobs=1, \n",
    "                  cv=3, \n",
    "                  verbose=1)\n",
    "\n",
    "# run the gridsearch\n",
    "gs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zp3IIVwMKkXT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9e2378efb868f104a4eb39e4f25563c",
     "grade": true,
     "grade_id": "cell-d07134c6fe5d056e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible Testing\n",
    "prediction = gs.predict([\"I wish dogs knew how to speak English.\"])[0]\n",
    "assert prediction in df.stars.values, 'You gs object should be able to accept raw text within a list. Did you include a vectorizer in your pipeline?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijbmK4L0KkXU"
   },
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "    - Set num_topics to `5`\n",
    "    - Name your LDA model `lda`\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "When you instantiate your LDA model, it should look like this: \n",
    "\n",
    "```python\n",
    "lda = LdaModel(corpus=corpus,\n",
    "               id2word=id2word,\n",
    "               random_state=723812,\n",
    "               num_topics = num_topics,\n",
    "               passes=1\n",
    "              )\n",
    "\n",
    "```\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSKagw20KkXU"
   },
   "source": [
    "## Note about  pyLDAvis\n",
    "\n",
    "**pyLDAvis** is the Topic modeling package that we used in class to visualize the topics that LDA generates for us.\n",
    "\n",
    "You are welcomed to use pyLDAvis if you'd like for your visualization. However, **you MUST comment out the code that imports the package and the cell that generates the visualization before you submit your notebook to CodeGrade.** \n",
    "\n",
    "Although you should leave the print out of the visualization for graders to see (i.e. comment out the cell after you run it to create the viz). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZAmlqL6mKkXU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "# Due to limited computationalresources on CodeGrader, use the non-multicore version of LDA \n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdnPkuTEKkXV"
   },
   "source": [
    "### 1. Estimate a LDA topic model of the review tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "id": "BtqrxaDxKkXV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9514841e71735eaa255bccc53b257896",
     "grade": false,
     "grade_id": "cell-66331a185ff52f15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Remember to read the LDA docs for more information on the various class attirbutes and methods available to you\n",
    "# in the LDA model: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "# don't change this value \n",
    "num_topics = 5\n",
    "\n",
    "# use tokenize function you created earlier to create tokens \n",
    "df['tokens'] = df['text'].apply(tokenize)\n",
    "# create a id2word object (hint: use corpora.Dictionary)\n",
    "id2word = corpora.Dictionary(df['tokens'] )\n",
    "# create a corpus object (hint: id2word.doc2bow)\n",
    "corpus = [id2word.doc2bow(text) for text in df['tokens']]\n",
    "# instantiate an lda model\n",
    "lda = LdaModel(corpus=corpus,\n",
    "               id2word=id2word,\n",
    "               random_state=723812,\n",
    "               num_topics = num_topics,\n",
    "               passes=1\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "au5DbMmRKkXV"
   },
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "huADFbEJKkXV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6479db0fa59c99d3ae3201c1f10ebca1",
     "grade": true,
     "grade_id": "cell-5a3c181311134fa9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible Testing\n",
    "assert lda.get_topics().shape[0] == 5, 'Did your model complete its training? Did you set num_topics to 5?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSkPpaGLKkXV"
   },
   "source": [
    "#### 2. Create 1-2 visualizations of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "id": "EQ5eVeXlKkXW",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "189591ed7b9e6e6146d59761fb418268",
     "grade": false,
     "grade_id": "cell-9b043e992fbd218c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el34431404756035446563271142310\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el34431404756035446563271142310_data = {\"mdsDat\": {\"x\": [-0.025825622949513984, -0.10682800288576576, -0.021507545447993333, 0.12535199588038212, 0.028809175402890956], \"y\": [-0.03534918085941122, -0.005481571762723209, -0.04176213244472319, -0.049471662744010395, 0.13206454781086815], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [26.86625306114973, 24.272527015502874, 23.81758104094521, 21.802032602615874, 3.241606279786317]}, \"tinfo\": {\"Term\": [\"t\", \"food\", \"dog\", \"good\", \"chicken\", \"love\", \"call\", \"tell\", \"place\", \"d\", \"s\", \"like\", \"car\", \"come\", \"go\", \"great\", \"order\", \"company\", \"sauce\", \"store\", \"dish\", \"appointment\", \"restaurant\", \"kid\", \"say\", \"work\", \"delicious\", \"drink\", \"cheese\", \"fry\", \"yuk\", \"bf\", \"freezer\", \"operation\", \"gluten\", \"penne\", \"removal\", \"chai\", \"Man\", \"macaron\", \"Tim\", \"Robert\", \"cannoli\", \"understaffed\", \"ikea\", \"Ami\", \"Juicy\", \"brioche\", \"mug\", \"infuse\", \"parm\", \"iffy\", \"Snooze\", \"scone\", \"Miami\", \"Alfredo\", \"dangerous\", \"George\", \"village\", \"shiny\", \"Holy\", \"tile\", \"vision\", \"yogurt\", \"glove\", \"coffee\", \"latte\", \"haunt\", \"gold\", \"Coffee\", \"wo\", \"tart\", \"slider\", \"gym\", \"brunch\", \"airline\", \"ice\", \"cafe\", \"cup\", \"pancake\", \"bakery\", \"cream\", \"cookie\", \"gelato\", \"don\", \"caramel\", \"toast\", \"t\", \"pastry\", \"wait\", \"donut\", \"seat\", \"breakfast\", \"waitress\", \"place\", \"s\", \"time\", \"good\", \"like\", \"little\", \"enjoy\", \"m\", \"get\", \"review\", \"lot\", \"see\", \"nice\", \"food\", \"go\", \"come\", \"didn\", \"eat\", \"try\", \"service\", \"great\", \"people\", \"long\", \"pretty\", \"drink\", \"want\", \"ve\", \"order\", \"ask\", \"know\", \"love\", \"look\", \"Cirque\", \"heartbeat\", \"yellowtail\", \"os\", \"ricotta\", \"Noodles\", \"cabbage\", \"Little\", \"tartar\", \"lil\", \"mongolian\", \"iPad\", \"Picasso\", \"Asia\", \"Cesar\", \"accent\", \"biryani\", \"burro\", \"gooey\", \"chive\", \"breast\", \"structure\", \"pollo\", \"frittata\", \"eating\", \"verde\", \"magician\", \"asado\", \"audience\", \"Bouchon\", \"Tacos\", \"overcooked\", \"taiwanese\", \"spaghetti\", \"mexican\", \"beef\", \"lamb\", \"taco\", \"soup\", \"chorizo\", \"chicken\", \"pork\", \"Thai\", \"rice\", \"noodle\", \"spicy\", \"dish\", \"sauce\", \"substitute\", \"chinese\", \"salsa\", \"seasoning\", \"steak\", \"dumpling\", \"veggie\", \"bean\", \"cheese\", \"meat\", \"portion\", \"delicious\", \"shrimp\", \"salmon\", \"pepper\", \"fry\", \"salad\", \"cook\", \"order\", \"taste\", \"fresh\", \"food\", \"good\", \"meal\", \"s\", \"bit\", \"restaurant\", \"t\", \"eat\", \"price\", \"menu\", \"like\", \"well\", \"try\", \"place\", \"time\", \"ve\", \"great\", \"come\", \"service\", \"get\", \"love\", \"little\", \"Josh\", \"knife\", \"tow\", \"acrylic\", \"curly\", \"Decor\", \"Wait\", \"Station\", \"battery\", \"bento\", \"Centre\", \"professionally\", \"Valentine\", \"TV\", \"Ale\", \"Pretty\", \"cheesesteak\", \"Cajun\", \"Nick\", \"implant\", \"ID\", \"Masala\", \"storage\", \"boba\", \"Nancy\", \"mop\", \"Indian\", \"sex\", \"rug\", \"novelty\", \"Yummy\", \"Kevin\", \"baklava\", \"exceptionally\", \"unit\", \"float\", \"develop\", \"Pasta\", \"North\", \"attach\", \"dentist\", \"Pineapple\", \"room\", \"raman\", \"great\", \"beer\", \"drink\", \"bar\", \"selection\", \"place\", \"friendly\", \"come\", \"server\", \"hotel\", \"staff\", \"clean\", \"food\", \"super\", \"awesome\", \"good\", \"love\", \"stay\", \"burger\", \"night\", \"service\", \"stop\", \"location\", \"nice\", \"amazing\", \"definitely\", \"like\", \"order\", \"try\", \"time\", \"s\", \"find\", \"experience\", \"look\", \"go\", \"t\", \"restaurant\", \"ve\", \"get\", \"want\", \"wait\", \"contractor\", \"harness\", \"passenger\", \"Wish\", \"Cox\", \"drama\", \"Pro\", \"humor\", \"warranty\", \"goal\", \"Care\", \"bouncer\", \"dedicated\", \"Peoria\", \"installer\", \"Earl\", \"billing\", \"Ford\", \"PA\", \"disgust\", \"Acura\", \"upcoming\", \"Pools\", \"info\", \"Salon\", \"scheduling\", \"horrific\", \"library\", \"breakdown\", \"escrow\", \"tire\", \"Mike\", \"company\", \"dealership\", \"therapist\", \"Johnny\", \"Dr\", \"appointment\", \"quote\", \"refer\", \"email\", \"tech\", \"repair\", \"office\", \"car\", \"doctor\", \"technician\", \"call\", \"contact\", \"nail\", \"rent\", \"schedule\", \"message\", \"phone\", \"client\", \"pedicure\", \"patient\", \"professional\", \"salon\", \"hair\", \"fix\", \"care\", \"job\", \"tell\", \"answer\", \"customer\", \"say\", \"work\", \"guy\", \"issue\", \"week\", \"day\", \"problem\", \"help\", \"thank\", \"need\", \"go\", \"know\", \"t\", \"charge\", \"look\", \"time\", \"ask\", \"service\", \"get\", \"year\", \"take\", \"experience\", \"want\", \"s\", \"come\", \"find\", \"give\", \"great\", \"like\", \"didn\", \"recommend\", \"ve\", \"grub\", \"tan\", \"caring\", \"mojito\", \"uptown\", \"centre\", \"Billy\", \"bi\", \"und\", \"exhaust\", \"tr\", \"PT\", \"re\", \"Killer\", \"noir\", \"hospitable\", \"Mont\", \"Goodyear\", \"inattentive\", \"alley\", \"Pastor\", \"shed\", \"screening\", \"Britney\", \"respective\", \"bowling\", \"abordable\", \"Vous\", \"Audrey\", \"temporary\", \"tar\", \"en\", \"il\", \"avec\", \"est\", \"sur\", \"le\", \"et\", \"dog\", \"de\", \"y\", \"les\", \"un\", \"sont\", \"une\", \"des\", \"Nordstrom\", \"Jeremy\", \"bike\", \"teacher\", \"pour\", \"l\", \"au\", \"la\", \"pas\", \"d\", \"vet\", \"t\", \"kid\", \"love\", \"like\", \"s\", \"dance\", \"come\", \"good\", \"store\", \"go\", \"e\", \"feel\", \"friend\", \"m\", \"get\", \"place\", \"food\", \"want\", \"price\", \"service\", \"look\", \"try\", \"order\", \"tell\", \"great\", \"time\", \"people\"], \"Freq\": [7347.0, 4605.0, 325.0, 6183.0, 1278.0, 2081.0, 905.0, 1603.0, 5032.0, 770.0, 6056.0, 3704.0, 768.0, 3761.0, 3028.0, 4068.0, 3487.0, 436.0, 941.0, 796.0, 920.0, 439.0, 1641.0, 438.0, 1584.0, 1544.0, 1214.0, 1321.0, 819.0, 990.0, 43.450186141952884, 34.966180204717, 22.651928567124664, 23.682827571940898, 58.19235196454763, 15.670902125800575, 18.978627321494656, 26.32048421342023, 26.4194960187601, 16.57502174332023, 20.9839106344339, 38.864017616609324, 14.73646735115092, 11.177507796351263, 11.688480280993591, 15.31717625014943, 12.306822197435787, 10.645904889966413, 10.35204011886209, 10.36620088458312, 14.87909700072725, 10.661956588422662, 10.335863016511652, 16.56599760009978, 12.446146911312496, 17.990296520446194, 17.185432604029124, 9.78340257896961, 9.015427156088991, 9.91871338080002, 17.20260405064294, 25.02715139305829, 11.416352843170264, 49.237957007639395, 20.912738493604085, 460.62813351070685, 48.88226021724177, 17.175443739512428, 35.96896503886109, 43.96123367144528, 30.298368807094924, 31.05649360597213, 61.05338157711326, 98.21031054225003, 136.79472289669982, 21.68047100815423, 306.06256893352787, 73.4163057245669, 123.92183739521528, 103.6894772358199, 42.30101250370349, 335.6049299964633, 103.71665927045963, 31.7464046110037, 810.3334062747099, 42.17836552032174, 99.65143234563034, 2787.4358297203785, 58.298003264253886, 750.1292361706738, 102.6250568813978, 305.9395901729143, 259.9012019456756, 242.82973087106532, 1805.0850288959127, 2080.2113932186667, 1498.7876564757148, 1871.9148219182498, 1246.1133209881543, 579.6832649823866, 401.60866090710414, 665.080007052536, 906.824842174298, 412.684143254926, 441.7316822748991, 311.5205255826317, 619.9663925629332, 1283.6113524952152, 934.1246103346048, 1072.5872563705007, 545.5800413074958, 556.0882161866494, 775.9633073140681, 942.3017499804624, 1003.141352478111, 507.2098148545926, 369.22881456587913, 404.5498186708956, 456.562630104851, 520.5070355361827, 531.8828236755954, 669.0998199938884, 464.12174776231143, 474.2043946432164, 482.7608030618503, 457.84935700354265, 24.05686347973964, 18.98255080069716, 18.831824578868538, 18.625582762539604, 16.142894688128617, 26.696788346582867, 39.36932944554575, 16.392658660865916, 17.766089311487857, 16.199041891835204, 16.026835604372987, 14.282561139330305, 17.547181411104358, 17.140075377733773, 12.436139050931121, 12.718094574810054, 11.107005794571002, 12.795546991884091, 17.71681487236476, 15.928797009270841, 40.172918345015354, 15.726714178986585, 19.44350241100572, 10.753074476382576, 10.24808150548762, 14.955330212209596, 9.95588909510875, 11.030401767829117, 22.61536177138426, 17.233328730205358, 33.000417724809736, 33.12904476124608, 24.2836165374112, 32.222403645573706, 181.99745769368818, 367.8395241943594, 115.82453193004831, 369.07022722222536, 412.8822387300614, 36.38434790512424, 949.9445854219935, 282.68235977039205, 147.1358410797834, 386.90709958577867, 239.55462725348522, 272.8363073378331, 633.0695723974771, 631.5080346084147, 39.24980543218962, 121.64870390506425, 170.5825593610248, 57.37795669224154, 299.46703542774316, 54.599250868078116, 153.3744700201751, 158.61638394893578, 517.2316160954854, 434.13485368112634, 318.57523363952504, 685.9983382410315, 211.83900151519066, 142.37530614495444, 126.39687785520101, 555.3005942166228, 418.84442980609504, 323.2998417862821, 1562.4773380882036, 614.5644075358991, 488.3029152924829, 1690.2522373581162, 2022.1248645117332, 468.3401248457217, 1867.391371310825, 452.58346632405477, 700.9875594613574, 1850.7795781553123, 636.7386438029073, 667.1283387735265, 533.1301832889883, 935.6838915211719, 498.2543358416677, 700.8003462105224, 878.9012960899055, 758.3486946519132, 560.4596067607503, 653.8158722804103, 572.2529567656185, 545.5693876071973, 506.52055166025036, 491.66584535879235, 454.94253027788926, 19.849697002243136, 19.12041935623766, 29.38311997634734, 27.23063148151047, 16.147168742743016, 15.68168935909751, 15.38179382462162, 18.297920478731356, 37.87551008807508, 21.550467126085152, 13.08346835869232, 13.640066305111755, 16.455916144898506, 12.05657642496462, 14.637436839944383, 15.386909559858925, 11.32464724541469, 16.812147794523582, 14.407566651904636, 12.246830303302216, 16.040114717227834, 10.97094282502132, 11.498548103566103, 51.30977046963604, 11.014868642692884, 12.267254727422126, 12.268006680976, 11.642479366990566, 10.852543842511713, 9.598246797819597, 20.618853572696004, 18.77708734289884, 17.28080131505657, 15.313095558031435, 36.21989219276491, 34.416747838167915, 14.766401689037538, 14.659549876876195, 38.29588276307792, 25.345747412074605, 51.23463113872757, 17.53080179870156, 610.2638928818474, 102.53914581192885, 1853.4885396697946, 318.68746130265754, 651.6781401635805, 499.3756500677843, 276.67932653144567, 1929.8705215343916, 621.7042035395087, 1408.1576875974376, 373.44687953977586, 241.46707311005233, 624.2262575099691, 386.8025270727313, 1485.5455329577285, 290.45955557838766, 266.39877861324936, 1810.3171966536602, 764.8385677718315, 295.35326580547354, 291.4222802252631, 408.7163899786917, 1043.4493102040399, 290.27963296412787, 372.85436465003227, 569.7005453869359, 484.13519900058157, 479.15736801829996, 969.7823866400058, 899.1968654446167, 707.691190154703, 1004.8932637666761, 1208.2383681025392, 515.6443183977957, 474.8225860058063, 540.3744198133659, 665.0683828123091, 852.7874756165958, 449.77182848476644, 471.62190752140265, 508.52108601726883, 451.79184709904365, 424.3739152796971, 30.968538061372644, 32.76317644522829, 23.130125295564667, 23.40332500781645, 23.389425592637654, 19.97773857405676, 27.14222281790149, 15.865277754640259, 81.38218006844407, 32.23161075609829, 15.26890421927305, 37.31457864849707, 15.31879339193717, 26.30054615969992, 19.800767431436114, 18.184582602173954, 14.985122489446542, 44.51558792074488, 15.687429985696212, 12.107755241050045, 11.485529242180386, 18.408205650231565, 11.7427854133384, 25.800231317564794, 24.71410252113165, 18.924264508821953, 10.624018713086073, 32.14312758869927, 15.356191841448572, 10.080651422044781, 143.53719799552405, 41.006185112737406, 396.2455599963694, 78.93838947516574, 40.78636184715287, 22.030167598684407, 277.7355531358545, 380.78601278956734, 111.91416142598129, 82.44156775549484, 129.23052818912112, 97.57923444346481, 191.45709933756658, 303.5592786799293, 612.6325529600861, 136.93090355984143, 72.23658604063661, 706.8104900896401, 116.57649291782909, 327.75008817204315, 95.53430081429332, 145.21793585642484, 79.09498346293819, 314.1426424449864, 96.27155797573603, 130.37496946343913, 151.49534958354855, 279.50952429289754, 178.36552656662576, 299.8777173597384, 282.38252523705626, 435.46913589743286, 361.18368663394114, 890.860042505866, 183.18865728842624, 611.90186648955, 782.3477168699283, 749.340155595985, 396.5030285130262, 253.07498732214393, 381.31971956236833, 700.2257988035724, 265.88038567020374, 353.0770823044382, 346.4447230597543, 601.3747093760726, 990.3929965678582, 661.2507937421136, 1696.2770512918164, 295.76363149627815, 647.691349316041, 1010.877654329715, 542.9972297294312, 823.5499105002326, 667.3303431321413, 429.01590971293865, 497.54574144288387, 487.43828801237026, 517.738955408274, 789.2317496024109, 627.5546497028779, 457.66973760258963, 403.7146855097578, 522.9970979202432, 458.7187403927929, 410.75046594481734, 407.31619337820064, 399.47903224888, 19.527929921360084, 23.951707417972585, 13.035782575125943, 9.379231717690502, 7.5126835024470795, 7.406409920138136, 7.732605005238276, 10.47934915421631, 6.3387168871689905, 6.073498057030417, 16.403856414481083, 10.698787421974048, 9.196538375001591, 6.084401345324795, 6.615115888791725, 5.561024444668676, 6.662663070767265, 6.634175078824378, 5.408422252357125, 7.985945975442016, 4.725145547883192, 4.844181967895344, 4.824042409203075, 5.763325025012011, 5.757285063656928, 19.063160756813176, 4.177628817459014, 4.45596218890503, 4.374370288664977, 7.233850216257376, 4.773796926675817, 28.464325816890163, 9.649424913982172, 11.037624774300609, 21.44313237477503, 7.990716849130333, 32.56311393317042, 37.953283340343624, 169.79600361492814, 70.12087654707543, 31.28777947664298, 14.260638509746085, 26.20794417983267, 11.297293550001648, 15.93236961578069, 17.273464068163047, 9.095571799924004, 9.532383863876422, 27.970850394477957, 22.477991159879053, 27.842900813883183, 26.226129829612532, 14.251606684939274, 28.321097621726672, 12.532470889310792, 70.93531153182882, 22.55858751970008, 160.69299171111624, 45.87049937838387, 83.62011106620342, 94.55104383574995, 111.68078217682157, 22.375549145291025, 81.14583331973955, 85.29332047112737, 43.5477229633565, 63.56161004688437, 25.734739847541956, 42.44606146222674, 40.10236964369993, 43.01255017162724, 46.08897880108508, 52.078866712937874, 49.244879101331435, 37.58971521994379, 37.540221277503086, 39.609100096568255, 34.68803276609601, 35.13610887664687, 35.756094658942764, 33.64003990059541, 35.023151972731206, 33.93399930114604, 31.7108779594092], \"Total\": [7347.0, 4605.0, 325.0, 6183.0, 1278.0, 2081.0, 905.0, 1603.0, 5032.0, 770.0, 6056.0, 3704.0, 768.0, 3761.0, 3028.0, 4068.0, 3487.0, 436.0, 941.0, 796.0, 920.0, 439.0, 1641.0, 438.0, 1584.0, 1544.0, 1214.0, 1321.0, 819.0, 990.0, 44.3231633777802, 35.76421583394474, 23.449693048919602, 24.526060881351015, 60.49481517000104, 16.47747288843842, 20.035581149425852, 27.919527325703204, 28.158648435114436, 17.705907215502982, 22.473178038767102, 41.63738922854958, 15.790775827999852, 11.987107939617461, 12.551976422217786, 16.456154236134992, 13.23446413334979, 11.463273988117008, 11.147770442156638, 11.164295221734479, 16.030556213149534, 11.497017213406941, 11.165275344707727, 17.900887264696234, 13.477502925953837, 19.49863358723523, 18.667853975794564, 10.655412741925634, 9.821612595316468, 10.807716776508357, 18.79919326787065, 27.468676583042306, 12.446292497244032, 54.964341019680575, 23.086955209476326, 567.2705292308301, 55.65121551614038, 18.849339214972886, 40.72416145257147, 50.41656897314758, 34.125006762613424, 35.267788083072105, 72.50425159599337, 122.02487934674481, 180.23992657097477, 24.19602189246892, 446.9872801116182, 92.7365369926068, 166.22727127804953, 137.78961467193162, 51.2052442455361, 534.4213604964515, 144.03392544871096, 37.15503113461274, 1545.480989385546, 51.58164644846614, 141.33814242814427, 7347.972926495219, 75.2651069297425, 1619.2649693697444, 147.6395834407177, 556.63913600605, 462.19575074839173, 428.6352319873781, 5032.264590599766, 6056.753664411263, 4306.841268525166, 6183.2742566027355, 3704.849383377875, 1401.2658153928496, 880.5086831758447, 1733.5034469993363, 2635.2858017850435, 939.4857254252782, 1044.8821314271042, 653.3895801270735, 1671.062918449822, 4605.271351311429, 3028.102626305872, 3761.6983837561747, 1494.4262544162884, 1539.969466292241, 2509.536810987911, 3394.479458388501, 4068.46601432129, 1411.0899574513846, 877.2943904659694, 1040.343847491495, 1321.9107341166705, 1780.750116876121, 1988.0885913820864, 3487.065728771696, 1498.1133976718695, 1704.2349762912563, 2081.540388895064, 1991.38423501761, 25.04317234282558, 19.80123020503178, 19.65604007489419, 19.445776410388138, 16.902450688619492, 27.983411614517973, 41.408903313467796, 17.254682401040938, 18.715707150215742, 17.065390074235143, 16.88929980028462, 15.066718651127099, 18.527153416858287, 18.12031738383215, 13.194481740489604, 13.516135182688627, 11.859153704571305, 13.669111273423924, 18.944603625044376, 17.122884094369944, 43.35246900630307, 16.972316127240052, 20.98526568127602, 11.611160574455653, 11.08009889194641, 16.17190550324556, 10.789856511062625, 11.955539979161351, 24.533518439762172, 18.71291968648167, 35.93763845515478, 36.273418939607254, 26.616108258397993, 35.53114315580561, 208.8120510300932, 447.075265575457, 135.52725346691034, 455.2229344720752, 518.2694981682241, 40.86402499701332, 1278.9467002596455, 358.3830896470164, 179.69576886096766, 506.05746650324323, 303.905065622193, 358.3074077796285, 920.1399839579658, 941.4463053862164, 44.82579779367573, 156.32183327910278, 228.6758988833275, 68.55482124951052, 431.1965237003135, 65.11590333312188, 206.0201837963986, 215.00990417671701, 819.5007384888844, 680.7154508536053, 494.2957859332213, 1214.9704365367036, 310.99714113375853, 196.60296694679332, 171.53603227174145, 990.4374553417131, 716.9276074646192, 531.4271663172151, 3487.065728771696, 1184.205674848796, 897.3806364303529, 4605.271351311429, 6183.2742566027355, 909.1921419771797, 6056.753664411263, 888.0799841382169, 1641.627886942462, 7347.972926495219, 1539.969466292241, 1780.9214396131965, 1283.1424570479714, 3704.849383377875, 1199.8166963541, 2509.536810987911, 5032.264590599766, 4306.841268525166, 1988.0885913820864, 4068.46601432129, 3761.6983837561747, 3394.479458388501, 2635.2858017850435, 2081.540388895064, 1401.2658153928496, 20.660313109635464, 19.970917881083004, 30.848703381359144, 28.624119224158168, 16.996946142038972, 16.52562146772398, 16.237791826897226, 19.354909787659473, 40.12331392802632, 22.93861811828854, 13.935321885712707, 14.52878896306336, 17.562880791412667, 12.879017252066811, 15.659448827906816, 16.489887638841623, 12.137289927010363, 18.02206034025466, 15.463713776914924, 13.152979170930678, 17.23288253830743, 11.819207544258253, 12.389719492406423, 55.31200755588418, 11.87677004183311, 13.234753875323719, 13.251965744704178, 12.63990508497832, 11.796787600867981, 10.447012453319662, 22.49684067089217, 20.476559568680493, 18.890745430267877, 16.70268503964531, 41.02860679333519, 39.220461082252555, 16.112885846150938, 16.0003950968713, 45.455543874358725, 29.245227457874833, 63.847920305550325, 19.505002078128765, 1068.8746604769772, 144.2139158988801, 4068.46601432129, 558.157882322009, 1321.9107341166705, 972.6729218485519, 493.1163995353166, 5032.264590599766, 1359.0549367708534, 3761.6983837561747, 773.6978834479526, 460.26156164166895, 1476.2650556887586, 838.7913770598666, 4605.271351311429, 602.7501191093933, 550.2814995700953, 6183.2742566027355, 2081.540388895064, 637.0669987943792, 636.2526993485011, 997.4109917021023, 3394.479458388501, 644.4741907086601, 925.8433751159276, 1671.062918449822, 1352.7317915749047, 1359.4719051377178, 3704.849383377875, 3487.065728771696, 2509.536810987911, 4306.841268525166, 6056.753664411263, 1596.0185900618003, 1454.4927765367024, 1991.38423501761, 3028.102626305872, 7347.972926495219, 1641.627886942462, 1988.0885913820864, 2635.2858017850435, 1780.750116876121, 1619.2649693697444, 31.772608557328283, 33.62173447248084, 23.888063483615895, 24.282819515495948, 24.270914556175043, 20.745759210577894, 28.413483216968512, 16.63673322085706, 85.3412983448618, 33.80896124706234, 16.026204078995626, 39.23230970714058, 16.137069333010782, 27.756377225338085, 20.94487440861899, 19.25029195528335, 15.864899686262648, 47.371245135280795, 16.695360888641208, 12.889867042119779, 12.234178061348805, 19.62515286784307, 12.536054072266765, 27.591158934678262, 26.43591373873116, 20.26539021440165, 11.381832376621285, 34.48740378906905, 16.48012113659667, 10.82029971415457, 156.12687692011994, 44.0722256970464, 436.08438016275505, 86.03395246339218, 44.35683056602819, 23.67282273929155, 315.30119316158886, 439.7071024696379, 125.75472192923606, 91.96309188347166, 147.12650400678234, 109.86680210975133, 223.1096787711398, 363.31547744038306, 768.070450276592, 158.7772699880991, 81.1483306460208, 905.1030660052892, 138.08156816309878, 422.3213959590174, 112.77693108366891, 179.7760080892955, 92.32794697144128, 425.23077536383806, 114.75391835827286, 161.96736204983662, 194.7190305255065, 391.60510030135794, 236.868352469542, 431.0318122190685, 403.5344531998063, 682.6131904158169, 551.1266451672245, 1603.475935403169, 252.99276349756857, 1142.971852336706, 1584.1712108745699, 1544.4693902417432, 697.232175169368, 387.53535661151057, 680.9638784599293, 1581.9929324220757, 421.0062120784452, 637.499817192778, 630.6968089417551, 1415.6508705503684, 3028.102626305872, 1704.2349762912563, 7347.972926495219, 516.4072255616232, 1991.38423501761, 4306.841268525166, 1498.1133976718695, 3394.479458388501, 2635.2858017850435, 1010.7891124978942, 1454.4535090456984, 1454.4927765367024, 1780.750116876121, 6056.753664411263, 3761.6983837561747, 1596.0185900618003, 1046.4386761945661, 4068.46601432129, 3704.849383377875, 1494.4262544162884, 1420.4978026607962, 1988.0885913820864, 20.393684089469986, 25.692546385962327, 14.135869418488324, 10.215058533930234, 8.416432042153094, 8.3263162217776, 8.712534354492648, 11.813412751754472, 7.166439068955674, 6.910943876287439, 18.701981120028904, 12.234378668166384, 10.52109502987776, 6.96588492371463, 7.613755024099407, 6.427638703132481, 7.717885257452374, 7.6869820808481215, 6.272522745104523, 9.338398220688084, 5.549313468892281, 5.698893659736298, 5.677414558125507, 6.863066609556457, 6.8696232035430285, 22.806972986717234, 5.0075579122225795, 5.348900753370739, 5.2509600977508555, 8.69437679361507, 5.737715768883045, 35.68337645435438, 11.773458169901797, 13.669993246775608, 27.750817596567124, 9.811697355321353, 45.515724856480865, 55.94646352908068, 325.92725568364665, 128.0440971433194, 49.92942407655861, 20.05860972963824, 44.411735585362266, 15.375710174963473, 24.377392218238565, 27.481813311537124, 11.918298554257689, 12.811399945425187, 63.47120256477752, 49.38176736634862, 72.42173709930131, 72.43559258677504, 25.705341557831378, 102.78603177550877, 22.187184610251926, 770.0550888333121, 79.73878782951932, 7347.972926495219, 438.09828990048493, 2081.540388895064, 3704.849383377875, 6056.753664411263, 88.77930597751003, 3761.6983837561747, 6183.2742566027355, 796.4634053552124, 3028.102626305872, 146.66124901451104, 1156.5822065747184, 1003.6279585522577, 1733.5034469993363, 2635.2858017850435, 5032.264590599766, 4605.271351311429, 1780.750116876121, 1780.9214396131965, 3394.479458388501, 1991.38423501761, 2509.536810987911, 3487.065728771696, 1603.475935403169, 4068.46601432129, 4306.841268525166, 1411.0899574513846], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.0467, -8.2639, -8.698, -8.6535, -7.7545, -9.0665, -8.875, -8.5479, -8.5442, -9.0104, -8.7745, -8.1582, -9.128, -9.4044, -9.3597, -9.0893, -9.3081, -9.4531, -9.4811, -9.4797, -9.1183, -9.4516, -9.4827, -9.0109, -9.2969, -8.9285, -8.9742, -9.5376, -9.6194, -9.5239, -8.9732, -8.5983, -9.3832, -7.9216, -8.7779, -5.6857, -7.9289, -8.9748, -8.2356, -8.035, -8.4072, -8.3825, -7.7065, -7.2312, -6.8998, -8.7419, -6.0945, -7.5221, -6.9986, -7.1769, -8.0735, -6.0024, -7.1766, -8.3605, -5.1208, -8.0764, -7.2166, -3.8854, -7.7527, -5.198, -7.1872, -6.0949, -6.258, -6.3259, -4.3199, -4.1781, -4.5059, -4.2836, -4.6905, -5.4558, -5.8228, -5.3184, -5.0083, -5.7956, -5.7276, -6.0768, -5.3886, -4.6609, -4.9787, -4.8405, -5.5164, -5.4974, -5.1642, -4.97, -4.9074, -5.5894, -5.9069, -5.8155, -5.6946, -5.5635, -5.5419, -5.3124, -5.6781, -5.6566, -5.6388, -5.6917, -8.5363, -8.7732, -8.7812, -8.7922, -8.9353, -8.4322, -8.0438, -8.9199, -8.8395, -8.9318, -8.9425, -9.0577, -8.8519, -8.8753, -9.1962, -9.1737, -9.3092, -9.1677, -8.8422, -8.9486, -8.0236, -8.9614, -8.7492, -9.3416, -9.3897, -9.0117, -9.4186, -9.3161, -8.5981, -8.8699, -8.2202, -8.2164, -8.527, -8.2441, -6.5128, -5.8091, -6.9647, -5.8058, -5.6936, -8.1226, -4.8604, -6.0724, -6.7254, -5.7586, -6.238, -6.1079, -5.2662, -5.2687, -8.0468, -6.9156, -6.5775, -7.6671, -6.0148, -7.7167, -6.6839, -6.6503, -5.4683, -5.6434, -5.9529, -5.1859, -6.3609, -6.7583, -6.8773, -5.3973, -5.6793, -5.9382, -4.3627, -5.2958, -5.5258, -4.2841, -4.1049, -5.5676, -4.1845, -5.6018, -5.1643, -4.1934, -5.2604, -5.2138, -5.438, -4.8755, -5.5057, -5.1645, -4.9381, -5.0856, -5.388, -5.2339, -5.3672, -5.4149, -5.4892, -5.519, -5.5966, -8.7097, -8.7471, -8.3174, -8.3935, -8.9161, -8.9453, -8.9647, -8.7911, -8.0635, -8.6274, -9.1265, -9.0848, -8.8972, -9.2082, -9.0143, -8.9643, -9.2709, -8.8757, -9.0301, -9.1926, -8.9227, -9.3026, -9.2556, -7.76, -9.2986, -9.1909, -9.1908, -9.2432, -9.3134, -9.4363, -8.6716, -8.7652, -8.8482, -8.9691, -8.1082, -8.1593, -9.0055, -9.0127, -8.0525, -8.4652, -7.7614, -8.8339, -5.2839, -7.0676, -4.173, -5.9336, -5.2183, -5.4845, -6.075, -4.1326, -5.2654, -4.4478, -5.7751, -6.2111, -5.2613, -5.7399, -4.3943, -6.0264, -6.1128, -4.1966, -5.0582, -6.0097, -6.0231, -5.6848, -4.7476, -6.027, -5.7767, -5.3527, -5.5155, -5.5258, -4.8208, -4.8963, -5.1358, -4.7852, -4.6009, -5.4524, -5.5349, -5.4056, -5.198, -4.9493, -5.5891, -5.5417, -5.4663, -5.5846, -5.6472, -8.1764, -8.1201, -8.4683, -8.4565, -8.4571, -8.6148, -8.3083, -8.8453, -7.2103, -8.1365, -8.8836, -7.99, -8.8803, -8.3398, -8.6237, -8.7088, -8.9024, -7.8136, -8.8566, -9.1156, -9.1683, -8.6966, -9.1462, -8.359, -8.402, -8.669, -9.2463, -8.1392, -8.8779, -9.2988, -6.6428, -7.8957, -5.6274, -7.2408, -7.9011, -8.517, -5.9828, -5.6672, -6.8917, -7.1973, -6.7478, -7.0288, -6.3548, -5.8938, -5.1917, -6.6899, -7.3295, -5.0487, -6.8509, -5.8172, -7.0499, -6.6312, -7.2388, -5.8596, -7.0422, -6.739, -6.5889, -5.9764, -6.4256, -5.906, -5.9662, -5.533, -5.72, -4.8172, -6.3989, -5.1928, -4.9471, -4.9902, -5.6267, -6.0757, -5.6658, -5.058, -6.0264, -5.7427, -5.7617, -5.2102, -4.7113, -5.1153, -4.1732, -5.9199, -5.136, -4.6908, -5.3123, -4.8958, -5.1061, -5.5479, -5.3997, -5.4203, -5.3599, -4.9384, -5.1676, -5.4833, -5.6087, -5.3498, -5.481, -5.5914, -5.5998, -5.6193, -6.7316, -6.5274, -7.1358, -7.465, -7.6869, -7.7011, -7.658, -7.3541, -7.8568, -7.8996, -6.906, -7.3334, -7.4847, -7.8978, -7.8141, -7.9877, -7.807, -7.8113, -8.0155, -7.6258, -8.1506, -8.1257, -8.1299, -7.952, -7.953, -6.7557, -8.2737, -8.2092, -8.2277, -7.7247, -8.1403, -6.3548, -7.4366, -7.3022, -6.6381, -7.6252, -6.2203, -6.0671, -4.5689, -5.4533, -6.2603, -7.046, -6.4374, -7.2789, -6.9351, -6.8543, -7.4957, -7.4488, -6.3723, -6.5909, -6.3769, -6.4367, -7.0466, -6.3599, -7.1752, -5.4417, -6.5874, -4.624, -5.8777, -5.2772, -5.1543, -4.9878, -6.5955, -5.3072, -5.2574, -5.9296, -5.5515, -6.4556, -5.9553, -6.0121, -5.942, -5.8729, -5.7507, -5.8067, -6.0768, -6.0781, -6.0244, -6.1571, -6.1443, -6.1268, -6.1878, -6.1475, -6.1791, -6.2468], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2944, 1.2917, 1.2797, 1.2793, 1.2755, 1.2641, 1.2601, 1.2553, 1.2505, 1.2483, 1.2457, 1.2454, 1.2452, 1.2444, 1.243, 1.2426, 1.2416, 1.2403, 1.2402, 1.2401, 1.2398, 1.2389, 1.2371, 1.2368, 1.2347, 1.2338, 1.2316, 1.2289, 1.2287, 1.2285, 1.2255, 1.2212, 1.2279, 1.2043, 1.2154, 1.1061, 1.1846, 1.2213, 1.1901, 1.1773, 1.1954, 1.1871, 1.1424, 1.0972, 1.0385, 1.2045, 0.9356, 1.0807, 1.0206, 1.03, 1.1233, 0.849, 0.9859, 1.157, 0.6687, 1.113, 0.9648, 0.345, 1.0589, 0.5448, 0.9506, 0.7158, 0.7386, 0.7461, 0.289, 0.2456, 0.2588, 0.1194, 0.2247, 0.4316, 0.5293, 0.3563, 0.2475, 0.4916, 0.4533, 0.5736, 0.3227, 0.0368, 0.1382, 0.0595, 0.3067, 0.2957, 0.1406, 0.0327, -0.0858, 0.2911, 0.4489, 0.3698, 0.2512, 0.0843, -0.0042, -0.3366, 0.1425, 0.0351, -0.147, -0.1557, 1.3756, 1.3736, 1.373, 1.3727, 1.3698, 1.3688, 1.3653, 1.3646, 1.3638, 1.3637, 1.3634, 1.3624, 1.3615, 1.3602, 1.3566, 1.355, 1.3503, 1.3498, 1.3488, 1.3435, 1.3397, 1.3396, 1.3395, 1.339, 1.3378, 1.3376, 1.3354, 1.3353, 1.3344, 1.3335, 1.3306, 1.3252, 1.3241, 1.3181, 1.2784, 1.2207, 1.2587, 1.206, 1.1885, 1.2997, 1.1184, 1.1785, 1.2159, 1.1474, 1.1779, 1.1433, 1.0419, 1.0165, 1.283, 1.165, 1.1227, 1.2379, 1.0513, 1.2397, 1.1207, 1.1116, 0.9556, 0.966, 0.9765, 0.8442, 1.0319, 1.0931, 1.1105, 0.8372, 0.8783, 0.9188, 0.613, 0.7599, 0.8073, 0.4135, 0.2981, 0.7525, 0.2392, 0.7417, 0.5649, 0.037, 0.5327, 0.4339, 0.5375, 0.0397, 0.537, 0.1402, -0.3291, -0.321, 0.1497, -0.4124, -0.4672, -0.4123, -0.2334, -0.0272, 0.2909, 1.3947, 1.3912, 1.3861, 1.3848, 1.3835, 1.3823, 1.3806, 1.3786, 1.3771, 1.3723, 1.3717, 1.3716, 1.3696, 1.3688, 1.3673, 1.3655, 1.3654, 1.3653, 1.364, 1.3634, 1.363, 1.3603, 1.3601, 1.3596, 1.3594, 1.3588, 1.3576, 1.3525, 1.3513, 1.35, 1.3476, 1.3481, 1.3457, 1.3479, 1.3101, 1.3041, 1.3475, 1.3472, 1.2634, 1.2916, 1.2147, 1.328, 0.8743, 1.0937, 0.6485, 0.8743, 0.7275, 0.7681, 0.8569, 0.4763, 0.6527, 0.4522, 0.7063, 0.7897, 0.574, 0.6607, 0.3033, 0.7047, 0.7093, 0.2064, 0.4335, 0.666, 0.6539, 0.5426, 0.2551, 0.6372, 0.5252, 0.3586, 0.4072, 0.3919, 0.0944, 0.0794, 0.1689, -0.0206, -0.1773, 0.3049, 0.3153, 0.1304, -0.0811, -0.7189, 0.14, -0.004, -0.2105, 0.0632, 0.0956, 1.4975, 1.4973, 1.4909, 1.4863, 1.4862, 1.4854, 1.4774, 1.4757, 1.4757, 1.4754, 1.4748, 1.4731, 1.4711, 1.4693, 1.467, 1.4662, 1.4661, 1.461, 1.4609, 1.4606, 1.46, 1.4592, 1.4578, 1.4561, 1.4558, 1.4547, 1.4543, 1.4528, 1.4525, 1.4524, 1.4391, 1.4511, 1.4274, 1.4371, 1.4392, 1.4513, 1.3963, 1.3793, 1.4066, 1.4139, 1.3935, 1.4046, 1.3702, 1.3435, 1.2971, 1.3751, 1.4068, 1.2759, 1.3539, 1.2697, 1.3572, 1.3097, 1.3685, 1.2204, 1.3475, 1.3062, 1.2722, 1.1859, 1.2395, 1.1604, 1.1662, 1.0737, 1.1006, 0.9354, 1.2003, 0.8984, 0.8176, 0.7999, 0.9587, 1.097, 0.9433, 0.7081, 1.0636, 0.9323, 0.9241, 0.667, 0.4056, 0.5764, 0.0572, 0.9658, 0.4, 0.0738, 0.5083, 0.1069, 0.1497, 0.6662, 0.4505, 0.4299, 0.2878, -0.5147, -0.2676, 0.274, 0.5707, -0.5283, -0.5658, 0.2317, 0.274, -0.0816, 3.3857, 3.3589, 3.3481, 3.3437, 3.3155, 3.312, 3.3098, 3.3093, 3.3064, 3.2999, 3.298, 3.295, 3.2945, 3.2938, 3.2885, 3.2843, 3.2821, 3.2818, 3.2809, 3.2726, 3.2683, 3.2666, 3.2662, 3.2545, 3.2525, 3.2498, 3.2479, 3.2465, 3.2465, 3.2452, 3.2452, 3.2031, 3.2302, 3.2152, 3.1712, 3.2238, 3.0942, 3.0411, 2.777, 2.8269, 2.9617, 3.0879, 2.9017, 3.1209, 3.0038, 2.9647, 3.1588, 3.1335, 2.6097, 2.6421, 2.4732, 2.4132, 2.8393, 2.1401, 2.8579, 1.0444, 2.1665, -0.3936, 1.1725, 0.2145, -0.2392, -0.5642, 2.0509, -0.4073, -0.8544, 0.5228, -0.4346, 1.6888, 0.1241, 0.2092, -0.2673, -0.6171, -1.1418, -1.1091, -0.429, -0.4304, -1.0217, -0.6211, -0.8395, -1.151, -0.4351, -1.3259, -1.4144, -0.3664]}, \"token.table\": {\"Topic\": [4, 3, 1, 2, 1, 2, 5, 5, 2, 4, 5, 3, 4, 3, 2, 2, 1, 2, 3, 5, 4, 3, 1, 2, 3, 4, 5, 4, 1, 4, 1, 5, 1, 5, 3, 3, 2, 3, 4, 5, 3, 4, 3, 1, 1, 3, 5, 2, 1, 3, 3, 1, 1, 3, 4, 5, 5, 3, 3, 2, 5, 4, 5, 3, 4, 4, 4, 5, 3, 5, 5, 3, 4, 2, 3, 4, 4, 3, 2, 4, 1, 4, 3, 4, 1, 3, 3, 1, 2, 3, 1, 2, 3, 4, 5, 1, 4, 3, 5, 3, 4, 2, 3, 5, 2, 1, 3, 1, 2, 3, 4, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 2, 3, 1, 2, 4, 5, 1, 2, 5, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 1, 5, 1, 3, 4, 5, 4, 2, 1, 2, 3, 4, 5, 1, 3, 4, 5, 2, 4, 5, 3, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 2, 4, 5, 1, 3, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 4, 2, 4, 5, 1, 2, 4, 5, 1, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 1, 2, 1, 2, 3, 4, 5, 2, 4, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 2, 1, 2, 3, 4, 5, 4, 5, 1, 2, 3, 4, 5, 4, 2, 1, 2, 3, 4, 5, 1, 1, 4, 5, 3, 5, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 2, 4, 5, 3, 4, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 3, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 2, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 1, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 2, 3, 5, 5, 1, 3, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 1, 4, 5, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 2, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 5, 1, 2, 4, 5, 1, 2, 3, 5, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1], \"Freq\": [0.8991204758374476, 0.9578881201277272, 0.9231416098707395, 0.05128564499281885, 0.9115130901643157, 0.9381734127443181, 0.7617654534669422, 0.9182173262679616, 0.9084632587976588, 0.05343901522339169, 0.8742447569495126, 0.9432883743058081, 0.9359671152359406, 0.9328812141274144, 0.909471113456156, 0.9583450399755592, 0.8727289638339906, 0.05950424753413572, 0.03966949835609048, 0.01983474917804524, 0.9476363136941746, 0.9681935430537021, 0.041230418031870955, 0.015857853089181136, 0.05708827112105209, 0.8816966317584711, 0.0031715706178362273, 0.9350507536099888, 0.042219705103559865, 0.949943364830097, 0.9384901591519965, 0.910630456319169, 0.9042941235704185, 0.0531937719747305, 0.92845755574746, 0.9055260352446584, 0.07805548216899506, 0.07805548216899506, 0.07805548216899506, 0.7805548216899506, 0.04224253317878418, 0.929335729933252, 0.96803954005288, 0.9067235272307671, 0.04883632900565629, 0.9278902511074695, 0.8613406718181095, 0.9272845264908935, 0.9233397710799729, 0.03551306811846049, 0.9306884542647512, 0.8903726503291209, 0.022690027203845466, 0.04538005440769093, 0.9302911153576641, 0.022690027203845466, 0.9069842018240443, 0.9261777369819492, 0.9053452619447706, 0.9648573366226807, 0.03573545691195114, 0.16780918777080986, 0.7551413449686443, 0.8359816374661317, 0.15399661742797163, 0.9583500534502192, 0.0817368848163888, 0.8991057329802769, 0.937476850364344, 0.062498456690956265, 0.9010123554973852, 0.03602775650012155, 0.9367216690031603, 0.9715469826908963, 0.9228401990371309, 0.05126889994650727, 0.9572390108421225, 0.9096484056487917, 0.03519455859613864, 0.9502530820957432, 0.9366581508251436, 0.04803375132436634, 0.03782732875750399, 0.9456832189375998, 0.8956339804678386, 0.9299965847155045, 0.9317481112989621, 0.055651959504677086, 0.918257331827172, 0.027825979752338543, 0.055649613028657875, 0.8180493115212708, 0.10016930345158417, 0.02225984521146315, 0.005564961302865787, 0.9344472759381957, 0.044497489330390265, 0.9110122758347916, 0.7478172029045975, 0.92377092648479, 0.9471717230086348, 0.0444506859709356, 0.9334644053896476, 0.7987925591907173, 0.9618134048148845, 0.034935572765362875, 0.9432604646647976, 0.9092403742140588, 0.04132910791882086, 0.04132910791882086, 0.04132910791882086, 0.1070847458383839, 0.8566779667070712, 0.24542928765906527, 0.2276874114427473, 0.3577945036957457, 0.15524141689278223, 0.01404565200458506, 0.06719559786998959, 0.043479504504110915, 0.15020192465056498, 0.7233408476592997, 0.015810728910585787, 0.04548482361933422, 0.009096964723866844, 0.07504995897190148, 0.866485889948317, 0.004548482361933422, 0.9200755481703989, 0.30972288260760183, 0.10947101885268687, 0.20559191345504607, 0.3624558733963961, 0.012682618037811283, 0.1025808400471918, 0.8548403337265984, 0.03890242025184608, 0.27231694176292254, 0.11670726075553824, 0.5446338835258451, 0.040760562022741566, 0.937492926523056, 0.040760562022741566, 0.14630585135597982, 0.804682182457889, 0.16900440969332203, 0.176273416561852, 0.48338895675724364, 0.12902487191640713, 0.03997953777691489, 0.8202284867269511, 0.09764624841987514, 0.058587749051925084, 0.019529249683975028, 0.05293597352689643, 0.8999115499572392, 0.05293597352689643, 0.29095083624016416, 0.12851185346297003, 0.5130193190241763, 0.05243283621289177, 0.014393327587852642, 0.024923165663579334, 0.024923165663579334, 0.9470802952160147, 0.024923165663579334, 0.10697181642895975, 0.7395008179219392, 0.13022655913090753, 0.018603794161558218, 0.009301897080779109, 0.08276011412166832, 0.823127621534431, 0.06262927555153279, 0.02013083857013554, 0.011183799205630856, 0.24007544145492074, 0.12541254404361532, 0.5715228792844755, 0.04299858652923953, 0.01970768549256812, 0.04359460516946826, 0.9590813137283017, 0.9786318302771397, 0.8464954378669998, 0.1417965886311161, 0.23632764771852685, 0.17330694166025304, 0.4411449424079168, 0.9454834443730167, 0.9275535399933192, 0.20381047116565792, 0.5100891902654312, 0.1610215324678955, 0.09571209971867914, 0.030402666969462783, 0.018079257003818846, 0.9220421071947612, 0.018079257003818846, 0.03615851400763769, 0.025489195193062832, 0.9431002221433248, 0.025489195193062832, 0.13153871852030508, 0.8330785506285988, 0.9101874844044787, 0.5625322162287419, 0.28126610811437097, 0.11683361413981563, 0.034617367152537964, 0.0043271708940672456, 0.023066736979953987, 0.9226694791981594, 0.023066736979953987, 0.023066736979953987, 0.9595862413654909, 0.7600979572417448, 0.17199296842696415, 0.022192641087350213, 0.03883712190286287, 0.011096320543675106, 0.24990070794640248, 0.2719045438662115, 0.45736544661888756, 0.012573620525605157, 0.007858512828503222, 0.9510493945041738, 0.024149395902372542, 0.9418264401925291, 0.024149395902372542, 0.7871762561698821, 0.05391618192944398, 0.11861560024477676, 0.021566472771777593, 0.021566472771777593, 0.04419386200572903, 0.06739563955873676, 0.09943618951289031, 0.7811265109512605, 0.0066290793008593535, 0.9499216608092387, 0.03515302533807537, 0.08592961749307312, 0.06900408677474054, 0.798103871564452, 0.01171767511269179, 0.8142431056744397, 0.09693370305648091, 0.03877348122259237, 0.03877348122259237, 0.019386740611296183, 0.13770609961805672, 0.07031800831560343, 0.14063601663120687, 0.6372569503601561, 0.014649585065750714, 0.9196462994343511, 0.8407079209520532, 0.9312478573397605, 0.03581722528229848, 0.08133116254196972, 0.19558208135092717, 0.13167902506795098, 0.5731910502957865, 0.01936456250999279, 0.2281877138327148, 0.6308719147139762, 0.10372168810577946, 0.01952408246697025, 0.01830382731278461, 0.9062978693061098, 0.13448566696726408, 0.7427987419703539, 0.10477371728844992, 0.010946507776405216, 0.007818934126003726, 0.10235294497495438, 0.7804412054340272, 0.08955882685308508, 0.01919117718280395, 0.006397059060934649, 0.9344220232887548, 0.07341420724510776, 0.8809704869412932, 0.024471402415035922, 0.2801650165071111, 0.15021613651019575, 0.46137813356702984, 0.10491285724521608, 0.004768766238418913, 0.08714299383467701, 0.0261428981504031, 0.043571496917338505, 0.8365727408128992, 0.0087142993834677, 0.8126634052805038, 0.061698957016957996, 0.09342984919710783, 0.024679582806783198, 0.007051309373366628, 0.28524349656353243, 0.15205897486891012, 0.3742990150619326, 0.1669458675134188, 0.0215328268608072, 0.02522447604267456, 0.01605193929988381, 0.043569549528256056, 0.9080811375362842, 0.0068794025570930615, 0.04345257719634917, 0.05793676959513223, 0.0506946733957407, 0.8473252553288089, 0.007242096199391528, 0.975683187739079, 0.18817254054398272, 0.6077973059570642, 0.1467745816243065, 0.03575278270335672, 0.020698979459838097, 0.7220521115147511, 0.21522707170151237, 0.04165685258738949, 0.01388561752912983, 0.01388561752912983, 0.6287173845144818, 0.19647418266077557, 0.15530816343661308, 0.005613548076022159, 0.013098278844051705, 0.7459666458254273, 0.13234892103354357, 0.060158600469792524, 0.04211102032885477, 0.02406344018791701, 0.9413455726865427, 0.21872804609220853, 0.06299367727455606, 0.17498243687376683, 0.5354462568337265, 0.007874209659319507, 0.24024255236114092, 0.32465209778532556, 0.15843022371923887, 0.18440239154206492, 0.09220119577103246, 0.20274995171239388, 0.3491804723935672, 0.03379165861873231, 0.16895829309366156, 0.24780549653737027, 0.9106563626457993, 0.053568021332105836, 0.25410985836992755, 0.11314841952292794, 0.17193502854880668, 0.4424798528829584, 0.017699194115318336, 0.01561961890177105, 0.24210409297745128, 0.01561961890177105, 0.17181580791948153, 0.5466866615619868, 0.011623318136238183, 0.03486995440871455, 0.03486995440871455, 0.9182421327628164, 0.9295368130640217, 0.3133569722110919, 0.23464993928483172, 0.35234269880073477, 0.08164935191415774, 0.01765391392738546, 0.26008863302119845, 0.5646227919384246, 0.13086738180497012, 0.03127648118609349, 0.013169044709934099, 0.07831108634505278, 0.7987730807195383, 0.10963552088307389, 0.1091631023758018, 0.25471390554353757, 0.6185909134628769, 0.06206212900334554, 0.9309319350501831, 0.36535760689861774, 0.20074593785638337, 0.14186046275184425, 0.2750219348632452, 0.018067134407074505, 0.9309638307973239, 0.10324516014548046, 0.6879388039167277, 0.17388658550817762, 0.024996196666800533, 0.009781120434834992, 0.07557756850775581, 0.037788784253877904, 0.018894392126938952, 0.8628439071302121, 0.00629813070897965, 0.12272677200959538, 0.06443155530503757, 0.030681693002398844, 0.26386255982063006, 0.5215887810407803, 0.524108679151104, 0.207055280652288, 0.0698811572201472, 0.181173370570752, 0.0168232415529984, 0.6976448835712002, 0.16933128242019424, 0.09482551815530876, 0.006773251296807769, 0.03386625648403885, 0.9640524502859531, 0.34571169459893775, 0.13767949325384393, 0.4932254373709134, 0.018155537571935464, 0.006051845857311821, 0.015357231472074805, 0.8446477309641143, 0.12285785177659844, 0.03071446294414961, 0.1500055409852896, 0.3068295156517287, 0.10909493889839243, 0.259100479883682, 0.17727927570988772, 0.3610461195303255, 0.41364456500147007, 0.2045495101655621, 0.012987270486702356, 0.007792362292021414, 0.9025190205900164, 0.047578103260560785, 0.02039061568309748, 0.04078123136619496, 0.8767964743731915, 0.013593743788731653, 0.05604850770101223, 0.028024253850506116, 0.11209701540202446, 0.7846791078141713, 0.4565542710493831, 0.2532626926467971, 0.21124152839598323, 0.05905677137952219, 0.02044272855444999, 0.9241888177014639, 0.10810492301931712, 0.10810492301931712, 0.7567344611352199, 0.017874230772070253, 0.14299384617656202, 0.16086807694863228, 0.6792207693386696, 0.05987061347480426, 0.8980592021220639, 0.8681882109601506, 0.17944399876736958, 0.14575527869226956, 0.3265743272586228, 0.3348246260525249, 0.013062973090344912, 0.22307104374714629, 0.13574478243527896, 0.2732187977678226, 0.3302834833775577, 0.0363138908425587, 0.1829552624375718, 0.19360676744249894, 0.3233045048554351, 0.2869640760150955, 0.013784300594611575, 0.056996372472344424, 0.04956206301942993, 0.17842342686994778, 0.6988250885739621, 0.01486861890582898, 0.025496895559254523, 0.050993791118509046, 0.8668944490146537, 0.025496895559254523, 0.025496895559254523, 0.2788109325272135, 0.36697077567834174, 0.322673711631962, 0.021062819669111922, 0.010639981069963755, 0.9808230731216193, 0.14486606320938764, 0.5438049142013935, 0.2919608350835351, 0.007800480326659334, 0.011143543323799049, 0.3238251756844393, 0.12355175933806299, 0.34773841942729017, 0.16539993588805207, 0.03985540623808483, 0.24207998595090774, 0.1530475291118201, 0.4576709764786159, 0.13906722597179808, 0.008093859712644333, 0.9473643852794358, 0.12923582333206335, 0.5603584527288684, 0.288761292757579, 0.010096548697817448, 0.010096548697817448, 0.861256175080676, 0.08074276641381338, 0.05382851094254225, 0.026914255471271124, 0.34417519321268014, 0.1923889999546073, 0.19314793092089766, 0.25310347725783644, 0.017455412224678373, 0.25132863108272574, 0.12423088228423705, 0.21979309942595787, 0.3860713572525521, 0.018156821256926955, 0.9096045714759429, 0.043314503403616325, 0.043314503403616325, 0.9587598513527122, 0.03306068452940387, 0.3084439714447299, 0.12383992429526093, 0.21960946575026272, 0.32693740013948885, 0.02113534707972453, 0.029577956941427473, 0.9464946221256791, 0.8839961024593872, 0.024555447290538535, 0.07366634187161561, 0.024555447290538535, 0.3027522186972391, 0.3270112105800307, 0.2927251687190186, 0.06372028534546592, 0.013746762066915238, 0.05278548022393145, 0.9501386440307661, 0.24653026385605006, 0.16074854692109347, 0.45545421627643146, 0.12854967895983468, 0.008602750982015706, 0.9806957836679808, 0.2007939463866422, 0.0989627307191308, 0.11617364040941441, 0.5693942622535496, 0.015776667216093315, 0.803114910046533, 0.016390100205031285, 0.10653565133270336, 0.0573653507176095, 0.016390100205031285, 0.17400107805008563, 0.004640028748002284, 0.1160007187000571, 0.6960043122003425, 0.009280057496004569, 0.9815079595911471, 0.9018883795404418, 0.05305225762002599, 0.9595363420991806, 0.16156867378183595, 0.06117648813098643, 0.19764711550011, 0.553725648980467, 0.026666674313506906, 0.9664524688128792, 0.9334687708996349, 0.11080655924881563, 0.2129224079683124, 0.5236153093914622, 0.14774207899842084, 0.004345355264659436, 0.961727268664812, 0.9292003338067708, 0.6845832389762592, 0.10067400573180281, 0.19016201082673864, 0.013423200764240376, 0.011186000636866979, 0.9567698991676417, 0.9560247403555704, 0.084936811731021, 0.84936811731021, 0.9123408350346308, 0.7971274402954248, 0.9423308408883689, 0.03624349388032188, 0.8957126089367605, 0.9548875591142162, 0.11353802756146537, 0.07225147208456888, 0.15224417332105586, 0.652843658478426, 0.010321638869224126, 0.22136472817964079, 0.01814464985079023, 0.0870943192837931, 0.6550218596135273, 0.01814464985079023, 0.29217187775162395, 0.12326001092646637, 0.3081500273161659, 0.17347705241502673, 0.10499926856698986, 0.9513834122765742, 0.2781306607328969, 0.1343711419996485, 0.180726252121798, 0.3878573138068457, 0.01877675346719979, 0.013805367834908489, 0.28991272453307826, 0.3313288280378037, 0.3589395637076207, 0.029186845217959095, 0.32105529739755007, 0.01945789681197273, 0.3599710910214955, 0.2724105553676182, 0.0590287178065867, 0.8559164081955072, 0.0590287178065867, 0.014757179451646675, 0.007378589725823338, 0.8804839129845897, 0.035938118897330194, 0.05390717834599529, 0.017969059448665097, 0.017969059448665097, 0.02197042897049705, 0.15379300279347938, 0.10985214485248526, 0.7250241560264027, 0.09970780761763544, 0.14956171142645316, 0.6979546533234481, 0.02899609393957787, 0.9278750060664919, 0.33631596620102455, 0.2526418494094374, 0.26181901060593404, 0.12389167615270487, 0.025642068049034777, 0.9375701305624629, 0.4139114746315245, 0.32470641544369594, 0.1705600731671282, 0.074932249717776, 0.016413730890560457, 0.26678378507495654, 0.18577656288620456, 0.4028759183520599, 0.1306916517978532, 0.014041251846050344, 0.4206113751667875, 0.12880510946842003, 0.21771483104839134, 0.20859588524531736, 0.023937232733069205, 0.22999077322511285, 0.15617277395853732, 0.2711681605710938, 0.3254017926853125, 0.01757571411108941, 0.42301422017459006, 0.20672188135228833, 0.2162923388223017, 0.1425998163031989, 0.011484548964016019, 0.23203969645594483, 0.23636341750791895, 0.3675162894178008, 0.12442708360681098, 0.04035472981842519, 0.383616197101369, 0.20017266224687977, 0.1915196653197812, 0.20017266224687977, 0.02480525785768251, 0.9601315421508082, 0.9267963841546177, 0.3332628891194323, 0.5147426802240737, 0.11548713979386267, 0.020897672915079914, 0.015398285305848356, 0.24826820044715742, 0.6375644910891498, 0.08373542855318326, 0.01909755388055057, 0.011752340849569581, 0.34134947183313796, 0.4153864577330195, 0.19717218350178975, 0.031173467747318537, 0.014028060486293343, 0.04332382697989887, 0.03249287023492416, 0.06498574046984831, 0.8556455828530027, 0.010830956744974718, 0.0670459388284174, 0.8715972047694261, 0.0335229694142087, 0.004788995630601243, 0.019155982522404972, 0.8810522201224488, 0.9473453718744674, 0.906703676777403, 0.8970403590464865, 0.054460892154826905, 0.002367864876296822, 0.16338267646448074, 0.7766596794253576, 0.004735729752593644, 0.24511693329097123, 0.0812347185258262, 0.23734665586676176, 0.4245397029045352, 0.012008610564687352, 0.3710213380685564, 0.18610909054729202, 0.3411002624178664, 0.0885663839260425, 0.013165273286303616, 0.2586696979945225, 0.21355289020478022, 0.41006165302232445, 0.09123621130814553, 0.02707008467384538, 0.9193886561681166, 0.042776516322242775, 0.7897203013337128, 0.14149155398895688, 0.026324010044457093, 0.0032905012555571366, 0.9572114558763047, 0.041286432677400516, 0.03302914614192041, 0.08532529419996106, 0.8367383689286504, 0.005504857690320069, 0.9785509428564202, 0.19185184680635556, 0.4479410832758257, 0.2578098808354464, 0.09205447357973115, 0.010323866195857699, 0.9770759263615723, 0.9097570883776556, 0.027568396617504713, 0.055136793235009425, 0.75477386483457, 0.05080208705617298, 0.16692114318456838, 0.021772323024074135, 0.007257441008024712, 0.9357130096144642, 0.1352131896272141, 0.2704263792544282, 0.5859238217179278, 0.9628239650223221, 0.7706094147204373, 0.1727227998511325, 0.02657273843863577, 0.013286369219317884, 0.07703407293841846, 0.04108483890048985, 0.06676286321329601, 0.7754763342467459, 0.03081362917536739, 0.04939266713214997, 0.13582983461341241, 0.8026308408974371, 0.006174083391518747, 0.9710226870545516, 0.3592967247217244, 0.18567207470826785, 0.18708941879001037, 0.24520052614145296, 0.022677505307880045, 0.0991045424967577, 0.7345395502700864, 0.13991229528954027, 0.017489036911192534, 0.005829678970397512, 0.07525325506513493, 0.0846599119482768, 0.09406656883141866, 0.7384225653266365, 0.009406656883141866, 0.3586854322747113, 0.174672850398599, 0.3835251436510763, 0.07273067490999685, 0.010333319932567859, 0.9053971623982173, 0.04765248223148512, 0.15346706245032754, 0.7896577940625945, 0.03627403294280469, 0.011161240905478367, 0.008370930679108775, 0.12340789004468879, 0.6453625725287824, 0.19826185613736888, 0.012138480988002177, 0.022253881811337323, 0.2899681896777176, 0.06904004516136134, 0.1242720812904504, 0.1242720812904504, 0.38662425290362346, 0.3892943674118388, 0.2633744609156638, 0.2893274187431197, 0.04229370905215039, 0.01634075122469447, 0.22404132553239403, 0.3745252233837264, 0.19877350686332704, 0.18136678733574757, 0.02133726909832324, 0.15676728301506004, 0.061756808460478194, 0.13538992624027912, 0.6318196557879692, 0.01187630931932273, 0.09192934405679692, 0.0408574862474653, 0.14300120186612855, 0.7150060093306427, 0.010214371561866324, 0.9636040578187416, 0.047711926104661774, 0.007951987684110295, 0.03975993842055148, 0.8906226206203531, 0.01590397536822059, 0.09014386662321366, 0.16641944607362524, 0.7142167893993083, 0.027736574345604205, 0.006934143586401051, 0.09504714073584587, 0.8554242666226128, 0.23231292535747866, 0.17669861898402164, 0.292855081662761, 0.286519274607557, 0.011263656987029268, 0.02174785513447331, 0.02174785513447331, 0.054369637836183274, 0.8916620605134057, 0.9483128968557257, 0.026601184933595222, 0.026601184933595222, 0.07093649315625392, 0.8512379178750471, 0.026601184933595222, 0.08964200975124655, 0.01792840195024931, 0.026892602925373963, 0.8560811931244046, 0.008964200975124655, 0.8734103490429406, 0.25340700125093607, 0.42701516316563987, 0.27411815039163756, 0.026193512148534256, 0.018883694804757253, 0.4396022087648503, 0.1138920976703123, 0.11921415830911194, 0.31932363832797844, 0.008515297022079425, 0.057305744741569076, 0.7647352832754218, 0.16006087324369295, 0.013832421144516674, 0.0039521203270047635, 0.9466082933626236, 0.094491902310538, 0.15904577616625207, 0.570693667420081, 0.16372359311231832, 0.012162324059772217, 0.9324572393920735, 0.3434182922481763, 0.3082509382823775, 0.19944677742105624, 0.13026780412683225, 0.018491754197978723, 0.24828169280506387, 0.5844383667714705, 0.11995632349008704, 0.025107137474669382, 0.023712296503854414, 0.10681425782187322, 0.7222678386050475, 0.1525917968883903, 0.010172786459226021, 0.010172786459226021, 0.08865684157912278, 0.004221754360910608, 0.1519831569927819, 0.7514722762420882, 0.004221754360910608, 0.052476015437562605, 0.7477832199852672, 0.17492005145854203, 0.0087460025729271, 0.0174920051458542, 0.12746345629427216, 0.6713075364831668, 0.17632444787374316, 0.013808541098546151, 0.011684150160308281, 0.14581757225121666, 0.10415540875086905, 0.24366053198688153, 0.4936335129889673, 0.011993653128887952, 0.11124954999593671, 0.011124954999593671, 0.055624774997968356, 0.8065592374705411, 0.011124954999593671, 0.049345213164923296, 0.9375590501335427, 0.949673597103036, 0.8806825622490447, 0.08752119676838686, 0.8314513692996752, 0.04376059838419343, 0.014586866128064478, 0.5497277862918251, 0.10240027391710467, 0.2730673971122791, 0.06108086514353612, 0.014371968269067323, 0.477509910610024, 0.10101171185981278, 0.15151756778971917, 0.2540597601322564, 0.018365765792693233, 0.21090355157119775, 0.1804847700945827, 0.5617334979348247, 0.020279187651076707, 0.02839086271150739, 0.3489734246095597, 0.12795692235683856, 0.48210032362728067, 0.021972400808750056, 0.018094918313088282, 0.2775094124290875, 0.16084940465635006, 0.30726360633072003, 0.24274708688064553, 0.011783839168963375, 0.9493742175533577, 0.8773632740905298, 0.9252648090978837, 0.10611030017734742, 0.6816782920484138, 0.18971235486253024, 0.012861854566951204, 0.009646390925213402, 0.8413299724808262, 0.027584589261666435, 0.11033835704666574, 0.013792294630833217, 0.06503764630191305, 0.19511293890573916, 0.7154141093210435, 0.07717992307356755, 0.7968827057345851, 0.11191088845667296, 0.007717992307356755, 0.005788494230517567, 0.05628864771476429, 0.9006183634362286, 0.028144323857382143, 0.11721778307701487, 0.7619155900005966, 0.10047238549458418, 0.013954497985358912, 0.01116359838828713, 0.25334203946560835, 0.10702685089723561, 0.4226883225308546, 0.20795723560412238, 0.008806006719392804, 0.11458763385664182, 0.1475511997606073, 0.4630596162699909, 0.26684791446067274, 0.006278774457898182, 0.13914768951546716, 0.6934193194187447, 0.12291379240532933, 0.016233897110137837, 0.02782953790309343, 0.22498961493020975, 0.1520619466424866, 0.4499792298604195, 0.16447516677656712, 0.007758262583800336, 0.8878328526115403, 0.2059102764763663, 0.06528862424860395, 0.3201653689114232, 0.3540652315020445, 0.055244220518049496, 0.05891947760712696, 0.9427116417140313, 0.022308582316879263, 0.8700347103582913, 0.022308582316879263, 0.022308582316879263, 0.0669257469506378, 0.2953130897145368, 0.11447529882192718, 0.48112806751244763, 0.0929074888989554, 0.016590623017670607, 0.10191916482805619, 0.8153533186244495, 0.3792882782611615, 0.25190620848992107, 0.11608643751588474, 0.23081195548293143, 0.02191080473629243, 0.06150832455854134, 0.8105918486464913, 0.10324611622326582, 0.004393451754181524, 0.01977053289381686, 0.037571232814792785, 0.901709587555027, 0.037571232814792785, 0.26195405877908273, 0.09419345420665179, 0.2832679060813178, 0.34239664375848605, 0.01856367345678539, 0.03892179408680065, 0.03892179408680065, 0.9341230580832157, 0.8714269234311244, 0.8789890629653475, 0.11341794360843194, 0.9617590110557221, 0.23222317359280775, 0.5193354609439155, 0.21786755922525236, 0.019422301791398467, 0.011822270655633849, 0.060751167080430594, 0.08100155610724079, 0.08100155610724079, 0.32400622442896315, 0.44550855858982435, 0.036407722107031054, 0.009101930526757764, 0.06371351368730435, 0.8919891916222609, 0.009101930526757764, 0.03696933721392713, 0.06161556202321188, 0.8872640931342511, 0.19395364353990377, 0.05799899951514807, 0.17150241792113677, 0.5556678340644832, 0.021203935306613273, 0.1150168693786511, 0.8051180856505578, 0.16489698144263865, 0.07293520333039787, 0.2029501310063245, 0.5485995728764709, 0.00951328739092146, 0.022544442135274553, 0.9243221275462566, 0.04508888427054911, 0.910127574745762, 0.07281020597966097, 0.3480509047210178, 0.17599905655672549, 0.2333496726114896, 0.23474280498528952, 0.007894416784866314, 0.025620188393613626, 0.012810094196806813, 0.03202523549201703, 0.9223267821700906, 0.006405047098403407, 0.707523095195903, 0.1698055428470167, 0.10612846427938544, 0.014150461903918058, 0.007075230951959029, 0.03241627330775487, 0.9400719259248914, 0.10694054213636747, 0.8555243370909398, 0.30922040936092815, 0.279334416188158, 0.2821237755509499, 0.1155591736013778, 0.013946796813959389, 0.02251657105536744, 0.06754971316610231, 0.31523199477514413, 0.5854308474395534, 0.8372358911124249, 0.9176525359920166, 0.0820432301410669, 0.2461296904232007, 0.6563458411285352, 0.04874647608860279, 0.024373238044301394, 0.8774365695948503, 0.024373238044301394, 0.9171903078265456, 0.950521546414511, 0.26759370900577545, 0.28167758842713203, 0.2374139673885827, 0.20069528175433157, 0.012574892340496966, 0.1262012270879961, 0.7426456824793618, 0.10193176034030456, 0.01941557339815325, 0.009707786699076624, 0.9275344823768374, 0.025081896206849526, 0.012540948103424763, 0.012540948103424763, 0.6646702494815124, 0.28844180637876954, 0.9163464668003437, 0.8837973237761941, 0.4631731150782058, 0.08460628902095226, 0.26184720105754566, 0.17415309126940537, 0.0160566679893778, 0.566915600645622, 0.09565242644638067, 0.3172860974806773, 0.011664930054436667, 0.009331944043549334, 0.2925733347214171, 0.1420749590873676, 0.2538256186066805, 0.29088865141208076, 0.02133932191826075, 0.011717656274211202, 0.023435312548422405, 0.011717656274211202, 0.9491301582111075, 0.15859872074896725, 0.0939844271104991, 0.17328378748498272, 0.55950104264219, 0.013216560062413937, 0.21253246497975242, 0.4150634021957518, 0.2150328469206907, 0.13835446739858392, 0.019169594880526688, 0.8791207049039274, 0.08791207049039274, 0.19877376783229597, 0.09258843257334959, 0.20978078429206481, 0.4849561957862856, 0.014244374242053784, 0.04005654054677913, 0.24033924328067477, 0.020028270273389565, 0.06008481082016869, 0.6208763784750765, 0.23941690408789812, 0.1019005831448492, 0.2196303830888983, 0.4244208754285467, 0.014839890749249884, 0.966623995861093, 0.8914870821876136, 0.05458084176658859, 0.03638722784439239, 0.018193613922196195, 0.018193613922196195, 0.9701473614032812], \"Term\": [\"Acura\", \"Ale\", \"Alfredo\", \"Alfredo\", \"Ami\", \"Asia\", \"Audrey\", \"Billy\", \"Bouchon\", \"Bouchon\", \"Britney\", \"Cajun\", \"Care\", \"Centre\", \"Cesar\", \"Cirque\", \"Coffee\", \"Coffee\", \"Coffee\", \"Coffee\", \"Cox\", \"Decor\", \"Dr\", \"Dr\", \"Dr\", \"Dr\", \"Dr\", \"Earl\", \"Ford\", \"Ford\", \"George\", \"Goodyear\", \"Holy\", \"Holy\", \"ID\", \"Indian\", \"Jeremy\", \"Jeremy\", \"Jeremy\", \"Jeremy\", \"Johnny\", \"Johnny\", \"Josh\", \"Juicy\", \"Kevin\", \"Kevin\", \"Killer\", \"Little\", \"Man\", \"Man\", \"Masala\", \"Miami\", \"Mike\", \"Mike\", \"Mike\", \"Mike\", \"Mont\", \"Nancy\", \"Nick\", \"Noodles\", \"Noodles\", \"Nordstrom\", \"Nordstrom\", \"North\", \"North\", \"PA\", \"PT\", \"PT\", \"Pasta\", \"Pasta\", \"Pastor\", \"Peoria\", \"Peoria\", \"Picasso\", \"Pineapple\", \"Pineapple\", \"Pools\", \"Pretty\", \"Pro\", \"Pro\", \"Robert\", \"Robert\", \"Salon\", \"Salon\", \"Snooze\", \"Station\", \"TV\", \"Tacos\", \"Tacos\", \"Tacos\", \"Thai\", \"Thai\", \"Thai\", \"Thai\", \"Thai\", \"Tim\", \"Tim\", \"Valentine\", \"Vous\", \"Wait\", \"Wish\", \"Yummy\", \"Yummy\", \"abordable\", \"accent\", \"acrylic\", \"acrylic\", \"airline\", \"airline\", \"airline\", \"airline\", \"alley\", \"alley\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"answer\", \"answer\", \"answer\", \"answer\", \"answer\", \"appointment\", \"appointment\", \"appointment\", \"appointment\", \"appointment\", \"asado\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"attach\", \"attach\", \"au\", \"au\", \"au\", \"au\", \"audience\", \"audience\", \"audience\", \"avec\", \"avec\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"bakery\", \"bakery\", \"bakery\", \"bakery\", \"baklava\", \"baklava\", \"baklava\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"battery\", \"battery\", \"battery\", \"battery\", \"bean\", \"bean\", \"bean\", \"bean\", \"bean\", \"beef\", \"beef\", \"beef\", \"beef\", \"beef\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"bento\", \"bento\", \"bf\", \"bi\", \"bike\", \"bike\", \"bike\", \"bike\", \"billing\", \"biryani\", \"bit\", \"bit\", \"bit\", \"bit\", \"bit\", \"boba\", \"boba\", \"boba\", \"boba\", \"bouncer\", \"bouncer\", \"bouncer\", \"bowling\", \"bowling\", \"breakdown\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"breast\", \"breast\", \"breast\", \"breast\", \"brioche\", \"brunch\", \"brunch\", \"brunch\", \"brunch\", \"brunch\", \"burger\", \"burger\", \"burger\", \"burger\", \"burger\", \"burro\", \"cabbage\", \"cabbage\", \"cabbage\", \"cafe\", \"cafe\", \"cafe\", \"cafe\", \"cafe\", \"call\", \"call\", \"call\", \"call\", \"call\", \"cannoli\", \"car\", \"car\", \"car\", \"car\", \"car\", \"caramel\", \"caramel\", \"caramel\", \"caramel\", \"caramel\", \"care\", \"care\", \"care\", \"care\", \"care\", \"caring\", \"centre\", \"chai\", \"chai\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"cheese\", \"cheese\", \"cheese\", \"cheese\", \"cheese\", \"cheesesteak\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"chinese\", \"chive\", \"chorizo\", \"chorizo\", \"chorizo\", \"clean\", \"clean\", \"clean\", \"clean\", \"clean\", \"client\", \"client\", \"client\", \"client\", \"client\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"come\", \"come\", \"come\", \"come\", \"come\", \"company\", \"company\", \"company\", \"company\", \"company\", \"contact\", \"contact\", \"contact\", \"contact\", \"contact\", \"contractor\", \"cook\", \"cook\", \"cook\", \"cook\", \"cook\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"cream\", \"cream\", \"cream\", \"cream\", \"cream\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"curly\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"d\", \"d\", \"d\", \"d\", \"d\", \"dance\", \"dance\", \"dance\", \"dance\", \"dance\", \"dangerous\", \"dangerous\", \"day\", \"day\", \"day\", \"day\", \"day\", \"de\", \"de\", \"de\", \"de\", \"de\", \"dealership\", \"dealership\", \"dealership\", \"dealership\", \"dedicated\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"dentist\", \"dentist\", \"dentist\", \"des\", \"des\", \"des\", \"develop\", \"develop\", \"didn\", \"didn\", \"didn\", \"didn\", \"didn\", \"disgust\", \"dish\", \"dish\", \"dish\", \"dish\", \"dish\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"don\", \"don\", \"don\", \"don\", \"don\", \"donut\", \"donut\", \"donut\", \"donut\", \"donut\", \"drama\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"dumpling\", \"dumpling\", \"dumpling\", \"dumpling\", \"e\", \"e\", \"e\", \"e\", \"e\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"eating\", \"email\", \"email\", \"email\", \"email\", \"email\", \"en\", \"en\", \"en\", \"en\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"escrow\", \"est\", \"est\", \"est\", \"et\", \"et\", \"et\", \"et\", \"exceptionally\", \"exceptionally\", \"exhaust\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fix\", \"fix\", \"fix\", \"fix\", \"fix\", \"float\", \"float\", \"float\", \"float\", \"float\", \"food\", \"food\", \"food\", \"food\", \"food\", \"freezer\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"frittata\", \"fry\", \"fry\", \"fry\", \"fry\", \"fry\", \"gelato\", \"gelato\", \"gelato\", \"gelato\", \"get\", \"get\", \"get\", \"get\", \"get\", \"give\", \"give\", \"give\", \"give\", \"give\", \"glove\", \"glove\", \"glove\", \"gluten\", \"gluten\", \"go\", \"go\", \"go\", \"go\", \"go\", \"goal\", \"goal\", \"gold\", \"gold\", \"gold\", \"gold\", \"good\", \"good\", \"good\", \"good\", \"good\", \"gooey\", \"gooey\", \"great\", \"great\", \"great\", \"great\", \"great\", \"grub\", \"guy\", \"guy\", \"guy\", \"guy\", \"guy\", \"gym\", \"gym\", \"gym\", \"gym\", \"gym\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"harness\", \"haunt\", \"haunt\", \"heartbeat\", \"help\", \"help\", \"help\", \"help\", \"help\", \"horrific\", \"hospitable\", \"hotel\", \"hotel\", \"hotel\", \"hotel\", \"hotel\", \"humor\", \"iPad\", \"ice\", \"ice\", \"ice\", \"ice\", \"ice\", \"iffy\", \"ikea\", \"il\", \"il\", \"implant\", \"inattentive\", \"info\", \"info\", \"infuse\", \"installer\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"job\", \"job\", \"job\", \"job\", \"job\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"knife\", \"know\", \"know\", \"know\", \"know\", \"know\", \"l\", \"l\", \"l\", \"l\", \"la\", \"la\", \"la\", \"la\", \"la\", \"lamb\", \"lamb\", \"lamb\", \"lamb\", \"lamb\", \"latte\", \"latte\", \"latte\", \"latte\", \"latte\", \"le\", \"le\", \"le\", \"le\", \"les\", \"les\", \"les\", \"library\", \"library\", \"like\", \"like\", \"like\", \"like\", \"like\", \"lil\", \"little\", \"little\", \"little\", \"little\", \"little\", \"location\", \"location\", \"location\", \"location\", \"location\", \"long\", \"long\", \"long\", \"long\", \"long\", \"look\", \"look\", \"look\", \"look\", \"look\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"love\", \"love\", \"m\", \"m\", \"m\", \"m\", \"m\", \"macaron\", \"magician\", \"meal\", \"meal\", \"meal\", \"meal\", \"meal\", \"meat\", \"meat\", \"meat\", \"meat\", \"meat\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"message\", \"message\", \"message\", \"message\", \"message\", \"mexican\", \"mexican\", \"mexican\", \"mexican\", \"mexican\", \"mojito\", \"mongolian\", \"mop\", \"mug\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail\", \"need\", \"need\", \"need\", \"need\", \"need\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"night\", \"night\", \"night\", \"night\", \"night\", \"noir\", \"noodle\", \"noodle\", \"noodle\", \"noodle\", \"noodle\", \"novelty\", \"office\", \"office\", \"office\", \"office\", \"office\", \"operation\", \"order\", \"order\", \"order\", \"order\", \"order\", \"os\", \"overcooked\", \"overcooked\", \"overcooked\", \"pancake\", \"pancake\", \"pancake\", \"pancake\", \"pancake\", \"parm\", \"pas\", \"pas\", \"pas\", \"passenger\", \"pastry\", \"pastry\", \"pastry\", \"pastry\", \"patient\", \"patient\", \"patient\", \"patient\", \"patient\", \"pedicure\", \"pedicure\", \"pedicure\", \"pedicure\", \"penne\", \"people\", \"people\", \"people\", \"people\", \"people\", \"pepper\", \"pepper\", \"pepper\", \"pepper\", \"pepper\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"place\", \"place\", \"place\", \"place\", \"place\", \"pollo\", \"pollo\", \"pork\", \"pork\", \"pork\", \"pork\", \"pork\", \"portion\", \"portion\", \"portion\", \"portion\", \"portion\", \"pour\", \"pour\", \"pour\", \"pour\", \"pour\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"price\", \"price\", \"price\", \"price\", \"price\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"professional\", \"professional\", \"professional\", \"professional\", \"professional\", \"professionally\", \"quote\", \"quote\", \"quote\", \"quote\", \"quote\", \"raman\", \"raman\", \"raman\", \"raman\", \"raman\", \"re\", \"re\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"refer\", \"refer\", \"refer\", \"refer\", \"removal\", \"rent\", \"rent\", \"rent\", \"rent\", \"rent\", \"repair\", \"repair\", \"repair\", \"repair\", \"repair\", \"respective\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"review\", \"review\", \"review\", \"review\", \"review\", \"rice\", \"rice\", \"rice\", \"rice\", \"rice\", \"ricotta\", \"room\", \"room\", \"room\", \"room\", \"room\", \"rug\", \"s\", \"s\", \"s\", \"s\", \"s\", \"salad\", \"salad\", \"salad\", \"salad\", \"salad\", \"salmon\", \"salmon\", \"salmon\", \"salmon\", \"salmon\", \"salon\", \"salon\", \"salon\", \"salon\", \"salon\", \"salsa\", \"salsa\", \"salsa\", \"salsa\", \"salsa\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"say\", \"say\", \"say\", \"say\", \"say\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"scheduling\", \"scheduling\", \"scone\", \"screening\", \"seasoning\", \"seasoning\", \"seasoning\", \"seasoning\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"see\", \"see\", \"see\", \"see\", \"see\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"server\", \"server\", \"server\", \"server\", \"server\", \"service\", \"service\", \"service\", \"service\", \"service\", \"sex\", \"shed\", \"shiny\", \"shrimp\", \"shrimp\", \"shrimp\", \"shrimp\", \"shrimp\", \"slider\", \"slider\", \"slider\", \"slider\", \"sont\", \"sont\", \"sont\", \"soup\", \"soup\", \"soup\", \"soup\", \"soup\", \"spaghetti\", \"spaghetti\", \"spaghetti\", \"spicy\", \"spicy\", \"spicy\", \"spicy\", \"spicy\", \"staff\", \"staff\", \"staff\", \"staff\", \"staff\", \"stay\", \"stay\", \"stay\", \"stay\", \"stay\", \"steak\", \"steak\", \"steak\", \"steak\", \"steak\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"storage\", \"store\", \"store\", \"store\", \"store\", \"store\", \"structure\", \"structure\", \"substitute\", \"substitute\", \"substitute\", \"substitute\", \"substitute\", \"super\", \"super\", \"super\", \"super\", \"super\", \"sur\", \"sur\", \"t\", \"t\", \"t\", \"t\", \"t\", \"taco\", \"taco\", \"taco\", \"taco\", \"taco\", \"taiwanese\", \"taiwanese\", \"taiwanese\", \"take\", \"take\", \"take\", \"take\", \"take\", \"tan\", \"tan\", \"tan\", \"tar\", \"tart\", \"tart\", \"tartar\", \"taste\", \"taste\", \"taste\", \"taste\", \"taste\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"tech\", \"tech\", \"tech\", \"tech\", \"tech\", \"technician\", \"technician\", \"technician\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"temporary\", \"temporary\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"therapist\", \"therapist\", \"therapist\", \"tile\", \"tile\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tire\", \"tire\", \"tire\", \"tire\", \"tire\", \"toast\", \"toast\", \"toast\", \"toast\", \"toast\", \"tow\", \"tow\", \"tr\", \"tr\", \"try\", \"try\", \"try\", \"try\", \"try\", \"un\", \"un\", \"un\", \"un\", \"und\", \"understaffed\", \"une\", \"une\", \"une\", \"unit\", \"unit\", \"unit\", \"unit\", \"upcoming\", \"uptown\", \"ve\", \"ve\", \"ve\", \"ve\", \"ve\", \"veggie\", \"veggie\", \"veggie\", \"veggie\", \"veggie\", \"verde\", \"vet\", \"vet\", \"vet\", \"vet\", \"vet\", \"village\", \"vision\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"waitress\", \"waitress\", \"waitress\", \"waitress\", \"waitress\", \"want\", \"want\", \"want\", \"want\", \"want\", \"warranty\", \"warranty\", \"warranty\", \"warranty\", \"week\", \"week\", \"week\", \"week\", \"week\", \"well\", \"well\", \"well\", \"well\", \"well\", \"wo\", \"wo\", \"work\", \"work\", \"work\", \"work\", \"work\", \"y\", \"y\", \"y\", \"y\", \"y\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yellowtail\", \"yogurt\", \"yogurt\", \"yogurt\", \"yogurt\", \"yogurt\", \"yuk\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 5, 2, 4, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el34431404756035446563271142310\", ldavis_el34431404756035446563271142310_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el34431404756035446563271142310\", ldavis_el34431404756035446563271142310_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el34431404756035446563271142310\", ldavis_el34431404756035446563271142310_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.025826 -0.035349       1        1  26.866253\n",
       "4     -0.106828 -0.005482       2        1  24.272527\n",
       "1     -0.021508 -0.041762       3        1  23.817581\n",
       "3      0.125352 -0.049472       4        1  21.802033\n",
       "2      0.028809  0.132065       5        1   3.241606, topic_info=         Term         Freq        Total Category  logprob  loglift\n",
       "106         t  7347.000000  7347.000000  Default  30.0000  30.0000\n",
       "138      food  4605.000000  4605.000000  Default  29.0000  29.0000\n",
       "1098      dog   325.000000   325.000000  Default  28.0000  28.0000\n",
       "43       good  6183.000000  6183.000000  Default  27.0000  27.0000\n",
       "1173  chicken  1278.000000  1278.000000  Default  26.0000  26.0000\n",
       "...       ...          ...          ...      ...      ...      ...\n",
       "368     order    35.756095  3487.065729   Topic5  -6.1268  -1.1510\n",
       "483      tell    33.640040  1603.475935   Topic5  -6.1878  -0.4351\n",
       "199     great    35.023152  4068.466014   Topic5  -6.1475  -1.3259\n",
       "32       time    33.933999  4306.841269   Topic5  -6.1791  -1.4144\n",
       "123    people    31.710878  1411.089957   Topic5  -6.2468  -0.3664\n",
       "\n",
       "[484 rows x 6 columns], token_table=       Topic      Freq     Term\n",
       "term                           \n",
       "17974      4  0.899120    Acura\n",
       "16544      3  0.957888      Ale\n",
       "8580       1  0.923142  Alfredo\n",
       "8580       2  0.051286  Alfredo\n",
       "1950       1  0.911513      Ami\n",
       "...      ...       ...      ...\n",
       "2665       2  0.054581   yogurt\n",
       "2665       3  0.036387   yogurt\n",
       "2665       4  0.018194   yogurt\n",
       "2665       5  0.018194   yogurt\n",
       "12598      1  0.970147      yuk\n",
       "\n",
       "[1218 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 5, 2, 4, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim_models\n",
    "# # Use pyLDAvis (or a ploting tool of your choice) to visualize your results \n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim_models.prepare(lda, corpus, id2word)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9oc9kzWyKkXW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f44a26c754500ff0bf585296075bf754",
     "grade": false,
     "grade_id": "cell-bf9e63d9645bba84",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### 3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fIv-303KkXW"
   },
   "source": [
    "It seems like topic 5 is about japanese food/ restaurant because they have sushi and fish. Topic 3 is about mexican food/restaurant. Topic two is about italian food/restaurant. These two topics 2,3 is nearly the same because they have more similar words: cheese, chicken, salad. Topic 4 is fastfood restaurant. It is close to these two topics above but because they have some similar word: chicken, salad. Topic 1 is about other companies: hotel, doctor, pool, anything not a restaurant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran a different model and everything run wild. Topic number 4 is service. Anything from salon, doctor, repair shop,... Topic 1 and 3 is drink store. Any boba shop, coffee shop in these two. Topic 2 this time is food restaurant because they have words like mexican, cheese, meat, Thai, salmon, salad, ... Topic 5 this time is other: random word, random topic, random single letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_415_Sprint_Challenge_1_AG.ipynb",
   "provenance": []
  },
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
