{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Autograded Notebook (Canvas & CodeGrade)\n",
    "\n",
    "This notebook will be automatically graded. It is designed to test your answers and award points for the correct answers. Following the instructions for each Task carefully.\n",
    "Instructions\n",
    "\n",
    "- **Download** this notebook as you would any other ipynb file \n",
    "- **Upload** to Google Colab or work locally (if you have that set-up)\n",
    "- **Delete** `raise NotImplementedError()`\n",
    "\n",
    "- **Write** your code in the `# YOUR CODE HERE` space\n",
    "\n",
    "\n",
    "- **Execute** the Test cells that contain assert statements - these help you check your work (others contain hidden tests that will be checked when you submit through Canvas)\n",
    "\n",
    "- **Save** your notebook when you are finished\n",
    "- **Download** as a ipynb file (if working in Colab)\n",
    "- **Upload** your complete notebook to Canvas (there will be additional instructions in Slack and/or Canvas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more manageable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "Successfully complete all these objectives to earn full credit. \n",
    "\n",
    "**Successful completion is defined as passing all the unit tests in each objective.**  \n",
    "\n",
    "Each unit test that you pass is 1 point. \n",
    "\n",
    "There are 5 total possible points in this sprint challenge. \n",
    "\n",
    "\n",
    "There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews\n",
    "\n",
    "____\n",
    "\n",
    "# Before you submit your notebook you must first\n",
    "\n",
    "1) Restart your notebook's Kernel\n",
    "\n",
    "2) Run all cells sequentially, from top to bottom, so that cell numbers are sequential numbers (i.e. 1,2,3,4,5...)\n",
    "- Easiest way to do this is to click on the **Cell** tab at the top of your notebook and select **Run All** from the drop down menu. \n",
    "\n",
    "3) Comment out the cell that generates a pyLDAvis visual in objective 4 (see instructions in that section). \n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bec125eb29f89460cf0c19ba9aa9a2f",
     "grade": false,
     "grade_id": "cell-395851cd95d17235",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Load reviews from URL\n",
    "data_url = 'https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_4/unit1_nlp/review_sample.json'\n",
    "\n",
    "# Import data into a DataFrame named df\n",
    "# YOUR CODE HERE\n",
    "df = pd.read_json(data_url, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "356579363f311da83f4ef7abaf3c9212",
     "grade": true,
     "grade_id": "cell-cb5006475e42b8f9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible Testing\n",
    "assert isinstance(df, pd.DataFrame), 'df is not a DataFrame. Did you import the data into df?'\n",
    "assert df.shape[0] == 10000, 'DataFrame df has the wrong number of rows.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Consider using spaCy in your function. The spaCy library can be imported by running this cell.\n",
    "# A pre-trained model (en_core_web_sm) has been made available to you in the CodeGrade container.\n",
    "# If you DON'T need use the en_core_web_sm model, you can comment it out below.\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4837ed2a1cc13057ba40203859d46ff6",
     "grade": false,
     "grade_id": "cell-3d570d5a1cd6cb64",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# STOP_WORDS = nlp.Defaults.stop_words.union(['s', 'year', 'american'])\n",
    "def tokenize(doc):\n",
    "    \"\"\"\n",
    "    Takes a doc and returns a list of tokens in the form of lemmas\n",
    "    Non-alphabet, stop words, punctation, and pronoun are filtered out.\n",
    "    \"\"\"\n",
    "    # use regex to strip out multi white space and non alphabet\n",
    "    non_alpha = '[^a-zA-Z]'\n",
    "    multi_white_spaces = \"[ ]{2,}\"\n",
    "    doc = re.sub(non_alpha, ' ', doc)\n",
    "    doc = re.sub(multi_white_spaces, \" \", doc)\n",
    "    \n",
    "    # use nlp\n",
    "    doc = nlp(doc)\n",
    "    \n",
    "    return [token.strip() for token in doc if (token.is_stop != True) and \n",
    "            (token.is_punct != True) and (token.pos_ != 'PRON')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2181ca9d36070260b1f75dcfd9e58965",
     "grade": true,
     "grade_id": "cell-02da164f6fbe730a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''Testing'''\n",
    "assert isinstance(tokenize(df.sample(n=1)[\"text\"].iloc[0]), list), \"Make sure your tokenizer function accepts a single document and returns a list of tokens!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews (i.e. create a doc-term matrix).\n",
    "2. Write a fake review and query for the 10 most similar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, use `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d70a0a1a96cf8406c60b17e50b255a1a",
     "grade": false,
     "grade_id": "cell-0e96491cb529202c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min, sys: 242 ms, total: 2min\n",
      "Wall time: 2min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaahhhs</th>\n",
       "      <th>aaasssk</th>\n",
       "      <th>aab</th>\n",
       "      <th>aamco</th>\n",
       "      <th>aand</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aback</th>\n",
       "      <th>...</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zupas</th>\n",
       "      <th>zur</th>\n",
       "      <th>zuzana</th>\n",
       "      <th>zuzu</th>\n",
       "      <th>zxkxko</th>\n",
       "      <th>zyr</th>\n",
       "      <th>zyrtec</th>\n",
       "      <th>zzaplon</th>\n",
       "      <th>zzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.156418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 21095 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 aa  aaa  aaaahhhs  aaasssk  aab  aamco  aand  aaron  aback  \\\n",
       "0     0.000000  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0    0.0   \n",
       "1     0.000000  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0    0.0   \n",
       "2     0.000000  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0    0.0   \n",
       "3     0.000000  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0    0.0   \n",
       "4     0.156418  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0    0.0   \n",
       "...        ...  ...  ...       ...      ...  ...    ...   ...    ...    ...   \n",
       "9995  0.000000  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0    0.0   \n",
       "9996  0.000000  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0    0.0   \n",
       "9997  0.000000  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0    0.0   \n",
       "9998  0.000000  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0    0.0   \n",
       "9999  0.000000  0.0  0.0       0.0      0.0  0.0    0.0   0.0    0.0    0.0   \n",
       "\n",
       "      ...  zuni  zupas  zur  zuzana  zuzu  zxkxko  zyr  zyrtec  zzaplon  \\\n",
       "0     ...   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0   \n",
       "1     ...   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0   \n",
       "2     ...   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0   \n",
       "3     ...   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0   \n",
       "4     ...   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0   \n",
       "...   ...   ...    ...  ...     ...   ...     ...  ...     ...      ...   \n",
       "9995  ...   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0   \n",
       "9996  ...   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0   \n",
       "9997  ...   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0   \n",
       "9998  ...   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0   \n",
       "9999  ...   0.0    0.0  0.0     0.0   0.0     0.0  0.0     0.0      0.0   \n",
       "\n",
       "      zzzzzzzzz  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "...         ...  \n",
       "9995        0.0  \n",
       "9996        0.0  \n",
       "9997        0.0  \n",
       "9998        0.0  \n",
       "9999        0.0  \n",
       "\n",
       "[10000 rows x 21095 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create a vector representation of the reviews \n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tokenize)\n",
    "\n",
    "# Name that doc-term matrix \"dtm\"\n",
    "dtm = tfidf_vect.fit_transform(df['text'])\n",
    "\n",
    "# View Feature Matrix as DataFrame\n",
    "dtm = pd.DataFrame(data=dtm.toarray(), columns=tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32b220e23c9aa1f602f08d1c2e879d0a",
     "grade": false,
     "grade_id": "cell-3d5bc610a8ec6b24",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create and fit a NearestNeighbors model named \"nn\"\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# YOUR CODE HERE\n",
    "nn = NearestNeighbors(n_neighbors=10).fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"instructor confirmed that unit test needs to be updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sklearn.neighbors._unsupervised'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.__module__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0c054bd863f1a18f6f06f62b7f2664f",
     "grade": true,
     "grade_id": "cell-c43704dcff67e99b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": " nn is not a NearestNeighbors instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-e945d8fc049a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m'''Testing.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sklearn.neighbors.unsupervised'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' nn is not a NearestNeighbors instance.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nn has the wrong value for n_neighbors'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m:  nn is not a NearestNeighbors instance."
     ]
    }
   ],
   "source": [
    "'''Testing.'''\n",
    "assert nn.__module__ == 'sklearn.neighbors.unsupervised', ' nn is not a NearestNeighbors instance.'\n",
    "assert nn.n_neighbors == 10, 'nn has the wrong value for n_neighbors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake review and find the 10 most similar reviews\n",
    "fake_review = 'Fake. This is the worst company I have ever seen. I could not believe what they did to me. I already called their office number to talk to their manager. They never answer the phone'\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tokenize)\n",
    "# YOUR CODE HERE\n",
    "df_new = df['text'].copy()\n",
    "df_new.loc[len(df_new.index)] = fake_review\n",
    "dtm_new = tfidf_vect.fit_transform(df_new)\n",
    "# View Feature Matrix as DataFrame\n",
    "dtm_new = pd.DataFrame(data=dtm_new.toarray(), columns=tfidf_vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3da2ced9f187ed0aa1a890785e2ba00e",
     "grade": false,
     "grade_id": "cell-496203e8746296ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Fit into NearestNeighbors\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='auto').fit(dtm_new)\n",
    "doc = [dtm_new.iloc[-1].values]\n",
    "\n",
    "# Query Using kneighbors \n",
    "neigh_dist, neigh_index = nn.kneighbors(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...\n",
       "2943     Well, from the outside it looks like a pretty ...\n",
       "3180     This Walmart has the rudest of employees I hav...\n",
       "4406     Probably the worst HVAC service I have used. A...\n",
       "4491     This is a update to my earlier review. The mec...\n",
       "5956     Yesterday my two friends and I were at Madison...\n",
       "6019     I overall liked the atmosphere of this locatio...\n",
       "8470     if could leave a 0 star i would i was on hold ...\n",
       "9587     Other than the pricing, this company is awful....\n",
       "10000    Fake. This is the worst company I have ever se...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.loc[df_new.index.isin(neigh_index[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All the text are negative review of the companies. There is a pattern that using these word: worst, rudest, fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a pipeline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier.\n",
    "    - Use that pipeline to train a model to predict the `stars` feature (i.e. the labels). \n",
    "    - Use that Pipeline to predict a star rating for your fake review from Part 2. \n",
    "\n",
    "\n",
    "\n",
    "2. Create a parameter dict including `one parameter for the vectorizer` and `one parameter for the model`. \n",
    "    - Include 2 possible values for each parameter\n",
    "    - **Use `n_jobs` = 1** \n",
    "    - Due to limited computational resources on CodeGrader `DO NOT INCLUDE ADDITIONAL PARAMETERS OR VALUES PLEASE.`\n",
    "    \n",
    "    \n",
    "3. Train the entire pipeline with a GridSearch\n",
    "    - Name your GridSearch object as `gs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    4462\n",
       "4    2185\n",
       "1    1496\n",
       "3    1098\n",
       "2     759\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stars.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "Y = df['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1d18da8521d51d8bfc4b5b9d005fa34",
     "grade": false,
     "grade_id": "cell-e2beb0252d274bba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(tokenizer=<function tokenize at 0x7f2c0b2a49d0>)),\n",
       "                                       ('clf', KNeighborsClassifier())]),\n",
       "             n_jobs=1,\n",
       "             param_grid={'clf__n_neighbors': [15, 10],\n",
       "                         'vect__min_df': [0.05, 0.01]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "vect = TfidfVectorizer(tokenizer=tokenize)\n",
    "clf = KNeighborsClassifier()\n",
    "pipe = Pipeline([('vect', vect),\n",
    "                 ('clf', clf)])\n",
    "\n",
    "# create a hyper-parameter dict\n",
    "params = {\n",
    "    \"vect__min_df\": [.5, .01],\n",
    "    \"clf__n_estimators\": [50, 100]  \n",
    "}\n",
    "\n",
    "# Name the gridsearch instance \"gs\"\n",
    "gs = GridSearchCV(pipe, \n",
    "                  params, \n",
    "                  n_jobs=1, \n",
    "                  cv=3, \n",
    "                  verbose=1)\n",
    "\n",
    "# run the gridsearch\n",
    "gs.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9e2378efb868f104a4eb39e4f25563c",
     "grade": true,
     "grade_id": "cell-d07134c6fe5d056e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Visible Testing\n",
    "prediction = gs.predict([\"I wish dogs knew how to speak English.\"])[0]\n",
    "assert prediction in df.stars.values, 'You gs object should be able to accept raw text within a list. Did you include a vectorizer in your pipeline?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "    - Set num_topics to `5`\n",
    "    - Name your LDA model `lda`\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "When you instantiate your LDA model, it should look like this: \n",
    "\n",
    "```python\n",
    "lda = LdaModel(corpus=corpus,\n",
    "               id2word=id2word,\n",
    "               random_state=723812,\n",
    "               num_topics = num_topics,\n",
    "               passes=1\n",
    "              )\n",
    "\n",
    "```\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note about  pyLDAvis\n",
    "\n",
    "**pyLDAvis** is the Topic modeling package that we used in class to visualize the topics that LDA generates for us.\n",
    "\n",
    "You are welcomed to use pyLDAvis if you'd like for your visualization. However, **you MUST comment out the code that imports the package and the cell that generates the visualization before you submit your notebook to CodeGrade.** \n",
    "\n",
    "Although you should leave the print out of the visualization for graders to see (i.e. comment out the cell after you run it to create the viz). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "# Due to limited computationalresources on CodeGrader, use the non-multicore version of LDA \n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estimate a LDA topic model of the review tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9514841e71735eaa255bccc53b257896",
     "grade": false,
     "grade_id": "cell-66331a185ff52f15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Remember to read the LDA docs for more information on the various class attirbutes and methods available to you\n",
    "# in the LDA model: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "# don't change this value \n",
    "num_topics = 5\n",
    "\n",
    "# use tokenize function you created earlier to create tokens \n",
    "df['tokens'] = df['text'].apply(tokenize)\n",
    "# create a id2word object (hint: use corpora.Dictionary)\n",
    "id2word = corpora.Dictionary(df['tokens'] )\n",
    "# create a corpus object (hint: id2word.doc2bow)\n",
    "corpus = [id2word.doc2bow(text) for text in df['tokens']]\n",
    "# instantiate an lda model\n",
    "lda = LdaModel(corpus, num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6479db0fa59c99d3ae3201c1f10ebca1",
     "grade": true,
     "grade_id": "cell-5a3c181311134fa9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible Testing\n",
    "assert lda.get_topics().shape[0] == 5, 'Did your model complete its training? Did you set num_topics to 5?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create 1-2 visualizations of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "189591ed7b9e6e6146d59761fb418268",
     "grade": false,
     "grade_id": "cell-9b043e992fbd218c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el19941398226372190569997389883\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el19941398226372190569997389883_data = {\"mdsDat\": {\"x\": [-0.134366582069139, 0.019386784783809615, 0.032215458963640795, -0.010041426012422625, 0.0928057643341112], \"y\": [0.03275580008943345, -0.05133884750513629, -0.06303655697474066, 0.0014335643646567085, 0.08018604002578673], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [33.7736051086899, 25.880257593201804, 23.19511011988044, 12.60894539708113, 4.542081781146723]}, \"tinfo\": {\"Term\": [\"great\", \"food\", \"good\", \"sushi\", \"place\", \"s\", \"pizza\", \"order\", \"restaurant\", \"roll\", \"eat\", \"love\", \"menu\", \"burger\", \"service\", \"lunch\", \"delicious\", \"fresh\", \"dinner\", \"chicken\", \"time\", \"meal\", \"enjoy\", \"salad\", \"excellent\", \"friendly\", \"amazing\", \"price\", \"server\", \"recommend\", \"vet\", \"brake\", \"agent\", \"painting\", \"harness\", \"file\", \"disgusted\", \"Cox\", \"battery\", \"barber\", \"procedure\", \"Amazon\", \"lash\", \"installation\", \"perform\", \"Encore\", \"secure\", \"warranty\", \"mattress\", \"clerk\", \"rim\", \"insurance\", \"uneven\", \"Carl\", \"assistant\", \"plumbing\", \"dent\", \"physician\", \"verify\", \"rug\", \"tire\", \"appointment\", \"mechanic\", \"technician\", \"spray\", \"tub\", \"doctor\", \"car\", \"cat\", \"haircut\", \"facial\", \"apartment\", \"company\", \"office\", \"rent\", \"tech\", \"email\", \"contact\", \"quote\", \"information\", \"call\", \"hair\", \"Dr\", \"phone\", \"schedule\", \"room\", \"fix\", \"pool\", \"desk\", \"client\", \"hotel\", \"store\", \"professional\", \"work\", \"tell\", \"rude\", \"repair\", \"money\", \"say\", \"issue\", \"problem\", \"need\", \"day\", \"customer\", \"t\", \"purchase\", \"business\", \"care\", \"manager\", \"charge\", \"stay\", \"help\", \"know\", \"go\", \"time\", \"didn\", \"leave\", \"s\", \"take\", \"service\", \"look\", \"ask\", \"want\", \"experience\", \"get\", \"come\", \"new\", \"year\", \"people\", \"don\", \"find\", \"like\", \"place\", \"good\", \"great\", \"yuk\", \"surgery\", \"bum\", \"Johnny\", \"Dunkin\", \"Bouchon\", \"port\", \"bowling\", \"sensitive\", \"possibility\", \"Indian\", \"installer\", \"Breakfast\", \"Noodles\", \"mojito\", \"oxtail\", \"vision\", \"butternut\", \"Masala\", \"making\", \"FANTASTIC\", \"vary\", \"Hong\", \"bisque\", \"biryani\", \"flair\", \"Bobbie\", \"MSG\", \"Life\", \"omelet\", \"Depot\", \"enchilada\", \"okra\", \"sirloin\", \"calamari\", \"jalape\", \"gnocchi\", \"spaghetti\", \"theater\", \"Style\", \"pasta\", \"egg\", \"salad\", \"dish\", \"portion\", \"server\", \"order\", \"meal\", \"sauce\", \"chicken\", \"tasty\", \"meat\", \"place\", \"brunch\", \"cheese\", \"breakfast\", \"potato\", \"tomato\", \"italian\", \"huge\", \"pho\", \"garlic\", \"side\", \"pizza\", \"come\", \"food\", \"try\", \"delicious\", \"taste\", \"love\", \"option\", \"eat\", \"definitely\", \"t\", \"like\", \"restaurant\", \"good\", \"time\", \"great\", \"get\", \"fry\", \"s\", \"amazing\", \"wait\", \"service\", \"friendly\", \"go\", \"m\", \"nice\", \"price\", \"doughnut\", \"squid\", \"Wait\", \"heartbeat\", \"connoisseur\", \"Dragon\", \"Little\", \"Curry\", \"Ramen\", \"Decent\", \"farmer\", \"Jim\", \"tart\", \"flower\", \"btw\", \"Fe\", \"tartar\", \"Katie\", \"Nachos\", \"accent\", \"rank\", \"campus\", \"Pad\", \"asada\", \"Penn\", \"Pumpkin\", \"arepa\", \"Tacos\", \"eating\", \"asado\", \"Thai\", \"cupcake\", \"Yum\", \"donut\", \"raman\", \"brewery\", \"trail\", \"mexican\", \"spicy\", \"taiwanese\", \"fajita\", \"Tom\", \"pork\", \"taco\", \"pastry\", \"cake\", \"wine\", \"carne\", \"sandwich\", \"tea\", \"food\", \"soup\", \"fresh\", \"cookie\", \"flavor\", \"good\", \"beer\", \"bread\", \"sweet\", \"bar\", \"small\", \"drink\", \"little\", \"rice\", \"flavour\", \"menu\", \"eat\", \"s\", \"taste\", \"like\", \"try\", \"place\", \"ve\", \"t\", \"order\", \"delicious\", \"nice\", \"come\", \"restaurant\", \"think\", \"service\", \"great\", \"time\", \"get\", \"price\", \"love\", \"well\", \"go\", \"m\", \"attach\", \"Alex\", \"freezer\", \"kabob\", \"clam\", \"curly\", \"fi\", \"humor\", \"Moon\", \"lil\", \"froyo\", \"Nancy\", \"wi\", \"Southern\", \"uptown\", \"disgust\", \"Church\", \"Jessica\", \"benny\", \"shawarma\", \"Smash\", \"hotdog\", \"Diamond\", \"graduation\", \"Christina\", \"Hills\", \"contest\", \"Fountain\", \"magician\", \"Noir\", \"Robert\", \"tater\", \"snow\", \"Owner\", \"scorpion\", \"falafel\", \"increase\", \"laser\", \"stunning\", \"tot\", \"McDonald\", \"burger\", \"prime\", \"great\", \"liquor\", \"chowder\", \"s\", \"good\", \"love\", \"rib\", \"food\", \"play\", \"friendly\", \"awesome\", \"fry\", \"excellent\", \"service\", \"pedicure\", \"time\", \"visit\", \"special\", \"go\", \"year\", \"enjoy\", \"like\", \"look\", \"fun\", \"location\", \"place\", \"bar\", \"take\", \"pizza\", \"find\", \"order\", \"restaurant\", \"night\", \"come\", \"staff\", \"amazing\", \"t\", \"wait\", \"ve\", \"recommend\", \"grub\", \"Josh\", \"smokey\", \"caring\", \"je\", \"Pasta\", \"ai\", \"vous\", \"j\", \"ou\", \"Friendly\", \"brioche\", \"TV\", \"dan\", \"Hibachi\", \"sever\", \"Gabi\", \"definately\", \"nite\", \"Resort\", \"Ami\", \"thread\", \"sur\", \"Keg\", \"lox\", \"Pistachio\", \"stuffy\", \"und\", \"terrasse\", \"Group\", \"bumper\", \"que\", \"en\", \"une\", \"Mon\", \"bi\", \"pas\", \"les\", \"la\", \"l\", \"sushi\", \"de\", \"Roll\", \"et\", \"le\", \"nigiri\", \"est\", \"ce\", \"ayce\", \"un\", \"eyebrow\", \"roll\", \"sashimi\", \"japanese\", \"tour\", \"great\", \"pizza\", \"buffet\", \"lunch\", \"chef\", \"dinner\", \"food\", \"good\", \"place\", \"excellent\", \"service\", \"restaurant\", \"menu\", \"price\", \"quality\", \"t\", \"time\", \"eat\", \"s\", \"recommend\", \"enjoy\", \"go\", \"amazing\", \"fresh\", \"experience\", \"definitely\", \"come\", \"like\", \"love\", \"order\", \"staff\"], \"Freq\": [4040.0, 4580.0, 6177.0, 408.0, 5241.0, 6045.0, 711.0, 3438.0, 1614.0, 475.0, 1516.0, 2101.0, 1249.0, 628.0, 3428.0, 711.0, 1164.0, 864.0, 642.0, 1192.0, 4353.0, 875.0, 869.0, 685.0, 625.0, 1385.0, 1357.0, 1753.0, 811.0, 1416.0, 84.72611338773795, 36.593333830108726, 34.48018021700627, 35.514274553116024, 29.831328662833172, 35.24660753505193, 21.5485268717161, 21.154137766503748, 46.591821698975956, 55.36103283044051, 63.55753182562594, 19.613790628668568, 42.83971187936171, 25.900215578395425, 35.41410392592488, 15.932903021207615, 16.002479266278016, 75.66486350539968, 19.042370688266743, 39.270157932256666, 19.133034179795832, 111.38503041756974, 16.480497401889163, 16.28697338070389, 45.799096146678245, 27.40409821470185, 16.57884952095328, 22.911193973166966, 13.701137991333697, 13.557842033528718, 138.66824314936449, 390.65052062451304, 59.215738782283594, 72.59644322860991, 48.08584342841473, 55.73597072585726, 140.69813019239106, 663.1969696254728, 67.92614693286511, 86.9602195140812, 61.50048011921919, 86.95479750486696, 369.1998208009138, 314.0913731634566, 100.65822073405859, 96.39050365726138, 128.4881924499687, 120.19352132457811, 109.41294834823564, 83.53083840775206, 734.6294186964867, 368.9879453639265, 258.7655883581185, 353.6135502547675, 155.00912629659186, 934.4910338021406, 335.9619112756268, 232.53090879283192, 148.13785175497762, 99.33637262322154, 394.644376583892, 630.0561796058914, 305.6801288399598, 1081.4277218381872, 1102.8607455145311, 285.54707655961954, 177.24762618426487, 385.49563970306014, 1022.0566812205527, 294.50602325711327, 315.5470734028628, 907.3581080436944, 982.569201016045, 733.3428335063818, 3394.0195169457597, 246.16005248187238, 388.7699374432742, 451.7597024816846, 343.14430586977255, 361.93456678390424, 452.29983787578914, 434.54802082164235, 907.6067657556443, 1384.2248909216596, 1759.3875713211467, 807.096538067959, 617.1245088823665, 1925.9135615706193, 755.1559142095527, 1315.389842455183, 915.3358100992875, 759.4551504388152, 856.4057595644115, 733.9713099150832, 1026.5005838055533, 1212.7229659156437, 595.2039413152122, 567.7556651453971, 675.5427853021048, 704.950174412987, 694.8138374215786, 982.1447351518443, 1026.9751784289363, 1003.708760926273, 658.6797010989432, 47.03853418349957, 31.716240272492016, 19.603390596389104, 20.068794272433934, 19.161825727517687, 16.491420848911748, 18.783530434017216, 28.545416389069896, 18.588815452721214, 15.976296395252659, 14.24406966583105, 17.326608540562717, 19.621887501835364, 24.363919157009224, 12.816195314211267, 19.80655027204651, 12.485904845562272, 15.35353014868881, 12.466188510270179, 10.707641310283753, 10.705098770519387, 23.709035867742866, 11.756083692376722, 24.06513982091313, 10.121587321387487, 11.173179104587756, 9.094826440001196, 11.003352096705383, 11.037679712700962, 21.97029061044801, 24.31509630589611, 64.1314554497526, 25.949807646577355, 22.33013465578194, 56.134495397140455, 34.69833788780798, 19.732855617806337, 29.07372247986126, 56.63900072849151, 23.090033493175874, 131.0172776131282, 288.2585139112946, 443.1905571921939, 536.6370985262703, 314.62285351237625, 491.4208985672982, 1682.1339895590224, 511.4082859099927, 507.96657928838295, 638.6029534359067, 308.7709904089469, 376.6818717688459, 2193.000719255172, 126.64992291203704, 430.4488583881979, 276.49339388230675, 209.02751880873802, 138.90221372517584, 109.59297958220277, 281.88834936849236, 57.700787401775756, 99.73174528886767, 96.95795901227078, 359.2536662689746, 1397.3263137252814, 1570.8591632019882, 987.2682399651909, 528.828425290447, 519.273330667176, 809.1630269919069, 293.8496435093405, 612.9147068329077, 567.2825081793203, 1961.2668916582718, 1178.4797707783086, 632.4290543195243, 1616.5898598270232, 1232.5042644552773, 1143.8441583832885, 854.9200830864331, 422.1511348102085, 1232.253575551432, 515.4140121270675, 561.0197904347907, 799.5950265775687, 508.90638873412394, 718.736105947529, 542.9006076636483, 516.1612696051392, 516.6754314511509, 18.65424425141025, 18.712795959951123, 17.113767060344603, 16.91219580036885, 16.68895387804912, 22.602744641030647, 14.755393719182193, 25.960502093150726, 32.39015862545806, 15.673321032855563, 16.252171533703695, 14.00146012317694, 34.66428918489321, 32.80719014741224, 24.793045030877575, 13.768603656013962, 15.633926803629949, 11.673187008058633, 12.938020535146034, 11.340515060937182, 15.7964363111037, 17.922210641437825, 35.40944624789959, 38.3499283618015, 11.761273732213471, 12.439960173126144, 15.809955326188309, 29.994685996967355, 9.228764428304016, 10.038445845814367, 153.01549993538532, 60.310493358676204, 36.18945710159327, 136.4042808203249, 135.2599607279973, 28.49677776158558, 26.11159486143838, 163.12388353876756, 272.80935276475117, 22.17630999532925, 35.440907070997234, 37.02087738180116, 242.59149475082646, 299.4920863720588, 61.90278065333323, 151.6273499390068, 255.49271011266893, 37.98664553848866, 347.21296496427686, 213.5110413133783, 1963.4436578274824, 287.1173231794236, 469.86180735433834, 103.21090276085641, 398.19368405599886, 2293.8434897099464, 318.64405509874763, 263.9649672341252, 319.6651507922614, 493.69068352681944, 422.91208379990405, 602.0455502147591, 590.499939640576, 253.59873173399993, 106.31676050209047, 520.0255680508251, 601.3971012531022, 1710.034604916806, 478.9421115480022, 1098.420054054907, 827.9289404282566, 1362.161733331474, 682.0087733961451, 1526.8278314653141, 918.3810506256118, 466.3307204086521, 577.114556328734, 875.1233122909846, 523.4411508777206, 486.03162014108193, 712.4116180490685, 713.0029423945829, 705.3020858321831, 558.3808467303634, 449.74463994391425, 470.800353339902, 396.9124449338624, 454.8656953722365, 412.26843654163446, 27.64319008719476, 18.863791407002232, 20.991432479473428, 19.06441898341886, 35.98372764544385, 16.047948309299258, 15.909402928942624, 12.129830639465473, 13.309361778590173, 12.855954689297812, 11.220144155998993, 11.030162366569094, 13.769444960836166, 12.798083167996717, 9.069648842232416, 9.297261625694022, 11.096025179477328, 17.900843548889018, 8.82873948380027, 27.132049577094165, 9.513911639829576, 14.847064424170703, 18.32873077931906, 8.661831598865811, 13.589211768613861, 15.588711340140405, 7.969880469649936, 7.811720185062051, 8.056031079123771, 10.0276890351798, 35.163992778976244, 16.10264722666029, 24.46777408743053, 19.37472430293906, 18.442221578983958, 22.398024000088284, 22.314379590162776, 18.593737283482355, 13.736452845776126, 20.967955395686744, 25.355999383566544, 315.50813046419916, 49.05353975300476, 1138.46697796261, 22.116374996261598, 20.57513271455863, 1044.4569118756283, 1060.0871738539904, 452.30943320852475, 80.62321602916121, 690.9800362092304, 92.6982485784285, 279.37291491972525, 152.75854220103764, 212.55324540464787, 159.07254863826793, 464.45675997553235, 63.05286642352085, 529.5566427354629, 192.70762993391065, 148.94564876512482, 392.95138708006584, 199.7865759279572, 183.89838301028084, 438.80662322544896, 289.1391445016558, 117.76852638390233, 183.27950001767465, 481.57822551473384, 187.56955012025645, 229.84375336190507, 153.6498149765061, 232.05660892641527, 341.2643283928012, 231.86440269951643, 182.5578925027532, 352.2407112761631, 215.31921476059952, 201.70728870488102, 391.95644068896206, 215.06270746126543, 224.8656254318502, 191.1275378518128, 20.796432209728366, 17.610582487902082, 16.598951693828294, 14.103319730988666, 12.238163098929254, 13.40998089955782, 18.26453063394935, 14.381288826576634, 16.22721553902804, 9.674097248975698, 12.39385613510547, 8.88636100052612, 10.54494323609642, 10.464851068322858, 14.32425747113943, 10.729887812600031, 9.333663968381435, 8.33791899233232, 8.16624331410396, 9.354825026523587, 12.584885267045852, 10.385413297424627, 8.970332139165818, 6.828634732861638, 6.7597794942902265, 6.374050598140373, 6.150853853265882, 6.568252887174208, 6.222124350508804, 6.6489907710482266, 10.487233444714994, 18.944316772541665, 31.19380093741418, 20.282423496711534, 19.230087881488195, 10.847213827076226, 17.595794454023377, 16.939036407923656, 66.45939812554964, 48.15460338671372, 251.8571110777329, 86.39144017365803, 31.875479484929652, 39.52590247507517, 33.4643898213382, 20.340460486530628, 20.930539497545375, 13.40178891908171, 31.421557760106683, 27.427698841733765, 32.06319790288238, 130.3969483481315, 36.786495656639474, 40.9103044394153, 40.17350697794319, 387.00225631297604, 119.00026083615496, 63.693959666513955, 94.38276921275263, 49.57667457200415, 81.71351749793423, 183.58882643837956, 203.07912655656912, 177.30477878191448, 71.37030211493799, 136.96690066392586, 101.68448146958283, 84.72734364278517, 94.50896898240873, 65.644409586002, 151.8475757721152, 126.79036150058079, 88.26376846126955, 133.30382734649942, 78.11194680275851, 68.83181383428317, 93.52994852078777, 74.53407328749282, 65.74150605625553, 70.73765792594237, 68.02695375576108, 79.4996741716284, 77.80139408479425, 71.7146056856235, 70.65552444056893, 66.99231223074446], \"Total\": [4040.0, 4580.0, 6177.0, 408.0, 5241.0, 6045.0, 711.0, 3438.0, 1614.0, 475.0, 1516.0, 2101.0, 1249.0, 628.0, 3428.0, 711.0, 1164.0, 864.0, 642.0, 1192.0, 4353.0, 875.0, 869.0, 685.0, 625.0, 1385.0, 1357.0, 1753.0, 811.0, 1416.0, 86.46528359222498, 37.4320442110591, 35.385712693782104, 36.72428092406967, 30.847899307843438, 36.51931974382539, 22.332594741839983, 21.926049334794662, 48.32279105512967, 57.645796466542706, 66.26559374323514, 20.45076409020011, 44.668056411591046, 27.04871767783717, 37.033664503042665, 16.686490603770615, 16.75997045410032, 79.32354058067105, 19.975367873079456, 41.2305636174745, 20.092289355650934, 117.08964421057858, 17.331585627186847, 17.131153421972368, 48.203515469294196, 28.84504683797497, 17.45444112009989, 24.124376098474208, 14.44342657360866, 14.29988217972226, 146.34463190581158, 419.8177728874769, 62.52204052621787, 76.89285993446019, 50.78228355578716, 59.00117940645144, 150.3985155506002, 733.247738626857, 72.2344686376575, 93.1263063718408, 65.454278369591, 93.20679238839614, 408.87546562899365, 347.1492949295769, 108.61890692209285, 103.95171997170627, 140.02600624609497, 130.8447431644954, 118.87162594732717, 89.77198815893226, 865.2865961834428, 422.9330200941078, 294.5347359862063, 410.5913620385424, 173.08569789802664, 1171.8131671565704, 397.2294615636235, 269.58467781334207, 166.48378959639774, 108.97334114761026, 498.8197980116914, 844.1629016558651, 378.41469456551977, 1548.898598084134, 1594.8770054554736, 353.8988523381977, 208.05243870771787, 512.8566648880882, 1583.6783363214481, 379.08479207965934, 412.1975589472384, 1443.6363903644526, 1587.7032189446852, 1129.246020956377, 7425.918256530424, 308.42473971375483, 543.8280666781926, 657.1240758458183, 466.2911087522537, 501.934872485971, 668.7412806516093, 637.0930737440444, 1715.424629192253, 3044.3080278422785, 4353.540925844652, 1518.8996602010038, 1056.895233416941, 6045.9624812609845, 1469.910171452371, 3428.8201477212783, 1990.3012864488874, 1507.9913610369556, 1826.023888046795, 1461.8834824177786, 2670.3835186174488, 3916.9129773797013, 1075.4438151663885, 996.9801553557589, 1444.8890988014457, 1583.9219752772842, 1602.795430451488, 3775.652577295303, 5241.020635312231, 6177.308410873802, 4040.996036152401, 47.85063222854422, 32.586445167706074, 20.44232926887551, 20.952452040294677, 20.053780927098092, 17.294298640382568, 19.76232852250969, 30.039525163522793, 19.56851286801034, 16.83390767225095, 15.023709783302964, 18.330282155415865, 20.76467846790911, 25.81191901448852, 13.582103175274066, 21.015545398778993, 13.288195498028667, 16.41817596240031, 13.334281122110228, 11.475614483493143, 11.48583180200618, 25.495524065526393, 12.697571010734505, 26.006372219779934, 10.941753105743562, 12.109834314904205, 9.861393708184607, 11.930965067706298, 11.969813092845982, 23.827719657598546, 26.43758691429704, 70.64115388854069, 28.331517369720004, 24.369742735396102, 62.00310950418934, 38.26390121053251, 21.534880467836018, 32.62285739527067, 66.44232791356727, 25.652485422721007, 166.08506528218098, 401.0969925182389, 685.4445912995818, 875.8430769241812, 482.10767082348366, 811.0540154857584, 3438.615422945817, 875.1989029680574, 893.7016893882926, 1192.346239330108, 513.558485613751, 654.8906762419377, 5241.020635312231, 183.52024274675722, 784.5698050471581, 470.12849618368386, 339.74366668634633, 209.54129058983304, 158.28063588336192, 499.27119484041805, 73.92549835058719, 144.36067163605588, 140.95625663770386, 711.3410479988711, 3916.9129773797013, 4580.935789609171, 2551.109103486126, 1164.1116472418169, 1158.9660032014121, 2101.2652609810857, 565.4984151487456, 1516.1704626944274, 1386.1556631296382, 7425.918256530424, 3775.652577295303, 1614.9570533960575, 6177.308410873802, 4353.540925844652, 4040.996036152401, 2670.3835186174488, 956.3200403821888, 6045.9624812609845, 1357.1173020865465, 1682.9145831319936, 3428.8201477212783, 1385.8009756066233, 3044.3080278422785, 1769.992096326891, 1739.2613036235052, 1753.5737095040952, 19.42205183753781, 19.563361861021566, 17.904394660761106, 17.75053085445456, 17.570332179454216, 23.80519392544034, 15.562300863069837, 27.416196001769784, 34.24420714137792, 16.58646319289959, 17.21932049788345, 14.894468866399489, 36.882463204756384, 34.98135218948577, 26.456362173389024, 14.71011905024683, 16.721912584990754, 12.492232746138516, 13.854403036187723, 12.15281124984112, 16.935874108551722, 19.26528962185561, 38.06739023556621, 41.26464729995643, 12.672508509248111, 13.412133959914573, 17.099783529455053, 32.49488837119814, 10.028990127914714, 10.912547124137015, 166.43783956731482, 65.99514788404791, 39.45212775097851, 152.42859679373657, 152.93294803362926, 31.19521445732516, 28.554270632361053, 191.07387151539663, 334.3444955049292, 24.23733967605551, 39.94467821151231, 41.88147257890913, 331.60451329435494, 420.7899431380912, 75.52439254591411, 210.16367073568807, 383.49504267085814, 44.638934812466864, 555.9914381360788, 324.7020166482443, 4580.935789609171, 483.0568195380327, 864.3295703326884, 146.41054314089433, 735.8605043178142, 6177.308410873802, 578.0500660276936, 464.0938945879925, 590.5183494105568, 1012.5450811138926, 895.7916996699445, 1402.2090011078192, 1394.719250571424, 473.2361370307282, 156.92834877013667, 1249.1112558721265, 1516.1704626944274, 6045.9624812609845, 1158.9660032014121, 3775.652577295303, 2551.109103486126, 5241.020635312231, 1976.577270318686, 7425.918256530424, 3438.615422945817, 1164.1116472418169, 1739.2613036235052, 3916.9129773797013, 1614.9570533960575, 1462.1220885543564, 3428.8201477212783, 4040.996036152401, 4353.540925844652, 2670.3835186174488, 1753.5737095040952, 2101.2652609810857, 1172.6470930607513, 3044.3080278422785, 1769.992096326891, 28.541108080639034, 19.648444427806066, 21.8720408708154, 19.87147154304169, 37.84478844099799, 16.940115196088904, 16.825859544197108, 12.943148105482365, 14.244524942281128, 13.795631684824803, 12.048082081273208, 11.858425677283751, 14.862925114316083, 13.883104926035399, 9.857456930000314, 10.107470364608234, 12.066840057248204, 19.49411995720792, 9.626230062725629, 29.644037905005103, 10.400349388343605, 16.26556867590571, 20.086578519109253, 9.525504690718698, 14.995719943357715, 17.215963188725553, 8.806784833933166, 8.633452550003476, 8.912498688814539, 11.108437353251754, 39.04428461208501, 17.844710860990787, 27.5312077389157, 21.721375805974624, 20.800801567095647, 25.655589404034867, 26.04001602578674, 21.46546709663441, 15.600741337196895, 25.23199995510246, 31.783865415944163, 628.2808089675214, 72.01299225134919, 4040.996036152401, 27.745909430678758, 25.99588540217631, 6045.9624812609845, 6177.308410873802, 2101.2652609810857, 187.72025602318868, 4580.935789609171, 247.39039041747208, 1385.8009756066233, 563.384222209597, 956.3200403821888, 625.3732319924435, 3428.8201477212783, 145.06593012987068, 4353.540925844652, 903.9852294135626, 598.3045426131014, 3044.3080278422785, 996.9801553557589, 869.3575295046053, 3775.652577295303, 1990.3012864488874, 440.55156786008195, 951.8768373040539, 5241.020635312231, 1012.5450811138926, 1469.910171452371, 711.3410479988711, 1602.795430451488, 3438.615422945817, 1614.9570533960575, 1018.9047371301867, 3916.9129773797013, 1521.2802366654619, 1357.1173020865465, 7425.918256530424, 1682.9145831319936, 1976.577270318686, 1416.9196186486279, 21.643776924143637, 18.457263683889614, 17.45721525582154, 14.929078514988822, 13.07787191958149, 14.435220536673667, 19.7216824821277, 15.576695258881854, 17.592390135704758, 10.492315266928655, 13.462548859210106, 9.739168675856368, 11.57892773757389, 11.523752590980262, 15.81278255692926, 11.8547254047571, 10.328627908587539, 9.240387282830364, 9.054447335592208, 10.393364499838212, 14.027707412524856, 11.693270693839207, 10.101628198410705, 7.697993873619773, 7.631719038255443, 7.201967795617194, 6.953453309399339, 7.4426246651243115, 7.110524891897645, 7.61772170857113, 12.028648777261587, 21.954410789486403, 36.83186009893694, 23.69770429626756, 22.593307760699457, 12.535774462825017, 20.861503302740722, 20.06684470631023, 87.50070524254429, 63.96215837019663, 408.3527822948272, 124.57946761868314, 42.017936193860265, 57.751990503850834, 47.40738234031026, 26.45729345632413, 29.467711062052768, 16.21497516136957, 54.432115647417746, 44.95290577627247, 56.50745627768035, 475.24063815363934, 72.51590803890426, 87.87475242418104, 87.78461751795679, 4040.996036152401, 711.3410479988711, 240.6199917013906, 711.552219304207, 205.8040712133094, 642.9151895300488, 4580.935789609171, 6177.308410873802, 5241.020635312231, 625.3732319924435, 3428.8201477212783, 1614.9570533960575, 1249.1112558721265, 1753.5737095040952, 626.504615517017, 7425.918256530424, 4353.540925844652, 1516.1704626944274, 6045.9624812609845, 1416.9196186486279, 869.3575295046053, 3044.3080278422785, 1357.1173020865465, 864.3295703326884, 1461.8834824177786, 1386.1556631296382, 3916.9129773797013, 3775.652577295303, 2101.2652609810857, 3438.615422945817, 1521.2802366654619], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.6077, -8.4472, -8.5067, -8.4772, -8.6515, -8.4847, -8.9768, -8.9953, -8.2057, -8.0332, -7.8952, -9.0709, -8.2896, -8.7928, -8.48, -9.2787, -9.2744, -7.7208, -9.1004, -8.3766, -9.0957, -7.3341, -9.2449, -9.2567, -8.2228, -8.7364, -9.239, -8.9155, -9.4296, -9.4401, -7.115, -6.0793, -7.9659, -7.7622, -8.1741, -8.0265, -7.1005, -5.55, -7.8287, -7.5816, -7.9281, -7.5817, -6.1358, -6.2974, -7.4354, -7.4787, -7.1913, -7.258, -7.352, -7.6219, -5.4477, -6.1363, -6.4912, -6.1789, -7.0036, -5.2071, -6.2301, -6.5981, -7.049, -7.4486, -6.0691, -5.6013, -6.3246, -5.0611, -5.0414, -6.3927, -6.8695, -6.0926, -5.1175, -6.3618, -6.2928, -5.2366, -5.1569, -5.4495, -3.9173, -6.5411, -6.0841, -5.9339, -6.2089, -6.1556, -5.9328, -5.9728, -5.2363, -4.8142, -4.5744, -5.3537, -5.622, -4.4839, -5.4202, -4.8652, -5.2278, -5.4145, -5.2944, -5.4486, -5.1132, -4.9465, -5.6582, -5.7054, -5.5316, -5.489, -5.5035, -5.1574, -5.1127, -5.1356, -5.5569, -7.9299, -8.3241, -8.8052, -8.7817, -8.828, -8.9781, -8.8479, -8.4294, -8.8583, -9.0098, -9.1246, -8.9287, -8.8043, -8.5878, -9.2302, -8.7949, -9.2563, -9.0496, -9.2579, -9.4099, -9.4102, -8.615, -9.3165, -8.6001, -9.4662, -9.3674, -9.5732, -9.3827, -9.3796, -8.6912, -8.5898, -7.62, -8.5247, -8.675, -7.7531, -8.2342, -8.7986, -8.4111, -7.7442, -8.6415, -6.9056, -6.117, -5.6869, -5.4956, -6.0295, -5.5836, -4.3531, -5.5437, -5.5505, -5.3216, -6.0483, -5.8495, -4.0879, -6.9395, -5.7161, -6.1587, -6.4384, -6.8471, -7.0841, -6.1394, -7.7256, -7.1784, -7.2066, -5.8969, -4.5386, -4.4215, -4.886, -5.5102, -5.5285, -5.0849, -6.0978, -5.3627, -5.44, -4.1996, -4.7089, -5.3313, -4.3928, -4.6641, -4.7387, -5.0299, -5.7355, -4.6643, -5.5359, -5.4511, -5.0968, -5.5486, -5.2034, -5.484, -5.5345, -5.5335, -8.7453, -8.7422, -8.8315, -8.8433, -8.8566, -8.5533, -8.9797, -8.4148, -8.1935, -8.9194, -8.8831, -9.0322, -8.1256, -8.1807, -8.4608, -9.049, -8.9219, -9.2141, -9.1112, -9.243, -8.9116, -8.7853, -8.1044, -8.0246, -9.2065, -9.1504, -8.9107, -8.2703, -9.449, -9.3649, -6.6408, -7.5719, -8.0826, -6.7557, -6.7642, -8.3216, -8.409, -6.5768, -6.0626, -8.5723, -8.1035, -8.0599, -6.18, -5.9693, -7.5458, -6.6499, -6.1282, -8.0341, -5.8214, -6.3077, -4.0889, -6.0115, -5.5189, -7.0346, -5.6844, -3.9334, -5.9073, -6.0955, -5.9041, -5.4694, -5.6242, -5.271, -5.2904, -6.1356, -7.0049, -5.4175, -5.2721, -4.2271, -5.4998, -4.6697, -4.9524, -4.4545, -5.1463, -4.3404, -4.8487, -5.5265, -5.3133, -4.897, -5.4109, -5.4851, -5.1027, -5.1019, -5.1127, -5.3463, -5.5627, -5.5169, -5.6876, -5.5514, -5.6497, -7.7424, -8.1246, -8.0177, -8.114, -7.4788, -8.2862, -8.2949, -8.5662, -8.4734, -8.508, -8.6441, -8.6612, -8.4394, -8.5125, -8.8569, -8.8321, -8.6552, -8.177, -8.8838, -7.7611, -8.8091, -8.364, -8.1534, -8.9029, -8.4525, -8.3153, -8.9862, -9.0062, -8.9754, -8.7565, -7.5018, -8.2828, -7.8645, -8.0979, -8.1472, -7.9529, -7.9566, -8.139, -8.4418, -8.0188, -7.8288, -5.3076, -7.1689, -4.0244, -7.9655, -8.0377, -4.1106, -4.0957, -4.9475, -6.672, -4.5237, -6.5325, -5.4293, -6.033, -5.7026, -5.9925, -4.921, -6.9178, -4.7898, -5.8006, -6.0582, -5.0881, -5.7646, -5.8474, -4.9778, -5.3949, -6.2931, -5.8508, -4.8848, -5.8277, -5.6244, -6.0271, -5.6148, -5.2292, -5.6157, -5.8548, -5.1975, -5.6897, -5.755, -5.0907, -5.6909, -5.6463, -5.8089, -7.006, -7.1723, -7.2315, -7.3944, -7.5362, -7.4448, -7.1358, -7.3749, -7.2541, -7.7714, -7.5236, -7.8563, -7.6852, -7.6928, -7.3788, -7.6678, -7.8072, -7.92, -7.9408, -7.8049, -7.5083, -7.7004, -7.8469, -8.1197, -8.1298, -8.1886, -8.2242, -8.1586, -8.2127, -8.1463, -7.6906, -7.0993, -6.6006, -7.031, -7.0843, -7.6569, -7.1731, -7.2112, -5.8442, -6.1664, -4.5119, -5.5819, -6.579, -6.3638, -6.5303, -7.0282, -6.9996, -7.4454, -6.5933, -6.7292, -6.5731, -5.1702, -6.4357, -6.3294, -6.3476, -4.0824, -5.2617, -5.8867, -5.4934, -6.1373, -5.6376, -4.8281, -4.7272, -4.8629, -5.7729, -5.1211, -5.4189, -5.6014, -5.4921, -5.8565, -5.0179, -5.1983, -5.5605, -5.1482, -5.6827, -5.8091, -5.5025, -5.7295, -5.8551, -5.7818, -5.8209, -5.665, -5.6866, -5.7681, -5.783, -5.8362], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0652, 1.0628, 1.0596, 1.052, 1.052, 1.05, 1.0498, 1.0497, 1.049, 1.045, 1.0438, 1.0437, 1.0437, 1.0421, 1.0408, 1.0393, 1.0392, 1.0383, 1.0377, 1.0368, 1.0366, 1.0355, 1.0351, 1.035, 1.0343, 1.0342, 1.034, 1.0339, 1.0327, 1.0322, 1.0316, 1.0135, 1.0312, 1.028, 1.0309, 1.0286, 1.0188, 0.9851, 1.024, 1.017, 1.0232, 1.0161, 0.9834, 0.9854, 1.0094, 1.01, 0.9995, 1.0006, 1.0026, 1.0134, 0.9218, 0.949, 0.956, 0.9361, 0.9752, 0.8592, 0.918, 0.9376, 0.9687, 0.9929, 0.8512, 0.793, 0.872, 0.7262, 0.7166, 0.8709, 0.9252, 0.8, 0.6476, 0.833, 0.8183, 0.6211, 0.6056, 0.6538, 0.3025, 0.86, 0.7498, 0.7108, 0.7788, 0.7585, 0.6944, 0.7029, 0.4489, 0.2974, 0.1795, 0.4532, 0.5475, -0.0585, 0.4195, 0.1274, 0.3087, 0.3996, 0.3283, 0.3965, 0.1294, -0.0869, 0.4939, 0.5225, 0.3252, 0.276, 0.2496, -0.2611, -0.5444, -0.7317, -0.7285, 1.3346, 1.3246, 1.3098, 1.3086, 1.3062, 1.3042, 1.3009, 1.3007, 1.3003, 1.2994, 1.2984, 1.2954, 1.2951, 1.294, 1.2936, 1.2924, 1.2894, 1.2846, 1.2844, 1.2824, 1.2813, 1.279, 1.2746, 1.2741, 1.2738, 1.2712, 1.2708, 1.2708, 1.2706, 1.2705, 1.268, 1.255, 1.2639, 1.2643, 1.2523, 1.2539, 1.2643, 1.2365, 1.1921, 1.2465, 1.1145, 1.0213, 0.9156, 0.8618, 0.9249, 0.8507, 0.6367, 0.8144, 0.7867, 0.7273, 0.8429, 0.7986, 0.4804, 0.9808, 0.7514, 0.8209, 0.866, 0.9405, 0.9841, 0.7801, 1.1039, 0.9819, 0.9775, 0.6686, 0.3209, 0.2814, 0.4023, 0.5626, 0.5488, 0.3974, 0.6971, 0.446, 0.4583, 0.0203, 0.1873, 0.4142, 0.0111, 0.0897, 0.0896, 0.2127, 0.534, -0.2389, 0.3835, 0.2532, -0.1042, 0.3499, -0.0918, 0.1699, 0.1369, 0.1297, 1.4209, 1.4168, 1.4161, 1.4128, 1.4098, 1.4094, 1.408, 1.4067, 1.4056, 1.4046, 1.4034, 1.3994, 1.3992, 1.3971, 1.3963, 1.3951, 1.394, 1.3934, 1.3928, 1.392, 1.3916, 1.389, 1.3888, 1.388, 1.3866, 1.386, 1.3828, 1.3812, 1.3781, 1.3777, 1.3771, 1.3712, 1.3749, 1.3502, 1.3384, 1.3708, 1.3718, 1.3031, 1.2578, 1.3724, 1.3416, 1.3379, 1.1487, 1.1212, 1.2623, 1.1348, 1.0551, 1.2999, 0.9904, 1.042, 0.614, 0.941, 0.8517, 1.1116, 0.8471, 0.4706, 0.8656, 0.897, 0.8475, 0.7429, 0.7107, 0.6158, 0.6017, 0.8374, 1.0719, 0.5849, 0.5365, 0.1984, 0.5775, 0.2265, 0.3359, 0.1138, 0.3971, -0.1206, 0.141, 0.5464, 0.3581, -0.0375, 0.3346, 0.3599, -0.1101, -0.2735, -0.3589, -0.1037, 0.1005, -0.0346, 0.3779, -0.4398, 0.0042, 2.0388, 2.03, 2.0297, 2.0293, 2.0203, 2.0167, 2.0148, 2.0059, 2.0029, 2.0002, 1.9996, 1.9984, 1.9943, 1.9894, 1.9875, 1.9872, 1.9869, 1.9855, 1.9843, 1.9822, 1.9817, 1.9795, 1.9792, 1.9757, 1.9723, 1.9715, 1.9709, 1.9707, 1.9697, 1.9684, 1.9661, 1.968, 1.9528, 1.9564, 1.9504, 1.935, 1.9164, 1.9271, 1.9435, 1.8856, 1.8448, 1.382, 1.6868, 0.804, 1.844, 1.8369, 0.3149, 0.3082, 0.5348, 1.2256, 0.1792, 1.0891, 0.4693, 0.7657, 0.5669, 0.7018, 0.0717, 1.2375, -0.0359, 0.5251, 0.6802, 0.0234, 0.4633, 0.5174, -0.0815, 0.1416, 0.7515, 0.4233, -0.3164, 0.3847, 0.2152, 0.5383, 0.1382, -0.2394, 0.1299, 0.3513, -0.338, 0.1156, 0.1645, -0.8708, 0.0134, -0.1029, 0.0675, 3.0518, 3.0448, 3.0414, 3.0349, 3.0254, 3.0181, 3.015, 3.0119, 3.011, 3.0106, 3.0091, 3.0001, 2.9982, 2.9954, 2.9929, 2.9921, 2.9905, 2.989, 2.9885, 2.9865, 2.9832, 2.9732, 2.973, 2.9719, 2.9705, 2.9697, 2.9691, 2.9668, 2.9583, 2.9558, 2.9547, 2.9443, 2.9256, 2.9362, 2.9306, 2.9471, 2.9215, 2.9223, 2.8167, 2.8079, 2.6085, 2.7257, 2.8155, 2.7126, 2.7435, 2.8289, 2.7497, 2.9012, 2.5423, 2.5977, 2.5251, 1.7985, 2.4131, 2.3273, 2.3101, 0.746, 1.3038, 1.7627, 1.0717, 1.6684, 1.029, -0.1252, -0.3233, -0.2946, 0.9213, -0.1284, 0.3266, 0.401, 0.1711, 0.8359, -0.7981, -0.4444, 0.2482, -0.7227, 0.1937, 0.5557, -0.391, 0.1899, 0.5156, 0.0633, 0.0774, -0.8055, -0.7904, -0.2858, -0.7932, -0.0309]}, \"token.table\": {\"Topic\": [4, 1, 3, 5, 2, 2, 2, 1, 2, 4, 4, 1, 2, 3, 3, 2, 3, 2, 4, 1, 2, 3, 4, 5, 3, 2, 1, 2, 3, 4, 5, 5, 5, 2, 5, 4, 5, 2, 2, 1, 4, 3, 2, 5, 3, 5, 2, 3, 2, 2, 2, 3, 4, 3, 4, 5, 4, 3, 4, 4, 2, 4, 1, 2, 4, 2, 3, 5, 5, 3, 5, 3, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 4, 1, 2, 3, 5, 2, 3, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 3, 1, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 3, 1, 2, 3, 4, 5, 1, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 1, 2, 3, 4, 5, 4, 1, 5, 2, 2, 3, 1, 2, 3, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 2, 1, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 1, 3, 4, 5, 1, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 2, 4, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 4, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 2, 3, 4, 3, 4, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 3, 4, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 5, 2, 3, 1, 2, 3, 4, 5, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 5, 5, 1, 2, 3, 4, 5, 1, 2, 4, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 3, 1, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 3, 4, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 5, 2, 3, 4, 1, 2, 3, 4, 5, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 4, 1, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 3, 4, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 4, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 5, 1, 5, 1, 4, 2, 4, 1, 2, 3, 4, 5, 1, 1, 2, 4, 2, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2], \"Freq\": [0.9669976709765176, 0.9779585697526033, 0.07128748630065769, 0.92673732190855, 0.9126499018622812, 0.9251603856683525, 0.9631740761557716, 0.9339709712411102, 0.06668569456999932, 0.9335997239799904, 0.9115891109696624, 0.9577648795432971, 0.03647479030042853, 0.9483445478111419, 0.9646420586426981, 0.9077984340174848, 0.03782493475072853, 0.049784486643589185, 0.8961207595846054, 0.8793529874592774, 0.016975926398827747, 0.013580741119062198, 0.07129889087507654, 0.020371111678593297, 0.9661757040097104, 0.9474522569619702, 0.9588594977774721, 0.957701644044507, 0.9517258121554826, 0.9266281309436025, 0.8913616675040302, 0.8713645296987729, 0.9189099139870001, 0.06323997667075955, 0.8853596733906337, 0.9293700169200021, 0.05808562605750013, 0.945062641496962, 0.931860386145059, 0.05129751956975372, 0.9233553522555671, 0.9399462394783794, 0.9545422159438441, 0.9752258139819102, 0.9605968959959803, 0.9093278216274339, 0.9189784263694467, 0.9638677552877668, 0.9219706819671987, 0.8999360288048981, 0.06292500845402879, 0.12585001690805758, 0.78656260567536, 0.044260894004173976, 0.044260894004173976, 0.8409569860793056, 0.9126313480215067, 0.9383298555732773, 0.9276104855193241, 0.9002166265152243, 0.9298030102499752, 0.03874179209374897, 0.04603759950255741, 0.04603759950255741, 0.8747143905485909, 0.026269202953285303, 0.9194221033649856, 0.026269202953285303, 0.9005750876457073, 0.9469316979540926, 0.8331056414402936, 0.8947122088002492, 0.02920201936261743, 0.9344646196037577, 0.8659371082521062, 0.02561194320590726, 0.05122388641181452, 0.02561194320590726, 0.8964180122067541, 0.02561194320590726, 0.047598720479094914, 0.023799360239547457, 0.14279616143728474, 0.023799360239547457, 0.7615795276655186, 0.9615061597072591, 0.9363899552196507, 0.03898257745872361, 0.8965992815506431, 0.03898257745872361, 0.9500016106245092, 0.030774070942380913, 0.9232221282714274, 0.030774070942380913, 0.006008249101284181, 0.07209898921541018, 0.9192621124964797, 0.006008249101284181, 0.02387690638422262, 0.02387690638422262, 0.8834455362162369, 0.02387690638422262, 0.04775381276844524, 0.949487560015466, 0.02534717534912163, 0.05069435069824326, 0.9124983125683788, 0.9051403641395162, 0.9608397687006146, 0.050705613017866295, 0.9127010343215933, 0.19010884291529484, 0.37948082985029785, 0.22695164192988687, 0.14884490801895178, 0.05526419852188803, 0.9334083683243577, 0.03218649545946061, 0.010728831819820202, 0.010728831819820202, 0.010728831819820202, 0.9313564723826, 0.038111773805937596, 0.011909929314355498, 0.014291915177226597, 0.0047639717257421995, 0.058480272471137455, 0.9356843595381993, 0.024233819151074046, 0.024233819151074046, 0.9208851277408137, 0.024233819151074046, 0.9163763405778482, 0.5033185332561063, 0.15384703519817744, 0.2248023488456127, 0.10477513604013808, 0.013262675448118745, 0.9542872454873578, 0.020745374901899084, 0.9810410976648066, 0.22364843570845325, 0.3319226783927044, 0.13667404404405475, 0.2715731005031218, 0.03549975169975448, 0.01837150711681808, 0.0918575355840904, 0.31231562098590737, 0.5695167206213605, 0.10962474863626794, 0.19752206961489718, 0.48787951194879603, 0.18567074543800335, 0.01975220696148972, 0.9541025256181809, 0.017347318647603287, 0.017347318647603287, 0.017347318647603287, 0.972625938480653, 0.02069416890384368, 0.02069416890384368, 0.041518895006665724, 0.15569585627499646, 0.551855312796932, 0.23008387649527257, 0.022489401461943937, 0.9349454502286938, 0.07977169683177625, 0.8774886651495387, 0.9139303275588246, 0.9228507458547438, 0.03845211441061432, 0.0332894742695303, 0.9653947538163787, 0.0332894742695303, 0.9884579049804751, 0.008618945533750385, 0.346912557733453, 0.5688504052275254, 0.06464209150312789, 0.010773681917187982, 0.1063539019776085, 0.5870735389163989, 0.20632656983656048, 0.09359143374029547, 0.00425415607910434, 0.03205619892012582, 0.03205619892012582, 0.8975735697635228, 0.924103514328817, 0.010897980353915697, 0.6920217524736468, 0.19616364637048256, 0.05448990176957849, 0.04904091159262064, 0.037798091568531825, 0.9449522892132957, 0.1205219890290288, 0.5028676093969823, 0.09558640509198836, 0.020779653280867037, 0.26597956199509803, 0.9783620905887188, 0.08313485733246737, 0.8313485733246736, 0.020691384830555964, 0.41064440663718754, 0.062074154491667886, 0.5029598158812065, 0.00477493496089753, 0.7152996026411206, 0.11952306985005871, 0.10849017109466867, 0.042292778562328465, 0.01471053167385338, 0.9136215883147976, 0.014274588893020164, 0.17129506671624198, 0.723245837246355, 0.06661474816742743, 0.02379098148836694, 0.01612822337454597, 0.9031805089745745, 0.01612822337454597, 0.06451289349818388, 0.01612822337454597, 0.8494295453574532, 0.0554729499008949, 0.05200589053208897, 0.0369819666005966, 0.006934118737611863, 0.9343228341389586, 0.9041964469492821, 0.024548319826677344, 0.04227766192372209, 0.023184524280750823, 0.006818977729632595, 0.6878457457493206, 0.0928287400236915, 0.062393087556907395, 0.14304756659388526, 0.015217826233392049, 0.9377671894446784, 0.022401968241426713, 0.08960787296570685, 0.8512747931742151, 0.044803936482853426, 0.9413788359280604, 0.013843806410706769, 0.027687612821413538, 0.013843806410706769, 0.12334277296734839, 0.8017280242877645, 0.7212091046933939, 0.12352200135632713, 0.0916453558450169, 0.05179954895587912, 0.011953742066741336, 0.014020422311994053, 0.548071054014313, 0.32246971317586326, 0.09177003695123381, 0.024217093084353367, 0.03401293258550129, 0.4858990369357327, 0.15548769181943448, 0.0874618266484319, 0.24294951846786636, 0.011741555882178633, 0.5359181577651533, 0.2960549447435041, 0.11909292394781185, 0.038579397898586935, 0.03846762610810257, 0.1538705044324103, 0.807820148270154, 0.026423717536670933, 0.9512538313201536, 0.9459002394881371, 0.024253852294567616, 0.9084790734818268, 0.027529668893388693, 0.009176556297796231, 0.04588278148898115, 0.009176556297796231, 0.3096826523859769, 0.3566584215854986, 0.22339020679120347, 0.08986668890343272, 0.02016894438457723, 0.9024752791961943, 0.03179452203130224, 0.01712012724762428, 0.04402318435103387, 0.004891464927892652, 0.9675400457072103, 0.9171174714228938, 0.03821322797595391, 0.015285291190381564, 0.03057058238076313, 0.9083905364844879, 0.0204903276474634, 0.23222371333791852, 0.7035012492295767, 0.0204903276474634, 0.027320436863284534, 0.030305258251924188, 0.04545788737788628, 0.9091577475577256, 0.015152629125962094, 0.9445036125665807, 0.649105674403183, 0.134603086642952, 0.094753488623657, 0.112464421076677, 0.007969919603859, 0.8677728822317032, 0.619133341968898, 0.2103667713302258, 0.07621071655975245, 0.08124944162155427, 0.013226653287229766, 0.08829705416360548, 0.11237806893549789, 0.08829705416360548, 0.024081014771892402, 0.6903224234609155, 0.865764578381348, 0.1688122093529372, 0.40904496881673247, 0.2423970185580637, 0.12985554565610555, 0.049056539470084315, 0.016321458551692678, 0.4544237670444961, 0.4003052465836204, 0.07731217208696531, 0.05154144805797687, 0.9739641551984971, 0.8889754393433288, 0.04805272645099075, 0.024026363225495374, 0.030032954031869216, 0.006006590806373844, 0.531305668929576, 0.25083947938310835, 0.15471726418643167, 0.03226019551121341, 0.030285081500322796, 0.0388853777405294, 0.37174421119946105, 0.2846409650606752, 0.17731732249681406, 0.12754403898893643, 0.8904305108342349, 0.985107205603079, 0.011417570411264056, 0.6131235310848798, 0.2797304750759694, 0.06051312317969949, 0.03539446827491857, 0.937509253225055, 0.03324500897961188, 0.006649001795922376, 0.019947005387767128, 0.44509768221163887, 0.29862582083135486, 0.19382267863684133, 0.046088128796382466, 0.01641494998227321, 0.006560448767715028, 0.059044038909435256, 0.8922210324092438, 0.03280224383857514, 0.006560448767715028, 0.9782694515971739, 0.13977944788911614, 0.32234852268306374, 0.42932258994514244, 0.09342401874221537, 0.014976369416691015, 0.043530725353077795, 0.40430810062782857, 0.3963934232909054, 0.09761435382205323, 0.058040967137437054, 0.8973984304710182, 0.02243846293509854, 0.7180308139231533, 0.1969598413191983, 0.049863250966885646, 0.012465812741721411, 0.9141159091193437, 0.049990713779964106, 0.014283061079989745, 0.014283061079989745, 0.007141530539994873, 0.02715040721032882, 0.05430081442065764, 0.05430081442065764, 0.02715040721032882, 0.8416626235201935, 0.05662421661898808, 0.9059874659038093, 0.02831210830949404, 0.01415605415474702, 0.10467500068913585, 0.2852681337462164, 0.3197763757316458, 0.211650550843967, 0.07936895656648763, 0.23754814160012225, 0.03393544880001747, 0.03393544880001747, 0.7126444248003668, 0.259731307425669, 0.03463084099008921, 0.017315420495044604, 0.6926168198017841, 0.16630069001939093, 0.22386631348764163, 0.2414558095473849, 0.2542481703181073, 0.11353220184016112, 0.5020919990053193, 0.1730644083764929, 0.1648558198368964, 0.11218404337448551, 0.04856748219261263, 0.17696779608799804, 0.1415742368703984, 0.10618067765279882, 0.5662969474815936, 0.9472260873447228, 0.015277840118463271, 0.015277840118463271, 0.015277840118463271, 0.025034624004350938, 0.07510387201305281, 0.8762118401522828, 0.025034624004350938, 0.07795572218214013, 0.038977861091070064, 0.8575129440035414, 0.9291888145044209, 0.9509172448499411, 0.9583968224358211, 0.43361740793347964, 0.18093388244706346, 0.21774456887594876, 0.14474710595765075, 0.023084667760487405, 0.8458587101706794, 0.05034873274825472, 0.06797078921014388, 0.03020923964895283, 0.005034873274825472, 0.9083526424851021, 0.012230578957819632, 0.3737121348222665, 0.5408633805791349, 0.02989697078578132, 0.04212754974360095, 0.006372334940353996, 0.25489339761415986, 0.6754675036775236, 0.05097867952283197, 0.01911700482106199, 0.028586659388786198, 0.9433597598299446, 0.03754691353459779, 0.342943030016588, 0.42851506551404345, 0.1508425421651574, 0.040166465641662756, 0.9601298810675234, 0.02082538954796101, 0.2637882676075061, 0.5437740604189819, 0.09602818513782022, 0.07635976167585704, 0.20637884157557745, 0.36729660965723404, 0.1796794809521636, 0.20132761118736403, 0.04473946915274756, 0.9130083880402606, 0.015685125655220317, 0.4412748684335316, 0.3105654879733623, 0.2227287843041285, 0.010456750436813544, 0.29054487451206273, 0.15208208275240784, 0.22244841954829803, 0.2678460561908078, 0.06582657313163921, 0.0207812831985378, 0.6927094399512599, 0.17317735998781497, 0.09005222719366379, 0.027708377598050397, 0.3845889524257227, 0.3201787286504313, 0.20895874922449198, 0.06815500422734326, 0.018349424215053957, 0.04643629211193329, 0.9287258422386658, 0.04643629211193329, 0.45461891087970524, 0.23617846598447115, 0.14945925177042332, 0.129093375705003, 0.03087729597015339, 0.1625303341229778, 0.2617644923076246, 0.37135914987859664, 0.1715957710860124, 0.03286220899100049, 0.9448318269969748, 0.16307860589426887, 0.2830985207026458, 0.17644164795540773, 0.2816137382514081, 0.0957684681048286, 0.9702557956312373, 0.8724785780923252, 0.011822202955180558, 0.028373287092433343, 0.0803909800952278, 0.007093321773108336, 0.9342150826064197, 0.01073810439777494, 0.01073810439777494, 0.04295241759109976, 0.01073810439777494, 0.9725135478632787, 0.9577178361250975, 0.6827887759689624, 0.12713997897353094, 0.10830442653300783, 0.07534220976209241, 0.006278517480174367, 0.06147955967142461, 0.9221933950713692, 0.7918691310458812, 0.10424606282122992, 0.03207571163730152, 0.06214669129727168, 0.010023659886656723, 0.20630070603797174, 0.5648232922593013, 0.1542247996594546, 0.06209050375900121, 0.014020436332677692, 0.9271314754497113, 0.07680486824660372, 0.03840243412330186, 0.8448535507126409, 0.9357039063375366, 0.022278664436608014, 0.022278664436608014, 0.011139332218304007, 0.011139332218304007, 0.9612285620957013, 0.9274270770009495, 0.9479916071858009, 0.01708092985920362, 0.00854046492960181, 0.01708092985920362, 0.00854046492960181, 0.778190014908353, 0.07913796761779861, 0.08441383212565186, 0.047482780570679166, 0.010551729015706482, 0.006317892232483238, 0.6949681455731562, 0.1010862757197318, 0.14531152134711448, 0.05686103009234914, 0.056842759413937904, 0.9094841506230065, 0.9147002499150795, 0.07840287856414967, 0.02275966582922227, 0.14793782788994475, 0.341394987438334, 0.02275966582922227, 0.46657314949905654, 0.9175804805086374, 0.9561445894354589, 0.5293150072280078, 0.2250171726762236, 0.16147605396713455, 0.07636593165954739, 0.007578298561634473, 0.09380546486992407, 0.03126848828997469, 0.09380546486992407, 0.015634244144987344, 0.7504437189593925, 0.0799993552120136, 0.04571391726400777, 0.10285631384401747, 0.011428479316001942, 0.7542796348561281, 0.09317290842059439, 0.8851426299956467, 0.9626566153624226, 0.02238736314796332, 0.253125133842213, 0.021093761153517753, 0.021093761153517753, 0.6960941180660858, 0.5837853937568057, 0.16179465531995751, 0.15706381744510495, 0.08137041144746401, 0.015138681199528188, 0.0996668898011195, 0.04983344490055975, 0.8471685633095157, 0.26008748948597854, 0.3119990454322635, 0.29081065525010635, 0.11627129112458716, 0.020658680427603184, 0.942327274096481, 0.10812404644710938, 0.036041348815703124, 0.036041348815703124, 0.7929096739454687, 0.16562473050067822, 0.25739947294261245, 0.42302420344329067, 0.11471842805241782, 0.038717469467691015, 0.3351273899084312, 0.2689423567917191, 0.17439230948213036, 0.19225176286283044, 0.028365014192876622, 0.4597293918412477, 0.17283815387255652, 0.1894185581684704, 0.1452041467127001, 0.03265837209801213, 0.14134341128417552, 0.3850061270333266, 0.22415066233955108, 0.21510849124729742, 0.03426506940222437, 0.9172245420607296, 0.053404372819128285, 0.3850736355905566, 0.2909132940410409, 0.13772706674406768, 0.13210555381573838, 0.36271348405017523, 0.306781030902251, 0.23276940097924018, 0.07909639839100395, 0.018079176775086615, 0.897615840329968, 0.9585543341337163, 0.7355919801212426, 0.11151831768601929, 0.10937373465359584, 0.03645791155119862, 0.006433749097270344, 0.9511714688171552, 0.026279740436145685, 0.583867276646541, 0.19538415715569182, 0.13368389700126282, 0.061700260154429, 0.027485503539754503, 0.5756686019159694, 0.34356879424693126, 0.03359339321525551, 0.021377613864253502, 0.9436672172473171, 0.03198871922872261, 0.015994359614361305, 0.030421629635759298, 0.352250448414055, 0.41629598448933774, 0.13289448735621168, 0.0680483820799879, 0.005233577945896284, 0.07327009124254798, 0.8530732051810943, 0.05233577945896284, 0.010467155891792567, 0.9571418971154797, 0.7506970784595572, 0.07994436419958921, 0.07799450165813582, 0.07994436419958921, 0.009749312707266977, 0.6282745475618162, 0.18356422833945016, 0.1288413074382556, 0.04848866408966608, 0.011083123220495105, 0.5532599579904078, 0.16830260906935093, 0.15435487903597933, 0.09577441289581849, 0.028825308735634693, 0.26448009798277866, 0.29667767512850823, 0.33175003594796365, 0.08336872653804979, 0.02357322612455201, 0.23947282911574472, 0.2954152523108162, 0.22475113880125222, 0.17960462183680853, 0.06084965329990235, 0.07559352219083229, 0.15118704438166458, 0.7559352219083229, 0.8835437110062729, 0.9045099747752574, 0.04320907522811739, 0.025925445136870433, 0.017283630091246956, 0.005761210030415652, 0.03529637989205528, 0.9177058771934372, 0.03529637989205528, 0.9232943947695097, 0.04196792703497772, 0.15384658501141157, 0.5198953562454598, 0.1927503191522283, 0.10433274155946301, 0.028293624829684887, 0.12388707302285387, 0.4891503681324888, 0.2669679179224879, 0.09916782136336426, 0.02064784550380898, 0.9530784908379172, 0.951676467133801, 0.04758382335669005, 0.980277873225968, 0.09587036806389912, 0.04793518403194956, 0.04793518403194956, 0.862833312575092, 0.012042021939793144, 0.7887524370564509, 0.09031516454844858, 0.09031516454844858, 0.018063032909689716, 0.013240755288327046, 0.052963021153308185, 0.8209268278762768, 0.07944453172996227, 0.039722265864981135, 0.5445799708399831, 0.006893417352404849, 0.013786834704809698, 0.4342852932015055, 0.46785597632423886, 0.2450015023946458, 0.2117809596970667, 0.0602122336393621, 0.01522608206972375, 0.9450860580411755, 0.02700245880117644, 0.013527132346914467, 0.784573676121039, 0.1623255881629736, 0.013527132346914467, 0.027054264693828933, 0.8621710847554798, 0.07062983462686133, 0.0389681846217166, 0.026790626927430162, 0.0024355115388572874, 0.9533925315255999, 0.041451849196765214, 0.03655068138292121, 0.5046805621718736, 0.07450715820364708, 0.21649249742191792, 0.16728965709875476, 0.1959541988979055, 0.4184299495453815, 0.25987304663967603, 0.0919668197359206, 0.03377204791132354, 0.315291147196036, 0.12126582584462922, 0.16168776779283897, 0.3759240601183506, 0.02829535936374682, 0.9360359215799249, 0.03466799709555277, 0.8642924438061982, 0.06676937334125137, 0.018547048150347602, 0.04822232519090376, 0.0037094096300695203, 0.012062561996703964, 0.1960166324464394, 0.7328006412997657, 0.03920332648928788, 0.021109483494231935, 0.9614251669968251, 0.01659380359232881, 0.653381016447947, 0.2779462101715076, 0.04148450898082203, 0.01244535269424661, 0.9504626205342942, 0.014716977799077073, 0.6151696720014217, 0.2619622048235719, 0.09418865791409327, 0.014716977799077073, 0.31991811747602494, 0.2948265004190818, 0.2566188108096457, 0.07527485117082941, 0.05417508228203631, 0.013886383119724685, 0.11109106495779748, 0.13886383119724685, 0.6804327728665096, 0.06943191559862343, 0.7666226864784714, 0.058224507833807955, 0.08976278291045393, 0.07520665595200195, 0.009704084638967993, 0.9658104060454988, 0.01509078759446092, 0.01509078759446092, 0.01509078759446092, 0.8086366739836481, 0.0502094666852592, 0.039639052646257264, 0.08720591582176598, 0.01321301754875242, 0.7976013864137798, 0.05511879499607421, 0.09402617969918542, 0.042149666761703806, 0.009726846175777801, 0.2410197726562456, 0.228250513177769, 0.314443014657486, 0.11173102043667014, 0.10534639069743186, 0.0910978672658237, 0.8654297390253252, 0.9169555739760693, 0.016824872916992098, 0.025237309375488147, 0.016824872916992098, 0.016824872916992098, 0.006538813335240909, 0.08500457335813182, 0.8827398002575227, 0.006538813335240909, 0.013077626670481818, 0.9447401354926726, 0.40016384312666253, 0.2978291742494737, 0.11221525759636568, 0.13479946038305562, 0.05504899429255675, 0.9298565310774346, 0.04603250153848686, 0.009206500307697372, 0.009206500307697372, 0.009206500307697372, 0.8507470573255724, 0.019225922199447965, 0.024032402749309956, 0.07690368879779186, 0.028838883299171947, 0.07802065060184565, 0.3913416760346544, 0.3238476211489308, 0.1436570709494301, 0.06315957429673219, 0.015981226872125172, 0.2716808568261279, 0.2237371762097524, 0.4314931255473796, 0.05859783186445896, 0.012678659828571815, 0.3782466848857258, 0.5367299327428735, 0.027470429628572265, 0.04648841937142999, 0.9456363913381663, 0.04839653462582126, 0.24198267312910632, 0.35350512248425964, 0.08206368914813171, 0.27354563049377234, 0.7970553891849251, 0.0913114846282516, 0.049495944938678434, 0.049495944938678434, 0.01194729705416376, 0.8081405127776135, 0.0678159870862333, 0.08759564998638468, 0.03390799354311665, 0.0028256661285930543, 0.9790290454177655, 0.31855970095241165, 0.20377235284183343, 0.28283337935027203, 0.1726772210770082, 0.02199815172724338, 0.00875344276715961, 0.6462958576419513, 0.17652776247105215, 0.09628787043875572, 0.07148644926513682, 0.04316613234271784, 0.2518024386658541, 0.6241103301217955, 0.07374214275214298, 0.007194355390452974, 0.013790077612535822, 0.1930610865755015, 0.24822139702564477, 0.027580155225071644, 0.5102328716638254, 0.011189416019616845, 0.5684223337965357, 0.2562376268492258, 0.13203510903147878, 0.03133036485492717, 0.6453330683135384, 0.17364637356773294, 0.08966467289679302, 0.08019305252037122, 0.011365944451706156, 0.8955101541163625, 0.05777484865266855, 0.017332454595800566, 0.02310993946106742, 0.005777484865266855, 0.09615014082744572, 0.8653512674470114, 0.954655620892554, 0.9709475690950579, 0.05055150362019252, 0.6053850799393787, 0.1491885838547145, 0.13685894882539926, 0.056716321134850145, 0.38351384538903893, 0.23331640784124041, 0.20765160297870397, 0.13532351654791944, 0.03995543484281242, 0.9279000250470489, 0.03373359605073099, 0.03373359605073099, 0.9108070933697369, 0.06384959571629702, 0.6881567538312012, 0.14898239000469304, 0.09932159333646204, 0.007094399524033002, 0.9027588119773565, 0.0410344914535162, 0.1953579164268646, 0.23777849256526948, 0.472207992277507, 0.06028187135457536, 0.0346062594813303, 0.9738093820164662, 0.03632241670918373, 0.07264483341836746, 0.8717380010204094, 0.008280599379231136, 0.3477851739277077, 0.5941330054598339, 0.018631348603270054, 0.03312239751692454, 0.8889472693524426, 0.06130670823120293, 0.030653354115601467, 0.07354114312391735, 0.3810768325512081, 0.23232315668692075, 0.24903705285144742, 0.06351280542520135, 0.011963708252349643, 0.12561893664967125, 0.8165230882228631, 0.020936489441611875, 0.023927416504699286, 0.9452115312472967, 0.01969190690098535, 0.01969190690098535, 0.01969190690098535, 0.01969190690098535, 0.9712032182902051, 0.37139771252037024, 0.2879153948388003, 0.1551324958492166, 0.14132833308297274, 0.044041852635158946, 0.6758966629958592, 0.16299277935077133, 0.04037435818780574, 0.10018822216974019, 0.019439505794128693, 0.7463014529117845, 0.06396869596386724, 0.10661449327311208, 0.0758147507719908, 0.008292238365686495, 0.862880605222371, 0.0640995179899366, 0.8973932518591125, 0.0989939423980513, 0.8909454815824617, 0.9820034015773142, 0.024488629522254817, 0.1346874623724015, 0.1910113102735876, 0.03183521837893127, 0.6171134639608215, 0.059269961780080635, 0.24216012955861516, 0.5418967934178801, 0.14224790827219352, 0.015240847314877877, 0.4570478535789542, 0.2640750857007452, 0.20563113506631206, 0.05278808444400414, 0.020468849070124053, 0.009505930607964447, 0.14971840707544004, 0.7105683129453424, 0.11882413259955558, 0.007129447955973335, 0.041258653522437426, 0.9076903774936234, 0.5136368294220379, 0.19933190863663192, 0.09728485643357804, 0.1564721467113493, 0.033335370386330936, 0.9489604803696078, 0.05422631316397759, 0.9568283483530031, 0.024159466216140586, 0.44781296307774876, 0.4132994399118336, 0.0949121887062666, 0.019845275820401197, 0.019471978908203715, 0.6016841482634947, 0.25508292369746866, 0.07399351985117411, 0.04867994727050928, 0.05603901390109031, 0.896624222417445, 0.030797468100831615, 0.24637974480665292, 0.6590658173577966, 0.03695696172099794, 0.027717721290748454, 0.9235056430632357, 0.019239700897150745, 0.03847940179430149, 0.009619850448575372, 0.9493729334846138, 0.013005108677871422, 0.013005108677871422, 0.026010217355742844, 0.013005108677871422, 0.6915893803892416, 0.10282924604155541, 0.11348837520439957, 0.0846460257049389, 0.008151098771586708, 0.8438195620181185, 0.04515193994862139, 0.8578868590238063, 0.015050646649540464, 0.04515193994862139, 0.04515193994862139, 0.2954609627894557, 0.2585283424407738, 0.3323935831381377, 0.089595801216247, 0.02393780948525683, 0.08551927225347367, 0.8551927225347367, 0.40403892600566926, 0.28321773494314395, 0.16193714771688278, 0.12173998338999699, 0.029171656397225694, 0.949812768598587, 0.0068331853856013446, 0.013666370771202689, 0.0068331853856013446, 0.020499556156804034, 0.019089316424178215, 0.663353745740193, 0.13839754407529206, 0.15748686049947028, 0.019089316424178215, 0.039632213133298545, 0.11889663939989564, 0.039632213133298545, 0.8322764757992694, 0.21643883105275766, 0.03417455227148805, 0.17087276135744026, 0.12530669166212285, 0.455660696953174, 0.9105468087331828, 0.07004206221024484, 0.19795311745385977, 0.386890548370217, 0.32456471535009085, 0.06781364221686681, 0.02273520952935419, 0.9491335692499178, 0.016948813736605672, 0.016948813736605672, 0.016948813736605672, 0.2891915389118583, 0.022245502993219868, 0.0667365089796596, 0.022245502993219868, 0.6006285808169365, 0.9405284177235991, 0.0843963607190003, 0.843963607190003, 0.9231700055707494, 0.913014387373003, 0.9413417013244079, 0.039222570888516994, 0.32530982201182973, 0.1861804269056817, 0.3450408998632471, 0.11383314145048475, 0.029849579313682664, 0.9692990737794245, 0.9830535038879264, 0.011565335339857959, 0.011565335339857959, 0.9030571533794958, 0.2931463826813985, 0.275446978444031, 0.17810025513851002, 0.21349906361324492, 0.03982365953407677, 0.8987785770551799, 0.3559301262249621, 0.3333502517732951, 0.1604359500513185, 0.1277545528186425, 0.022579874451667047, 0.46877809518451574, 0.24315125497888435, 0.21193589116402756, 0.05859726189806447, 0.017524414773252926, 0.9581014594615699, 0.01260659815081013, 0.02521319630162026, 0.01260659815081013, 0.27629796033035026, 0.20381238431775836, 0.33855027855292913, 0.14070729461267836, 0.04093303116005189, 0.9419410978875951, 0.023468360731130546, 0.09126584728772989, 0.6649368873820322, 0.15906333384432925, 0.059974699646222505, 0.6979152807918557, 0.08974118781689912, 0.11621161012260317, 0.08134812708582222, 0.014849261293443739, 0.5697204673018962, 0.11635136304052808, 0.08525746429693869, 0.20060579834573808, 0.028084811768403332, 0.9822231768123474], \"Term\": [\"Alex\", \"Amazon\", \"Ami\", \"Ami\", \"Bobbie\", \"Bouchon\", \"Breakfast\", \"Carl\", \"Christina\", \"Christina\", \"Church\", \"Cox\", \"Curry\", \"Curry\", \"Decent\", \"Depot\", \"Depot\", \"Diamond\", \"Diamond\", \"Dr\", \"Dr\", \"Dr\", \"Dr\", \"Dr\", \"Dragon\", \"Dunkin\", \"Encore\", \"FANTASTIC\", \"Fe\", \"Fountain\", \"Friendly\", \"Gabi\", \"Group\", \"Hibachi\", \"Hibachi\", \"Hills\", \"Hills\", \"Hong\", \"Indian\", \"Jessica\", \"Jessica\", \"Jim\", \"Johnny\", \"Josh\", \"Katie\", \"Keg\", \"Life\", \"Little\", \"MSG\", \"Masala\", \"McDonald\", \"McDonald\", \"McDonald\", \"Mon\", \"Mon\", \"Mon\", \"Moon\", \"Nachos\", \"Nancy\", \"Noir\", \"Noodles\", \"Noodles\", \"Owner\", \"Owner\", \"Owner\", \"Pad\", \"Pad\", \"Pad\", \"Pasta\", \"Penn\", \"Pistachio\", \"Pumpkin\", \"Ramen\", \"Ramen\", \"Resort\", \"Robert\", \"Robert\", \"Robert\", \"Robert\", \"Robert\", \"Roll\", \"Roll\", \"Roll\", \"Roll\", \"Roll\", \"Smash\", \"Southern\", \"Style\", \"Style\", \"Style\", \"TV\", \"Tacos\", \"Tacos\", \"Tacos\", \"Thai\", \"Thai\", \"Thai\", \"Thai\", \"Tom\", \"Tom\", \"Tom\", \"Tom\", \"Tom\", \"Wait\", \"Yum\", \"Yum\", \"Yum\", \"accent\", \"agent\", \"ai\", \"ai\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"apartment\", \"apartment\", \"apartment\", \"apartment\", \"apartment\", \"appointment\", \"appointment\", \"appointment\", \"appointment\", \"appointment\", \"arepa\", \"arepa\", \"asada\", \"asada\", \"asada\", \"asada\", \"asado\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"assistant\", \"assistant\", \"attach\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"ayce\", \"ayce\", \"ayce\", \"ayce\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"barber\", \"barber\", \"barber\", \"barber\", \"battery\", \"battery\", \"battery\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"benny\", \"bi\", \"bi\", \"biryani\", \"bisque\", \"bisque\", \"bowling\", \"bowling\", \"bowling\", \"brake\", \"bread\", \"bread\", \"bread\", \"bread\", \"bread\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"brewery\", \"brewery\", \"brewery\", \"brioche\", \"brunch\", \"brunch\", \"brunch\", \"brunch\", \"brunch\", \"btw\", \"btw\", \"buffet\", \"buffet\", \"buffet\", \"buffet\", \"buffet\", \"bum\", \"bumper\", \"bumper\", \"burger\", \"burger\", \"burger\", \"burger\", \"burger\", \"business\", \"business\", \"business\", \"business\", \"business\", \"butternut\", \"cake\", \"cake\", \"cake\", \"cake\", \"cake\", \"calamari\", \"calamari\", \"calamari\", \"calamari\", \"calamari\", \"call\", \"call\", \"call\", \"call\", \"call\", \"campus\", \"car\", \"car\", \"car\", \"car\", \"car\", \"care\", \"care\", \"care\", \"care\", \"care\", \"caring\", \"carne\", \"carne\", \"carne\", \"carne\", \"cat\", \"cat\", \"cat\", \"cat\", \"ce\", \"ce\", \"charge\", \"charge\", \"charge\", \"charge\", \"charge\", \"cheese\", \"cheese\", \"cheese\", \"cheese\", \"cheese\", \"chef\", \"chef\", \"chef\", \"chef\", \"chef\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chowder\", \"chowder\", \"chowder\", \"clam\", \"clam\", \"clerk\", \"clerk\", \"client\", \"client\", \"client\", \"client\", \"client\", \"come\", \"come\", \"come\", \"come\", \"come\", \"company\", \"company\", \"company\", \"company\", \"company\", \"connoisseur\", \"contact\", \"contact\", \"contact\", \"contact\", \"contest\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"cupcake\", \"cupcake\", \"cupcake\", \"cupcake\", \"curly\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"dan\", \"day\", \"day\", \"day\", \"day\", \"day\", \"de\", \"de\", \"de\", \"de\", \"de\", \"definately\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"dent\", \"desk\", \"desk\", \"desk\", \"desk\", \"desk\", \"didn\", \"didn\", \"didn\", \"didn\", \"didn\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"disgust\", \"disgusted\", \"dish\", \"dish\", \"dish\", \"dish\", \"dish\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"don\", \"don\", \"don\", \"don\", \"don\", \"donut\", \"donut\", \"donut\", \"donut\", \"donut\", \"doughnut\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"eating\", \"egg\", \"egg\", \"egg\", \"egg\", \"egg\", \"email\", \"email\", \"email\", \"email\", \"email\", \"en\", \"en\", \"en\", \"en\", \"en\", \"enchilada\", \"enchilada\", \"enchilada\", \"enchilada\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"est\", \"est\", \"est\", \"est\", \"et\", \"et\", \"et\", \"et\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"eyebrow\", \"eyebrow\", \"eyebrow\", \"eyebrow\", \"facial\", \"facial\", \"facial\", \"facial\", \"fajita\", \"fajita\", \"fajita\", \"fajita\", \"falafel\", \"falafel\", \"falafel\", \"farmer\", \"fi\", \"file\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fix\", \"fix\", \"fix\", \"fix\", \"fix\", \"flair\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flavour\", \"flavour\", \"flavour\", \"flavour\", \"flavour\", \"flower\", \"flower\", \"food\", \"food\", \"food\", \"food\", \"food\", \"freezer\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"froyo\", \"fry\", \"fry\", \"fry\", \"fry\", \"fry\", \"fun\", \"fun\", \"fun\", \"fun\", \"fun\", \"garlic\", \"garlic\", \"garlic\", \"garlic\", \"garlic\", \"get\", \"get\", \"get\", \"get\", \"get\", \"gnocchi\", \"gnocchi\", \"gnocchi\", \"go\", \"go\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"good\", \"good\", \"graduation\", \"great\", \"great\", \"great\", \"great\", \"great\", \"grub\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"haircut\", \"haircut\", \"haircut\", \"haircut\", \"haircut\", \"harness\", \"heartbeat\", \"help\", \"help\", \"help\", \"help\", \"help\", \"hotdog\", \"hotdog\", \"hotel\", \"hotel\", \"hotel\", \"hotel\", \"hotel\", \"huge\", \"huge\", \"huge\", \"huge\", \"huge\", \"humor\", \"increase\", \"increase\", \"increase\", \"information\", \"information\", \"information\", \"information\", \"information\", \"installation\", \"installer\", \"insurance\", \"insurance\", \"insurance\", \"insurance\", \"insurance\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"italian\", \"italian\", \"italian\", \"italian\", \"italian\", \"j\", \"j\", \"jalape\", \"jalape\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"je\", \"kabob\", \"know\", \"know\", \"know\", \"know\", \"know\", \"l\", \"l\", \"l\", \"l\", \"l\", \"la\", \"la\", \"la\", \"la\", \"la\", \"laser\", \"laser\", \"lash\", \"lash\", \"le\", \"le\", \"le\", \"le\", \"leave\", \"leave\", \"leave\", \"leave\", \"leave\", \"les\", \"les\", \"les\", \"like\", \"like\", \"like\", \"like\", \"like\", \"lil\", \"liquor\", \"liquor\", \"liquor\", \"liquor\", \"little\", \"little\", \"little\", \"little\", \"little\", \"location\", \"location\", \"location\", \"location\", \"location\", \"look\", \"look\", \"look\", \"look\", \"look\", \"love\", \"love\", \"love\", \"love\", \"love\", \"lox\", \"lunch\", \"lunch\", \"lunch\", \"lunch\", \"lunch\", \"m\", \"m\", \"m\", \"m\", \"m\", \"magician\", \"making\", \"manager\", \"manager\", \"manager\", \"manager\", \"manager\", \"mattress\", \"meal\", \"meal\", \"meal\", \"meal\", \"meal\", \"meat\", \"meat\", \"meat\", \"meat\", \"meat\", \"mechanic\", \"mechanic\", \"mechanic\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"mexican\", \"mexican\", \"mexican\", \"mexican\", \"mexican\", \"mojito\", \"money\", \"money\", \"money\", \"money\", \"money\", \"need\", \"need\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"night\", \"night\", \"night\", \"night\", \"night\", \"nigiri\", \"nigiri\", \"nigiri\", \"nite\", \"office\", \"office\", \"office\", \"office\", \"office\", \"okra\", \"okra\", \"okra\", \"omelet\", \"omelet\", \"option\", \"option\", \"option\", \"option\", \"option\", \"order\", \"order\", \"order\", \"order\", \"order\", \"ou\", \"oxtail\", \"oxtail\", \"painting\", \"pas\", \"pas\", \"pas\", \"pas\", \"pasta\", \"pasta\", \"pasta\", \"pasta\", \"pasta\", \"pastry\", \"pastry\", \"pastry\", \"pastry\", \"pastry\", \"pedicure\", \"pedicure\", \"pedicure\", \"pedicure\", \"people\", \"people\", \"people\", \"people\", \"people\", \"perform\", \"perform\", \"pho\", \"pho\", \"pho\", \"pho\", \"pho\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"physician\", \"physician\", \"pizza\", \"pizza\", \"pizza\", \"pizza\", \"pizza\", \"place\", \"place\", \"place\", \"place\", \"place\", \"play\", \"play\", \"play\", \"play\", \"play\", \"plumbing\", \"plumbing\", \"pool\", \"pool\", \"pool\", \"pool\", \"pool\", \"pork\", \"pork\", \"pork\", \"pork\", \"pork\", \"port\", \"portion\", \"portion\", \"portion\", \"portion\", \"portion\", \"possibility\", \"potato\", \"potato\", \"potato\", \"potato\", \"potato\", \"price\", \"price\", \"price\", \"price\", \"price\", \"prime\", \"prime\", \"prime\", \"prime\", \"prime\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"professional\", \"professional\", \"professional\", \"professional\", \"professional\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"purchase\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"que\", \"que\", \"quote\", \"quote\", \"quote\", \"quote\", \"quote\", \"raman\", \"raman\", \"raman\", \"raman\", \"raman\", \"rank\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"rent\", \"rent\", \"rent\", \"rent\", \"rent\", \"repair\", \"repair\", \"repair\", \"repair\", \"repair\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"rib\", \"rib\", \"rib\", \"rib\", \"rib\", \"rice\", \"rice\", \"rice\", \"rice\", \"rice\", \"rim\", \"roll\", \"roll\", \"roll\", \"roll\", \"roll\", \"room\", \"room\", \"room\", \"room\", \"room\", \"rude\", \"rude\", \"rude\", \"rude\", \"rude\", \"rug\", \"s\", \"s\", \"s\", \"s\", \"s\", \"salad\", \"salad\", \"salad\", \"salad\", \"salad\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sashimi\", \"sashimi\", \"sashimi\", \"sashimi\", \"sashimi\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"say\", \"say\", \"say\", \"say\", \"say\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"schedule\", \"scorpion\", \"scorpion\", \"secure\", \"sensitive\", \"server\", \"server\", \"server\", \"server\", \"server\", \"service\", \"service\", \"service\", \"service\", \"service\", \"sever\", \"shawarma\", \"shawarma\", \"shawarma\", \"side\", \"side\", \"side\", \"side\", \"side\", \"sirloin\", \"sirloin\", \"small\", \"small\", \"small\", \"small\", \"small\", \"smokey\", \"snow\", \"snow\", \"snow\", \"soup\", \"soup\", \"soup\", \"soup\", \"soup\", \"spaghetti\", \"spaghetti\", \"spaghetti\", \"special\", \"special\", \"special\", \"special\", \"special\", \"spicy\", \"spicy\", \"spicy\", \"spicy\", \"spicy\", \"spray\", \"spray\", \"spray\", \"spray\", \"spray\", \"squid\", \"staff\", \"staff\", \"staff\", \"staff\", \"staff\", \"stay\", \"stay\", \"stay\", \"stay\", \"stay\", \"store\", \"store\", \"store\", \"store\", \"store\", \"stuffy\", \"stunning\", \"stunning\", \"sur\", \"sur\", \"surgery\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"sweet\", \"sweet\", \"sweet\", \"sweet\", \"sweet\", \"t\", \"t\", \"t\", \"t\", \"t\", \"taco\", \"taco\", \"taco\", \"taco\", \"taco\", \"taiwanese\", \"taiwanese\", \"take\", \"take\", \"take\", \"take\", \"take\", \"tart\", \"tart\", \"tartar\", \"taste\", \"taste\", \"taste\", \"taste\", \"taste\", \"tasty\", \"tasty\", \"tasty\", \"tasty\", \"tasty\", \"tater\", \"tater\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tech\", \"tech\", \"tech\", \"tech\", \"technician\", \"technician\", \"technician\", \"technician\", \"technician\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"terrasse\", \"theater\", \"theater\", \"theater\", \"theater\", \"theater\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thread\", \"thread\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tire\", \"tire\", \"tire\", \"tire\", \"tire\", \"tomato\", \"tomato\", \"tomato\", \"tomato\", \"tomato\", \"tot\", \"tot\", \"tot\", \"tot\", \"tour\", \"tour\", \"tour\", \"tour\", \"tour\", \"trail\", \"trail\", \"try\", \"try\", \"try\", \"try\", \"try\", \"tub\", \"tub\", \"tub\", \"tub\", \"un\", \"un\", \"un\", \"un\", \"un\", \"und\", \"une\", \"une\", \"uneven\", \"uptown\", \"vary\", \"vary\", \"ve\", \"ve\", \"ve\", \"ve\", \"ve\", \"verify\", \"vet\", \"vet\", \"vet\", \"vision\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"vous\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"want\", \"want\", \"want\", \"want\", \"want\", \"warranty\", \"warranty\", \"warranty\", \"warranty\", \"well\", \"well\", \"well\", \"well\", \"well\", \"wi\", \"wine\", \"wine\", \"wine\", \"wine\", \"wine\", \"work\", \"work\", \"work\", \"work\", \"work\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yuk\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 4, 5, 1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el19941398226372190569997389883\", ldavis_el19941398226372190569997389883_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el19941398226372190569997389883\", ldavis_el19941398226372190569997389883_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el19941398226372190569997389883\", ldavis_el19941398226372190569997389883_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2     -0.134367  0.032756       1        1  33.773605\n",
       "3      0.019387 -0.051339       2        1  25.880258\n",
       "4      0.032215 -0.063037       3        1  23.195110\n",
       "0     -0.010041  0.001434       4        1  12.608945\n",
       "1      0.092806  0.080186       5        1   4.542082, topic_info=      Term         Freq        Total Category  logprob  loglift\n",
       "199  great  4040.000000  4040.000000  Default  30.0000  30.0000\n",
       "138   food  4580.000000  4580.000000  Default  29.0000  29.0000\n",
       "43    good  6177.000000  6177.000000  Default  28.0000  28.0000\n",
       "720  sushi   408.000000   408.000000  Default  27.0000  27.0000\n",
       "125  place  5241.000000  5241.000000  Default  26.0000  26.0000\n",
       "..     ...          ...          ...      ...      ...      ...\n",
       "40    come    79.499674  3916.912977   Topic5  -5.6650  -0.8055\n",
       "17    like    77.801394  3775.652577   Topic5  -5.6866  -0.7904\n",
       "678   love    71.714606  2101.265261   Topic5  -5.7681  -0.2858\n",
       "368  order    70.655524  3438.615423   Topic5  -5.7830  -0.7932\n",
       "383  staff    66.992312  1521.280237   Topic5  -5.8362  -0.0309\n",
       "\n",
       "[483 rows x 6 columns], token_table=       Topic      Freq    Term\n",
       "term                          \n",
       "7052       4  0.966998    Alex\n",
       "14472      1  0.977959  Amazon\n",
       "1950       3  0.071287     Ami\n",
       "1950       5  0.926737     Ami\n",
       "6326       2  0.912650  Bobbie\n",
       "...      ...       ...     ...\n",
       "112        2  0.116351    year\n",
       "112        3  0.085257    year\n",
       "112        4  0.200606    year\n",
       "112        5  0.028085    year\n",
       "12598      2  0.982223     yuk\n",
       "\n",
       "[1208 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 4, 5, 1, 2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim_models\n",
    "# Use pyLDAvis (or a ploting tool of your choice) to visualize your results \n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim_models.prepare(lda, corpus, id2word)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f44a26c754500ff0bf585296075bf754",
     "grade": false,
     "grade_id": "cell-bf9e63d9645bba84",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### 3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like topic 5 is about japanese food/ restaurant because they have sushi and fish. Topic 3 is about mexican food/restaurant. Topic two is about italian food/restaurant. These two topics 2,3 is nearly the same because they have more similar words: cheese, chicken, salad. Topic 4 is fastfood restaurant. It is close to these two topics above but because they have some similar word: chicken, salad. Topic 1 is about other companies: hotel, doctor, pool, anything not a restaurant."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
