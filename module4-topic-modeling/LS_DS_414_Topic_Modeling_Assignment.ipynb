{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Topic Modeling\n",
    "## *Data Science Unit 4 Sprint 1 Assignment 4*\n",
    "\n",
    "**Analyze a corpus of Amazon reviews from Unit 4 Sprint 1 Module 1's using topic modeling: \n",
    "\n",
    "- Load in the Amazon Review dataset\n",
    "- Clean the dataset \n",
    "- Vectorize the dataset \n",
    "- Fit a Gensim LDA topic model on Amazon Reviews\n",
    "- Select appropriate number of topics\n",
    "- Create some dope visualization of the topics\n",
    "- Write a few bullets on your findings in markdown at the end\n",
    "- **Note**: You don't *have* to use generators for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import spacy\n",
    "spacy.util.fix_random_seed(0)\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Load the Amazon Review corpus \n",
    "This dataset is located in the Sprint 1 Module 1 directory. \n",
    "\n",
    "If the provided relative path doesn't work for you, then you'll have to provide the file path so pandas can read in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../module1-text-data/data/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     id             dateAdded           dateUpdated  \\\n",
       "0  AVpgNzjwLJeJML43Kpxn  2015-10-30T08:59:32Z  2019-04-25T09:08:16Z   \n",
       "1  AVpgNzjwLJeJML43Kpxn  2015-10-30T08:59:32Z  2019-04-25T09:08:16Z   \n",
       "\n",
       "                                                name                  asins  \\\n",
       "0  AmazonBasics AAA Performance Alkaline Batterie...  B00QWO9P0O,B00LH3DMUO   \n",
       "1  AmazonBasics AAA Performance Alkaline Batterie...  B00QWO9P0O,B00LH3DMUO   \n",
       "\n",
       "          brand                                         categories  \\\n",
       "0  Amazonbasics  AA,AAA,Health,Electronics,Health & Household,C...   \n",
       "1  Amazonbasics  AA,AAA,Health,Electronics,Health & Household,C...   \n",
       "\n",
       "  primaryCategories                                          imageURLs  \\\n",
       "0   Health & Beauty  https://images-na.ssl-images-amazon.com/images...   \n",
       "1   Health & Beauty  https://images-na.ssl-images-amazon.com/images...   \n",
       "\n",
       "                                                keys  ... reviews.didPurchase  \\\n",
       "0  amazonbasics/hl002619,amazonbasicsaaaperforman...  ...                 NaN   \n",
       "1  amazonbasics/hl002619,amazonbasicsaaaperforman...  ...                 NaN   \n",
       "\n",
       "  reviews.doRecommend reviews.id reviews.numHelpful reviews.rating  \\\n",
       "0                 NaN        NaN                NaN              3   \n",
       "1                 NaN        NaN                NaN              4   \n",
       "\n",
       "                                  reviews.sourceURLs  \\\n",
       "0  https://www.amazon.com/product-reviews/B00QWO9...   \n",
       "1  https://www.amazon.com/product-reviews/B00QWO9...   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I order 3 of them and one of the item is bad q...   \n",
       "1  Bulk is always the less expensive way to go fo...   \n",
       "\n",
       "                                       reviews.title  reviews.username  \\\n",
       "0  ... 3 of them and one of the item is bad quali...        Byger yang   \n",
       "1  ... always the less expensive way to go for pr...              ByMG   \n",
       "\n",
       "                                          sourceURLs  \n",
       "0  https://www.barcodable.com/upc/841710106442,ht...  \n",
       "1  https://www.barcodable.com/upc/841710106442,ht...  \n",
       "\n",
       "[2 rows x 24 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>dateAdded</th>\n      <th>dateUpdated</th>\n      <th>name</th>\n      <th>asins</th>\n      <th>brand</th>\n      <th>categories</th>\n      <th>primaryCategories</th>\n      <th>imageURLs</th>\n      <th>keys</th>\n      <th>...</th>\n      <th>reviews.didPurchase</th>\n      <th>reviews.doRecommend</th>\n      <th>reviews.id</th>\n      <th>reviews.numHelpful</th>\n      <th>reviews.rating</th>\n      <th>reviews.sourceURLs</th>\n      <th>reviews.text</th>\n      <th>reviews.title</th>\n      <th>reviews.username</th>\n      <th>sourceURLs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AVpgNzjwLJeJML43Kpxn</td>\n      <td>2015-10-30T08:59:32Z</td>\n      <td>2019-04-25T09:08:16Z</td>\n      <td>AmazonBasics AAA Performance Alkaline Batterie...</td>\n      <td>B00QWO9P0O,B00LH3DMUO</td>\n      <td>Amazonbasics</td>\n      <td>AA,AAA,Health,Electronics,Health &amp; Household,C...</td>\n      <td>Health &amp; Beauty</td>\n      <td>https://images-na.ssl-images-amazon.com/images...</td>\n      <td>amazonbasics/hl002619,amazonbasicsaaaperforman...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>https://www.amazon.com/product-reviews/B00QWO9...</td>\n      <td>I order 3 of them and one of the item is bad q...</td>\n      <td>... 3 of them and one of the item is bad quali...</td>\n      <td>Byger yang</td>\n      <td>https://www.barcodable.com/upc/841710106442,ht...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AVpgNzjwLJeJML43Kpxn</td>\n      <td>2015-10-30T08:59:32Z</td>\n      <td>2019-04-25T09:08:16Z</td>\n      <td>AmazonBasics AAA Performance Alkaline Batterie...</td>\n      <td>B00QWO9P0O,B00LH3DMUO</td>\n      <td>Amazonbasics</td>\n      <td>AA,AAA,Health,Electronics,Health &amp; Household,C...</td>\n      <td>Health &amp; Beauty</td>\n      <td>https://images-na.ssl-images-amazon.com/images...</td>\n      <td>amazonbasics/hl002619,amazonbasicsaaaperforman...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>https://www.amazon.com/product-reviews/B00QWO9...</td>\n      <td>Bulk is always the less expensive way to go fo...</td>\n      <td>... always the less expensive way to go for pr...</td>\n      <td>ByMG</td>\n      <td>https://www.barcodable.com/upc/841710106442,ht...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 24 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Clean data\n",
    "\n",
    "- Create a function called `clean_data` that uses regex expressions to clean your data in preparation for the vectorizer. \n",
    "\n",
    "- Save the clean text data to a column in your dataframe named `clean_text`\n",
    "\n",
    "- Feel free to re-use old code that you have written in previous modules  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2513c0ebd04cf8645fe0a1feb1e1a36",
     "grade": false,
     "grade_id": "cell-fa0950cfe5ef7725",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "def clean_data(text):\n",
    "    \"\"\"\n",
    "    Accepts a single text document and performs several regex substitutions in order to clean the document. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: string or object \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    text: string or object\n",
    "    \"\"\"\n",
    "    \n",
    "    # order of operations - apply the expression from top to bottom\n",
    "    non_alpha = '[^a-zA-Z]'\n",
    "    multi_white_spaces = \"[ ]{2,}\"\n",
    "\n",
    "    text = re.sub(non_alpha, ' ', text)\n",
    "    text = re.sub(multi_white_spaces, \" \", text)\n",
    "    \n",
    "    # apply case normalization \n",
    "    return text.lower().lstrip().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6ceefe5476e71efcb3d36c3e203616f",
     "grade": false,
     "grade_id": "cell-5b8e2bc0f9a745f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# save to cleaned review articles to a feature named `clean_text`\n",
    "# YOUR CODE HERE\n",
    "df['clean_text'] = df['reviews.text'].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "alphebetical_chars = [\"ABCDEFGHIJKLMNOP\"]\n",
    "# check if any of these alphabetical chars exist in your clean chars\n",
    "assert df.clean_text.isin(alphebetical_chars).sum() == 0, \"Did you case normalize your text inside of your clean_data function?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Determine number of topics\n",
    "\n",
    "We are going to run an experiment to determine how many topics exists within the `primaryCategories` of `Electronics`. This is the largest primary category containing nearly 14K documents, so we should have plenty of data. \n",
    "\n",
    "Just as we did in the guided project, we'll be running a gridseach over the number of topics and scoring each model using the Coherence metric to determine which number of topics we should use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# create a mask for docs that are in the Electronics primaryCategories - save result to `electronics_mask`\n",
    "electronics_mask = df.primaryCategories.isin([\"Electronics\"])\n",
    "\n",
    "# use msak to isolate all the documents in the Electronics primaryCategories - save result to `df_electronics`\n",
    "df_electronics = df[electronics_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                        id             dateAdded           dateUpdated  \\\n",
       "8343  AVpe7nGV1cnluZ0-aG2o  2014-10-28T11:14:38Z  2019-04-25T09:05:28Z   \n",
       "8344  AVpe7nGV1cnluZ0-aG2o  2014-10-28T11:14:38Z  2019-04-25T09:05:28Z   \n",
       "8345  AVpe7nGV1cnluZ0-aG2o  2014-10-28T11:14:38Z  2019-04-25T09:05:28Z   \n",
       "\n",
       "                                                 name                  asins  \\\n",
       "8343  AmazonBasics Nylon CD/DVD Binder (400 Capacity)  B00DIHVMEA,B00EZ1ZTV0   \n",
       "8344  AmazonBasics Nylon CD/DVD Binder (400 Capacity)  B00DIHVMEA,B00EZ1ZTV0   \n",
       "8345  AmazonBasics Nylon CD/DVD Binder (400 Capacity)  B00DIHVMEA,B00EZ1ZTV0   \n",
       "\n",
       "             brand                                         categories  \\\n",
       "8343  Amazonbasics  Audio & Video Accessories,TV, Video & Home Aud...   \n",
       "8344  Amazonbasics  Audio & Video Accessories,TV, Video & Home Aud...   \n",
       "8345  Amazonbasics  Audio & Video Accessories,TV, Video & Home Aud...   \n",
       "\n",
       "     primaryCategories                                          imageURLs  \\\n",
       "8343       Electronics  http://ecx.images-amazon.com/images/I/41jQha7Z...   \n",
       "8344       Electronics  http://ecx.images-amazon.com/images/I/41jQha7Z...   \n",
       "8345       Electronics  http://ecx.images-amazon.com/images/I/41jQha7Z...   \n",
       "\n",
       "                                                   keys  ...  \\\n",
       "8343  amazonbasicsnyloncddvdbinder400capacity/b00ez1...  ...   \n",
       "8344  amazonbasicsnyloncddvdbinder400capacity/b00ez1...  ...   \n",
       "8345  amazonbasicsnyloncddvdbinder400capacity/b00ez1...  ...   \n",
       "\n",
       "     reviews.doRecommend reviews.id reviews.numHelpful reviews.rating  \\\n",
       "8343                 NaN        NaN                NaN              5   \n",
       "8344                 NaN        NaN                NaN              5   \n",
       "8345                 NaN        NaN                NaN              5   \n",
       "\n",
       "                                     reviews.sourceURLs  \\\n",
       "8343  https://www.ebay.com/itm/Amazonbasics-Nylon-Cd...   \n",
       "8344  http://www.amazon.co.uk/gp/product-reviews/B00...   \n",
       "8345  https://www.ebay.com/itm/Amazonbasics-Nylon-Cd...   \n",
       "\n",
       "                                           reviews.text  \\\n",
       "8343  Great case to keep everything in its place! My...   \n",
       "8344  After discarding and getting rid of broken cd ...   \n",
       "8345     A few dollars more, but I am boycotting amazon   \n",
       "\n",
       "                     reviews.title  reviews.username  \\\n",
       "8343             Excellent product           qs341_5   \n",
       "8344  It was a much needed storage          Diablita   \n",
       "8345               it was worth it  coldbloodblazing   \n",
       "\n",
       "                                             sourceURLs  \\\n",
       "8343  https://www.ebay.com/itm/AmazonBasics-Nylon-CD...   \n",
       "8344  https://www.ebay.com/itm/AmazonBasics-Nylon-CD...   \n",
       "8345  https://www.ebay.com/itm/AmazonBasics-Nylon-CD...   \n",
       "\n",
       "                                             clean_text  \n",
       "8343  great case to keep everything in its place my ...  \n",
       "8344  after discarding and getting rid of broken cd ...  \n",
       "8345      a few dollars more but i am boycotting amazon  \n",
       "\n",
       "[3 rows x 25 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>dateAdded</th>\n      <th>dateUpdated</th>\n      <th>name</th>\n      <th>asins</th>\n      <th>brand</th>\n      <th>categories</th>\n      <th>primaryCategories</th>\n      <th>imageURLs</th>\n      <th>keys</th>\n      <th>...</th>\n      <th>reviews.doRecommend</th>\n      <th>reviews.id</th>\n      <th>reviews.numHelpful</th>\n      <th>reviews.rating</th>\n      <th>reviews.sourceURLs</th>\n      <th>reviews.text</th>\n      <th>reviews.title</th>\n      <th>reviews.username</th>\n      <th>sourceURLs</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8343</th>\n      <td>AVpe7nGV1cnluZ0-aG2o</td>\n      <td>2014-10-28T11:14:38Z</td>\n      <td>2019-04-25T09:05:28Z</td>\n      <td>AmazonBasics Nylon CD/DVD Binder (400 Capacity)</td>\n      <td>B00DIHVMEA,B00EZ1ZTV0</td>\n      <td>Amazonbasics</td>\n      <td>Audio &amp; Video Accessories,TV, Video &amp; Home Aud...</td>\n      <td>Electronics</td>\n      <td>http://ecx.images-amazon.com/images/I/41jQha7Z...</td>\n      <td>amazonbasicsnyloncddvdbinder400capacity/b00ez1...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>https://www.ebay.com/itm/Amazonbasics-Nylon-Cd...</td>\n      <td>Great case to keep everything in its place! My...</td>\n      <td>Excellent product</td>\n      <td>qs341_5</td>\n      <td>https://www.ebay.com/itm/AmazonBasics-Nylon-CD...</td>\n      <td>great case to keep everything in its place my ...</td>\n    </tr>\n    <tr>\n      <th>8344</th>\n      <td>AVpe7nGV1cnluZ0-aG2o</td>\n      <td>2014-10-28T11:14:38Z</td>\n      <td>2019-04-25T09:05:28Z</td>\n      <td>AmazonBasics Nylon CD/DVD Binder (400 Capacity)</td>\n      <td>B00DIHVMEA,B00EZ1ZTV0</td>\n      <td>Amazonbasics</td>\n      <td>Audio &amp; Video Accessories,TV, Video &amp; Home Aud...</td>\n      <td>Electronics</td>\n      <td>http://ecx.images-amazon.com/images/I/41jQha7Z...</td>\n      <td>amazonbasicsnyloncddvdbinder400capacity/b00ez1...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>http://www.amazon.co.uk/gp/product-reviews/B00...</td>\n      <td>After discarding and getting rid of broken cd ...</td>\n      <td>It was a much needed storage</td>\n      <td>Diablita</td>\n      <td>https://www.ebay.com/itm/AmazonBasics-Nylon-CD...</td>\n      <td>after discarding and getting rid of broken cd ...</td>\n    </tr>\n    <tr>\n      <th>8345</th>\n      <td>AVpe7nGV1cnluZ0-aG2o</td>\n      <td>2014-10-28T11:14:38Z</td>\n      <td>2019-04-25T09:05:28Z</td>\n      <td>AmazonBasics Nylon CD/DVD Binder (400 Capacity)</td>\n      <td>B00DIHVMEA,B00EZ1ZTV0</td>\n      <td>Amazonbasics</td>\n      <td>Audio &amp; Video Accessories,TV, Video &amp; Home Aud...</td>\n      <td>Electronics</td>\n      <td>http://ecx.images-amazon.com/images/I/41jQha7Z...</td>\n      <td>amazonbasicsnyloncddvdbinder400capacity/b00ez1...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>https://www.ebay.com/itm/Amazonbasics-Nylon-Cd...</td>\n      <td>A few dollars more, but I am boycotting amazon</td>\n      <td>it was worth it</td>\n      <td>coldbloodblazing</td>\n      <td>https://www.ebay.com/itm/AmazonBasics-Nylon-CD...</td>\n      <td>a few dollars more but i am boycotting amazon</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 25 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df_electronics.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Tokenize your documents \n",
    "\n",
    "Remember that you'll need to use the [**corpora**](https://radimrehurek.com/gensim/corpora/dictionary.html) class from the Gensim library. So definitely check out the docs to learn more about this tool. There is an example on how to do this in the guided project.\n",
    "\n",
    "But before we can use the [**corpora**](https://radimrehurek.com/gensim/corpora/dictionary.html) class, we must first tokenize our articles. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f9f136b6fa56339c87e209fe6f2f12d",
     "grade": false,
     "grade_id": "cell-d7d585b64d6f0119",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# identify how many processors your machine has - save the result to `n_processors`\n",
    "\n",
    "# subtract 1 from n_processors - save the result to `nb_workers`\n",
    "\n",
    "# initialize just like we did in the guided project\n",
    "# COLAB only has 2 processors, set nb_workers=2 and hope that it doesn't crash your notebook (otherwise don't use this tool)\n",
    "# pandarallel.initialize(progress_bar=True, nb_workers=nb_workers)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/minh14496/.local/share/virtualenvs/DS-Unit-4-Sprint-1-NLP-1TUUJhOU/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# load in the spaCy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1400), Label(value='0 / 1400'))), …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cab08a0dcac43c59d013a83c500f53f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 3.63 s, sys: 713 ms, total: 4.34 s\nWall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create our tokens in the form of lemmas \n",
    "df_electronics['lemmas'] = df_electronics['clean_text'].parallel_apply(lambda x: [token.lemma_ for token in nlp(x) if (token.is_stop != True) and (token.is_punct != True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the corpora class to prep your data for LDA\n",
    "\n",
    "You'll need to create the same `id2word` and `corpus` objects that we created in the guided projects. So be sure to reference the guided project notebook if you need to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53bf811aebb39e7ed6fc592851f82f8b",
     "grade": false,
     "grade_id": "cell-9d92f28649aa999e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create lemma dictionary using Dictionary - save result to `id2word`\n",
    "\n",
    "# Create Term Document Frequency list - save result to `corpus`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch the number of topics \n",
    "\n",
    "Just as we did in the guided project, we're going to run a for loop over a range of possible number of topics and then plot the coherence values to determine which number of topics leads to the most sensible grouping of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                        id2word=id2word,\n",
    "                                                        num_topics=num_topics, \n",
    "                                                        chunksize=100,\n",
    "                                                        passes=10,\n",
    "                                                        random_state=1234,\n",
    "                                                        per_word_topics=True,\n",
    "                                                        workers=10)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=df_electronics['lemmas'], start=2, limit=22, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=2; limit=22;  step=2;\n",
    "x = range(start, limit, step)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.grid()\n",
    "plt.title(\"Coherence Score vs. Number of Topics\")\n",
    "plt.xticks(x)\n",
    "plt.plot(x, coherence_values, \"-o\")\n",
    "\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45f72ff929ab8b765d0f0b5f35137f0d",
     "grade": false,
     "grade_id": "cell-e97661faebf1ac3c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# use np.argmax() to get index of largest coherence value from coherence_values - save result to `max_cohereance_val_index`\n",
    "\n",
    "# use `max_cohereance_val_index` to index model_list for the corresponding model - save result to `lda_trained_model`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pyLDAvis to visual your topics \n",
    "\n",
    "Take a look at the topic bubbles and bar char for the terms on the right hand side.  \n",
    "\n",
    "- Describe the topic bubbles. \n",
    "- Do they overlap or not? \n",
    "- What does it mean when they overlap? \n",
    "- What does it mean when they don't overlap?\n",
    "- Are the terms in each topic distinct from the topics in the other topic bubbles?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ad72f47a2b4930f8503c7733e5257cc",
     "grade": false,
     "grade_id": "cell-5213c8c2af79e714",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# plot your topics here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Topic id/name dictionary \n",
    "\n",
    "When populating your topic id/name dictionary, use the index ordering as shown in the viz tool. \n",
    "\n",
    "We'll use a function to map the the viz tool index ordering with the train LDA model ordering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc10e9728e5dcf06baf0800c3cdb88ce",
     "grade": false,
     "grade_id": "cell-4905c0c1050f0d03",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create a dictionary \n",
    "# keys - use topic ids from pyLDAvis visualization \n",
    "# values - topic names that you create \n",
    "# save dictionary to `vis_topic_name_dict`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_id_lookup_dict(vis, vis_topic_name_dict):\n",
    "    \"\"\"\n",
    "    Both the starting index and the ordering of topic ids bewteen the trained LDA model \n",
    "    and the viz tool are different. So we need to create a look up dictionary that maps \n",
    "    the correct association between topic ids from both sources. \n",
    "    \"\"\"\n",
    "    # value is order of topic ids accoridng to pyLDAvis tool \n",
    "    # key is order of topic ids according to lda model\n",
    "    model_vis_tool_topic_id_lookup = vis.topic_coordinates.topics.to_dict()\n",
    "\n",
    "    # invert dictionary so that \n",
    "    # key is order of topic ids accoridng to pyLDAvis tool \n",
    "    # value is order of topic ids according to lda model\n",
    "    topic_id_lookup =  {v:k for k, v in model_vis_tool_topic_id_lookup.items()}\n",
    "    \n",
    "    return {v:vis_topic_name_dict[k]  for k, v in topic_id_lookup.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55f23aa9324a225091037f0c5daf2192",
     "grade": false,
     "grade_id": "cell-d38acb7b250b4079",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Each Document a Topic Name\n",
    "\n",
    "Now that we have a topic id/name look up dict that is aligned with the index ordering of the trained LDA model, we can move forward to giving each topic a topic name. \n",
    "\n",
    "The function below has been given to you. However, you highly encouraged to read through it and make sure that you understand what it is doing each step of the way. In fact, a good way to do this is to copy and paste the code inside of the function into a new cell, comment out all the lines of code and line by line, uncomment the code and see the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_ids_for_docs(lda_model, corpus):\n",
    "    \n",
    "    \"\"\"\n",
    "    Passes a Bag-of-Words vector into a trained LDA model in order to get the topic id of that document. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lda_model: Gensim object\n",
    "        Must be a trained model \n",
    "        \n",
    "    corpus: nested lists of tuples, \n",
    "        i.e. [[(),(), ..., ()], [(),(), ..., ()], ..., [(),(), ..., ()]]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    topic_id_list: list\n",
    "        Contains topic ids for all document vectors in corpus \n",
    "    \"\"\"\n",
    "    \n",
    "    # store topic ids for each document\n",
    "    doc_topic_ids = []\n",
    "\n",
    "    # iterature through the bow vectors for each doc\n",
    "    for doc_bow in corpus:\n",
    "        \n",
    "        # store the topic ids for the doc\n",
    "        topic_ids = []\n",
    "        # store the topic probabilities for the doc\n",
    "        topic_probs = []\n",
    "\n",
    "        # list of tuples\n",
    "        # each tuple has a topic id and the prob that the doc belongs to that topic \n",
    "        topic_id_prob_tuples = lda_trained_model.get_document_topics(doc_bow)\n",
    "        \n",
    "        # iterate through the topic id/prob pairs \n",
    "        for topic_id_prob in topic_id_prob_tuples:\n",
    "            \n",
    "            # index for topic id\n",
    "            topic_id = topic_id_prob[0]\n",
    "            # index for prob that doc belongs that the corresponding topic\n",
    "            topic_prob = topic_id_prob[1]\n",
    "\n",
    "            # store all topic ids for doc\n",
    "            topic_ids.append(topic_id)\n",
    "            # store all topic probs for doc\n",
    "            topic_probs.append(topic_prob)\n",
    "\n",
    "        # get index for largest prob score\n",
    "        max_topic_prob_ind = np.argmax(topic_probs)\n",
    "        # get corresponding topic id\n",
    "        max_prob_topic_id = topic_ids[max_topic_prob_ind]\n",
    "        # store topic id that had the highest prob for doc being a memebr of that topic\n",
    "        doc_topic_ids.append(max_prob_topic_id)\n",
    "        \n",
    "    return doc_topic_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09f2b8ebaf696dc05187cecce8c9e251",
     "grade": false,
     "grade_id": "cell-e28e8774a79ac647",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# use get_topic_ids_for_docs get the topic id for each doc in the corpus - save result to `doc_topic_ids`\n",
    "\n",
    "# create a new feature in df_electronics called topic_id using `doc_topic_ids`\n",
    "\n",
    "# iterate through topic_id and use the lookup dict `topic_name_dict` to assign each document a topic name\n",
    "# save results to a new feature in df_electronics called `new_topic_name`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have created new topic names for your documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"reviews.text\", \"new_topic_name\"]\n",
    "df_electronics[cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# Stretch Goals\n",
    "\n",
    "- Treat `new_topic_name` as a Y vector and train a supervised learning model to predict the topic of each document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0018056dc2a2ddb2107860c63abca7eff55c565c606fe6595ba04b2466e00a74c",
   "display_name": "Python 3.8.5 64-bit ('DS-Unit-4-Sprint-1-NLP-1TUUJhOU': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}